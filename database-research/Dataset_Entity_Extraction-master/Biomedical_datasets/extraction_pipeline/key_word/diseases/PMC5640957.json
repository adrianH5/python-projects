[{"source": "PMC", "date": "20201217", "key": "pmc.key", "infons": {}, "documents": [{"id": "5640957", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.1186/s12859-017-1857-8", "article-id_pmc": "5640957", "article-id_pmid": "29029598", "article-id_publisher-id": "1857", "elocation-id": "451", "kwd": "Text mining Named entity recognition Entity name normalization Disease names Plant names Neural networks", "license": "\nOpen Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.", "name_0": "surname:Cho;given-names:Hyejin", "name_1": "surname:Choi;given-names:Wonjun", "name_2": "surname:Lee;given-names:Hyunju", "section_type": "TITLE", "title": "Keywords", "type": "front", "volume": "18", "year": "2017"}, "text": "A method for named entity normalization in biomedical articles: application to diseases and plants", "sentences": [], "annotations": [], "relations": []}, {"offset": 99, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Background", "sentences": [], "annotations": [], "relations": []}, {"offset": 110, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "In biomedical articles, a named entity recognition (NER) technique that identifies entity names from texts is an important element for extracting biological knowledge from articles. After NER is applied to articles, the next step is to normalize the identified names into standard concepts (i.e., disease names are mapped to the National Library of Medicine\u2019s Medical Subject Headings disease terms). In biomedical articles, many entity normalization methods rely on domain-specific dictionaries for resolving synonyms and abbreviations. However, the dictionaries are not comprehensive except for some entities such as genes. In recent years, biomedical articles have accumulated rapidly, and neural network-based algorithms that incorporate a large amount of unlabeled data have shown considerable success in several natural language processing problems.", "sentences": [], "annotations": [], "relations": []}, {"offset": 968, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Results", "sentences": [], "annotations": [], "relations": []}, {"offset": 976, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "In this study, we propose an approach for normalizing biological entities, such as disease names and plant names, by using word embeddings to represent semantic spaces. For diseases, training data from the National Center for Biotechnology Information (NCBI) disease corpus and unlabeled data from PubMed abstracts were used to construct word representations. For plants, a training corpus that we manually constructed and unlabeled PubMed abstracts were used to represent word vectors. We showed that the proposed approach performed better than the use of only the training corpus or only the unlabeled data and showed that the normalization accuracy was improved by using our model even when the dictionaries were not comprehensive. We obtained F-scores of 0.808 and 0.690 for normalizing the NCBI disease corpus and manually constructed plant corpus, respectively. We further evaluated our approach using a data set in the disease normalization task of the BioCreative V challenge. When only the disease corpus was used as a dictionary, our approach significantly outperformed the best system of the task.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2085, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Conclusions", "sentences": [], "annotations": [], "relations": []}, {"offset": 2097, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The proposed approach shows robust performance for normalizing biological entities. The manually constructed plant corpus and the proposed model are available at http://gcancer.org/plant and http://gcancer.org/normalization, respectively.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2336, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Electronic supplementary material", "sentences": [], "annotations": [], "relations": []}, {"offset": 2370, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The online version of this article (doi:10.1186/s12859-017-1857-8) contains supplementary material, which is available to authorized users.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2510, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Background", "sentences": [], "annotations": [], "relations": []}, {"offset": 2521, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "With the rapid accumulation of biomedical articles, developing accurate and efficient text-mining techniques for extracting knowledge from articles has become important. In the text-mining, named entity recognition (NER) is an important element. Named entities are meaningful real-world objects in predefined specific domains, and they are presented as single words or multi-word phrases in texts. NER involves identifying both predefined entities as well as the domain of the entities or the entity types from informal texts. After single words or multi-word phrases in texts have been recognized, the next step is named entity normalization by assigning suitable identifiers to recognized entities. For general entities, several natural language processing (NLP) studies, such as assigning entities to relevant Wikipedia abstracts or corresponding nodes in knowledge base, have been performed.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3417, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In biomedical articles, named entity normalization is challenging because many biological terms have multiple synonyms and term variations, and they are often referred to using abbreviations. To resolve these ambiguities, several NER and normalization studies have been conducted for several entity types such as biological entities (genes, proteins, diseases, and disorders) and chemical entities (drugs and compounds). The Critical Assessment of Information Extraction in Biology (BioCreative) organized biomedical NLP challenges. One of the subtasks in BioCreative V was NER and normalization for disease names.", "sentences": [], "annotations": [], "relations": []}, {"offset": 4032, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Although machine-learning (ML) approaches have been used for normalization, most normalization tools rely on the accuracy of domain-specific dictionaries or rules. This is because biological entities (1) have many synonyms; (2) are often referred to using abbreviations; (3) are described by phrases; and (4) are mixtures of alphabets, figures, and punctuation marks. The ProMiner system follows a dictionary-based approach based on an approximate string-matching method; it was designed to detect and normalize gene and protein names. This system uses preprocessed dictionaries that include biological entities with known synonyms. MetaMap was developed to improve the retrieval of relevant MEDLINE citations. This program maps biological entities to concept identifiers in the Unified Medical Language System (UMLS) Metathesaurus. GenNorm and GNAT, which are used for gene name normalization, and ChemSpot, which is used for chemical name normalization, also normalize entities that were extracted by their own dictionary components. Gimli is an NER tool designed to recognize the names of various biomedical entities. Because Gimli only performs NER, its functionalities are integrated into Neji for providing general normalization based on prioritized dictionaries. Lee et al. achieved a highest F-score of 86.46% for disease NER and normalization among 16 teams in BioCreative V. They used a dictionary-lookup approach based on the priority of dictionaries they assigned. Moara recognized gene and protein mentions using a hybrid methodology for normalization; the normalization task consists of flexible matching and ML-based matching strategies. Flexible matching is accomplished by exact matching from dictionaries; ML-based matching follows a feature-based approach such as prefix/suffix, bigram/trigram similarity, and string/shape similarity. tmChem applied a rule-based approach for concept normalization that converts identified mentions from articles to lexical variations such as lowercasing and removing whitespace and punctuations, and then maps them to specific database identifiers.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6134, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Unlike previous studies, DNorm uses pairwise learning to normalize disease names; it assigns mentions in the text to proper concept names in a controlled vocabulary, where a mention and a concept name are represented as a vector. DNorm outperformed MetaMap and Lucene when it was trained and tested using the National Center for Biotechnology Information (NCBI) disease corpus. However, because the vector consists of tokens appearing in mentions or concept names, tokens not appearing in a labeled data set might not be normalized properly. Thus, the importance of the labeled data set and predefined dictionaries, including synonym and abbreviation dictionaries, is emphasized, and it requires domain-specific dictionaries for normalization.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6878, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "To some extent, the reliance on dictionaries can be reduced by understanding words at the semantic level. Word semantics are better understood within the context of these words, which are represented by the surrounding words to the left or right. For example, sentences similar to \u201cThe standard systemic treatment for prostate cancer (PCa) is androgen ablation, which causes tumor regression by inhibiting activity of the androgen receptor (AR). (PubMed ID: 18593950)\u201d and \u201cAR remains important in the development and progression of prostate cancer. (PubMed ID: 15082523)\u201d are frequently repeated in biomedical texts. This allows us to infer that \u201cprostate cancer\u201d, \u201candrogen receptor\u201d, and \u201cAR\u201d are related words in their semantics.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7632, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Rumelhart et al. represented words in a vector space, where similar words are located close together. Recently, neural-network-based approaches have been developed for word representations; these methods are useful for identifying word similarities. These methods have become popular because word representation can be learned from a large amount of unlabeled data. Deep learning approaches using a large amount of unstructured data have attracted much attention, and they have been applied to many NLP problems with considerable success. Lample et al. utilized a long short-term memory (LSTM) architecture and character-based word representations for the NER task. Ma et al. proposed a neural network architecture that combines bidirectional LSTM, convolutional neural networks, and conditional random fields for the sequence labeling tasks, including part-of-speech tagging and NER. To evaluate the proposed NER system, they used the English data set from the CoNLL 2003 shared task. However, these studies were not extended to the normalization task.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8686, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In this study, we propose a method for normalizing biological entities, for example, disease names and plant names, by representing words in continuous vector spaces using neural networks. We combine a dictionary-based approach and word representations using a training corpus and unlabeled PubMed abstracts to incorporate the contexts of words. We compared our new method to DNorm to normalize disease names with and without an abbreviation dictionary. We also applied our approach for normalizing plant mentions, which does not have an abbreviation dictionary. Without an abbreviation dictionary, this approach showed good performance for normalizing biological entities.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9360, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 9368, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Data resources", "sentences": [], "annotations": [], "relations": []}, {"offset": 9383, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Entity dictionary", "sentences": [], "annotations": [], "relations": []}, {"offset": 9401, "infons": {"section_type": "METHODS", "type": "title_4"}, "text": "Disease name dictionary", "sentences": [], "annotations": [], "relations": []}, {"offset": 9425, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For the disease name dictionary, we used MErged DIsease vocabulary (MEDIC) that combines the Diseases branch of the National Library of Medicine\u2019s Medical Subject Headings (MeSH) and the Online Mendelian Inheritance in Man (OMIM). MeSH is a controlled vocabulary that includes synonyms in a hierarchical tree structure ranging from 16 general categories (e.g., Neoplasms) to more specific ones (e.g., Retinoblastoma) across 13 hierarchical levels. This hierarchy provides a way to navigate from higher to specific levels so that the relationships between diseases can be found. To merge the disease names in the two dictionaries, the terms under the Diseases branch was used. OMIM is a well-known resource for human genetic diseases. OMIM, unlike MeSH, is a flat list of different concepts such as phenotypes and genes, and it does not provide connections between similar diseases. MEDIC is a disease dictionary that combines the strengths of MeSH and OMIM, and it provides disease information, including disease names, concept identifiers (IDs), definitions of the diseases, information about parent nodes, and synonyms. MEDIC contains around 9 700 disease names and 67,000 synonyms.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10612, "infons": {"section_type": "METHODS", "type": "title_4"}, "text": "Plant name dictionary", "sentences": [], "annotations": [], "relations": []}, {"offset": 10634, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In this study, the term \u201cplants\u201d refers to a wide range of organisms, including trees, shrubs, and primitive plants, such as fungi, mosses, algae, and lichens. For thousands of years, plants have been valued for their medicinal and healthful qualities. Various scientific and common names are used for plants, because plant names have been derived from several civilizations (e.g., Greek and Chinese), and plants have evolved into various structures. Compared to other biological entities such as genes or proteins, for which several normalization studies have been performed, few studies on plant name normalization have been performed. To normalize plant names, we need a well-organized dictionary of plant identifiers. We extracted a viridiplantae ontology for plants from the NCBI Taxonomy database that consists of NCBI taxonomy IDs, scientific names, synonyms, and hierarchical taxonomic information. The NCBI taxonomy database indexes over 150,000 viridiplantae that are constructed from whole, partial, or phonetically spelled organism names, and it provides information about organisms that are commonly used in biological research.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11780, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 11787, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_caption"}, "text": "NCBI disease corpus and our plant corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 11828, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">Data set</th><th align=\"left\">Abstracts</th><th align=\"left\">Total</th><th align=\"left\">Unique</th><th align=\"left\">Unique</th></tr><tr><th align=\"left\"/><th align=\"left\"/><th align=\"left\">disease</th><th align=\"left\">disease</th><th align=\"left\">concept</th></tr><tr><th align=\"left\"/><th align=\"left\"/><th align=\"left\">mentions</th><th align=\"left\">mentions</th><th align=\"left\">IDs</th></tr></thead><tbody><tr><td align=\"left\">Disease training set</td><td align=\"left\">592</td><td align=\"left\">5145</td><td align=\"left\">1170</td><td align=\"left\">670</td></tr><tr><td align=\"left\">Disease development set</td><td align=\"left\">100</td><td align=\"left\">787</td><td align=\"left\">368</td><td align=\"left\">176</td></tr><tr><td align=\"left\">Disease test set</td><td align=\"left\">100</td><td align=\"left\">960</td><td align=\"left\">427</td><td align=\"left\">203</td></tr><tr><td align=\"left\">Total</td><td align=\"left\">792</td><td align=\"left\">6892</td><td align=\"left\">2136</td><td align=\"left\">790</td></tr><tr><td align=\"left\">Plant training set</td><td align=\"left\">128</td><td align=\"left\">2647</td><td align=\"left\">1543</td><td align=\"left\">1143</td></tr><tr><td align=\"left\">Plant development set</td><td align=\"left\">40</td><td align=\"left\">709</td><td align=\"left\">400</td><td align=\"left\">329</td></tr><tr><td align=\"left\">Plant test set</td><td align=\"left\">40</td><td align=\"left\">629</td><td align=\"left\">427</td><td align=\"left\">298</td></tr><tr><td align=\"left\">Total</td><td align=\"left\">208</td><td align=\"left\">3985</td><td align=\"left\">2370</td><td align=\"left\">1770</td></tr></tbody></table>\n"}, "text": "Data set\tAbstracts\tTotal\tUnique\tUnique\t \t\t\tdisease\tdisease\tconcept\t \t\t\tmentions\tmentions\tIDs\t \tDisease training set\t592\t5145\t1170\t670\t \tDisease development set\t100\t787\t368\t176\t \tDisease test set\t100\t960\t427\t203\t \tTotal\t792\t6892\t2136\t790\t \tPlant training set\t128\t2647\t1543\t1143\t \tPlant development set\t40\t709\t400\t329\t \tPlant test set\t40\t629\t427\t298\t \tTotal\t208\t3985\t2370\t1770\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 12206, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Disease and plant corpora were used for training and testing normalization models. Table\u00a01 shows the size of the corpora used in this study.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 12350, "infons": {"section_type": "METHODS", "type": "title_4"}, "text": "Disease corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 12365, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For diseases, the NCBI disease corpus was used in the present study. This corpus consists of 793 PubMed abstracts, 6 892 disease mentions, and 790 unique disease concepts using disease terms in MEDIC. Preannotation was performed using PubTator. After this step, the abstracts were manually annotated by 14 annotators. Finally, the annotated abstracts were curated by biomedical experts. The annotated abstracts consist of a training set, a development set, and a test set; these were respectively used to construct the models, set the hyperparameters in normalization models, and evaluate the models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12966, "infons": {"section_type": "METHODS", "type": "title_4"}, "text": "Plant corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 12979, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For plants, we manually constructed training, development, and test sets because no appropriate corpus specific for plants is available. From 208 abstracts with 19 mentions per abstract, a total of 3 985 mentions were extracted and then mapped into concepts in the NCBI taxonomy database. Two annotators participated in constructing the corpus; their inter-annotator agreement (IAA) scores were 0.985 and 0.889 for plant name recognition and normalization, respectively, suggesting a high level of agreement. Details about the annotations, including the curator guidelines and IAA, are provided in the Additional file\u00a01.", "sentences": [], "annotations": [], "relations": []}, {"offset": 13601, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Abbreviation dictionary", "sentences": [], "annotations": [], "relations": []}, {"offset": 13625, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In biomedical articles, long disease names occur many times, and they are often referred to using acronyms and other shorthand. However, a general rule for using acronyms does not exist, different abbreviations are often used for the same names, and some authors even create new acronyms. Therefore, two different words written in the same paragraph may indicate the same entities, or two different diseases may be written using the same word. For example, \u201cAngelman Syndrome\u201d and \u201cAnkylosing Spondylitis\u201d are both abbreviated as \u201cAS\u201d. Therefore, resolving abbreviations is an important issue in NER research. DNorm used their own abbreviation dictionary to solve the problem of acronym normalization.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14339, "infons": {"file": "12859_2017_1857_Fig1_HTML.jpg", "id": "Fig1", "section_type": "FIG", "type": "fig_caption"}, "text": "Comparison between the NCBI disease corpus and the abbreviation dictionary. The upper and the lower pie charts represent the NCBI training corpus and the NCBI test corpus, respectively. The dark gray parts represent abstracts in which disease names are not abbreviated, and yellow parts represent abstracts that contain at least one disease name abbreviation. Among the abstracts in yellow, the red parts represent abstracts with disease abbreviation information in the abbreviation dictionary, and the gray parts represent abstracts that contain at least one disease name abbreviation that is not included in the abbreviation dictionary", "sentences": [], "annotations": [], "relations": []}, {"offset": 14977, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For disease names, we used the abbreviation dictionary provided by DNorm. It consists of PubMed IDs, disease acronyms, and original long words. However, this dictionary is optimized for the NCBI disease corpus. As shown in Fig.\u00a01, out of the 592 abstracts in the training corpus, 415 had abbreviations for disease names, 84% of which are in the dictionary. Similarly, out of the 100 abstracts in the test corpus, 68 had abbreviations for disease names, 83% of which are in the dictionary. In addition, although a well-constructed dictionary of disease abbreviation exists, dictionaries of other biological entities such as plant abbreviation names do not exist. Thus, when we compared our approach to DNorm, we measured performances with and without this abbreviation dictionary. For plant names, we did not use an abbreviation dictionary because no dictionary is available.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 15855, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Training a normalization model", "sentences": [], "annotations": [], "relations": []}, {"offset": 15886, "infons": {"file": "12859_2017_1857_Fig2_HTML.jpg", "id": "Fig2", "section_type": "FIG", "type": "fig_caption"}, "text": "A schematic of the proposed approach", "sentences": [], "annotations": [], "relations": []}, {"offset": 15923, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Figure\u00a02 shows an overview of the training and test steps in our approach. In the training step, abstracts in the NCBI disease corpus and plant corpus and unlabeled data are used to construct the normalization model. In this study, the unlabeled data include a set of abstracts (or sentences) from which disease and plant names were extracted using NER tools. Note that they are considered unlabeled data because the disease and plant names were not normalized. The disease and plant names in the unlabeled PubMed abstracts were extracted using BANNER and LingPipe, respectively. Then, we modified the training corpus and the unlabeled data from PubMed using synonyms and concepts of biological entities in the dictionaries. Finally, we represented all words in the modified training data sets and unlabeled data from PubMed in the vector space using Word2Vec. The details are described in the following subsections.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 16843, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Incorporating information in training data sets", "sentences": [], "annotations": [], "relations": []}, {"offset": 16891, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We describe how information in the entity dictionaries and the training corpus are incorporated before we construct word vectors for all tokens in the training corpus, unlabeled data, and entity dictionaries. Throughout this paper, the names for biological entities in the sentences are called mentions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 17195, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We replaced mentions in the sentences from the training corpus and unlabeled data with synonyms in the dictionary and concepts in the training corpus. For example, if \u201ccancer\u201d was mentioned in a sentence, new sentences were created in which \u201ccancer\u201d was replaced by its synonyms such as \u201cneoplasms\u201d, \u201ctumor\u201d, \u201ctumors\u201d, \u201ctumour\u201d, or \u201ctumours\u201d. We also added stemming variations of disease names. The lexical variations were obtained with a stemming analyzer in Apache Lucene, which implements the Porter Stemming Algorithm. For example, if \u201cmetabolism\u201d was mentioned in a sentence, the root form \u201cmetabole\u201d and common variations of \u201cmetabole\u201d, including \u201cmetabolic\u201d, \u201cmetabolite\u201d, and \u201cmetabolize\u201d, were replaced to create new sentences.", "sentences": [], "annotations": [], "relations": []}, {"offset": 17984, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "If mentions comprised multiple words, we connected each word using an underscore symbol, thus generating a single word. For example, if the mention \u201cbreast cancer\u201d was identified from a sentence, a new sentence was created in which \u201cbreast cancer\u201d was replaced by the single word \u201cbreast_cancer\u201d. In addition, mentions that were not included in the training data cannot be represented as vectors. To increase the coverage of entities to be represented in the vector space, disease or plant names and their synonyms in the entity dictionary that were not included in the training data were added to the training data.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18613, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Word representations", "sentences": [], "annotations": [], "relations": []}, {"offset": 18634, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "CBOW equation:   ", "sentences": [], "annotations": [], "relations": []}, {"offset": 18652, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Skip-gram equation:   ", "sentences": [], "annotations": [], "relations": []}, {"offset": 18675, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Mikolov et al. developed Word2Vec, a neural network approach for computing the vector representations of words. Vectors can be constructed using two algorithms: a continuous bag-of-word (CBOW) model and a skip-gram model. The CBOW model learns word representations by predicting a word in a sentence using its surrounding words, and the skip-gram model learns word representations by predicting the surrounding words of a word in the input layer. In Word2Vec, words are represented by vectors in hundreds of dimensions, and words that have related meanings are more likely to have similar values in the vector space. A vector w t for a word located at the t-th position in a sentence is calculated by maximizing the average log probability as follows:   ", "sentences": [], "annotations": [], "relations": []}, {"offset": 19430, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "where  and  are vectors for the surrounding c words in the sentence, and T is the number of tokens. We applied several options of a vector size of a word and a window size for surrounding words for both CBOW and skip-gram algorithms to train the models, and then, we chose the best options using the development sets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19748, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "\u201cAffected males and females are equally likely to transmit Van der Woude syndrome\u201d,", "sentences": [], "annotations": [], "relations": []}, {"offset": 19836, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "\u201cAffected males and females are equally likely to transmit Van_der_Woude_syndrome\u201d,", "sentences": [], "annotations": [], "relations": []}, {"offset": 19924, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "\u201cAffected males and females are equally likely to transmit lip pits\u201d, and", "sentences": [], "annotations": [], "relations": []}, {"offset": 20002, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "\u201cAffected males and females are equally likely to transmit lip_pits\u201d.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20076, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To use unlabeled data in PubMed, we collected four groups of texts: (1) all PubMed abstracts (hereafter referred to as \u201call abstracts\u201d), (2) biological-entity-specific abstracts that contain at least one biological entity name in the abstracts (\u201centity-specific abstracts\u201d), (3) sentences that include at least one biological entity name in the sentence (\u201cevidence sentences\u201d), and (4) a collection of \u201cevidence sentences\u201d and modified evidence sentences (\u201cmodified evidence sentences\u201d). Here, biological entities were identified using NER tools. For disease names, we used BANNER because it has been used in several disease name recognition systems including DNorm and in several studies. For plants, we applied LingPipe using exact matching based on the plant dictionary because several systems have used dictionary-based approaches for plant or species name recognition. Note that the NER systems were used to construct unlabeled data because the amount of unlabeled data is too large to manually curate entity names. Modified evidence sentences were constructed by replacing mentions of biological entities with concepts in the training set and synonyms in the dictionary as described in the \u201cIncorporating information in training data sets\u201d section. For example, \u201cVan der Woude syndrome\u201d is abbreviated as \u201cVWS\u201d and has a synonym of \u201clip pits\u201d. Thus, a sentence in the trainning data \u201cAffected males and females are equally likely to transmit VWS. (PubMed ID: 4019732)\u201d generates following modified sentences:   ", "sentences": [], "annotations": [], "relations": []}, {"offset": 21634, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We propose four semi-supervised learning models. Each model constructs a vector set V of words representing words in the vector space by applying Word2Vec to the training corpus and unlabeled data sets: (1) semi-supervised learning with unlabeled data of \u201call abstracts\u201d (hereafter referred to as \u201cSSL-all abstracts\u201d), (2) semi-supervised learning with unlabeled data of \u201centity-specific abstracts\u201d (\u201cSSL-entity abstracts\u201d), (3) semi-supervised learning with unlabeled data of \u201cevidence sentences\u201d (\u201cSSL-evidences\u201d), and (4) semi-supervised learning with unlabeled data of \u201cmodified evidence sentences\u201d (\u201cSSL-modified evidences\u201d). In addition to these four models, we constructed (5) semi-supervised model that used only modified evidence sentences without the training corpus (\u201cSSL-only modified evidences\u201d). For comparison, we also constructed a supervised learning model with the training corpus (\u201cSL-only training data\u201d).", "sentences": [], "annotations": [], "relations": []}, {"offset": 22600, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Prediction for normalizing biological entities", "sentences": [], "annotations": [], "relations": []}, {"offset": 22647, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "As shown in Fig.\u00a02, in the test step, abstracts in the NCBI disease corpus and in the plant corpus were used to test the normalization model. Biological mentions were extracted from the abstracts. If an extracted mention was exactly matched to a concept name, it was assigned to a corresponding concept ID, and additional normalization steps were not performed. Next, we applied an abbreviation resolution step, in which acronyms were changed to the original long words by using the abbreviation dictionary. The abbreviation resolution step is indicated by a dashed square because we investigated our proposed tool with and without the abbreviation step. For plants, we did not use the abbreviation step.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23353, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For the normalization, test mentions are mapped to their concepts by calculating the cosine similarities between a vector of the test mention and vectors of every possible concept in the entity dictionary. Then, words with high cosine similarities were considered candidate concepts (Fig.\u00a02). Let a mention m and a candidate concept c be represented vectors v m and v c, respectively. When a mention m comprises a single token such as \u201ccancer\u201d or \u201ctumours\u201d, a vector for the single token in the vector set V is assigned to v m. When a mention m comprises multiple tokens, v m is assigned as the average of vectors for tokens in the mention as follows:   ", "sentences": [], "annotations": [], "relations": []}, {"offset": 24017, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "where v is the vector of the i-th token in the mention and n, the number of tokens. If the j-th term vector v, we assign a zero vector to v and calculate the average vector v m by using Eq. (3). Note that concepts with multiple tokens were converted into a single token using an underscore symbol in the training step. After the mentions for biological entities were represented as vectors, concepts with high cosine similarities in word vectors v c\u2208V to the vector v m of a query biological entity were recommended as normalized concepts.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24559, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Evaluation metric", "sentences": [], "annotations": [], "relations": []}, {"offset": 24577, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table_caption"}, "text": "An example of candidate normalized disease names for the mention \u201cC7 defects\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 24659, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\"/><th align=\"left\">Ranks</th><th align=\"left\">Candidate names</th><th align=\"left\">Cosine</th></tr><tr><th align=\"left\"/><th align=\"left\"/><th align=\"left\"/><th align=\"left\">similarity</th></tr></thead><tbody><tr><td align=\"left\">\u2217</td><td align=\"left\">1</td><td align=\"left\">COMPLEMENT_COMPONENT_7_DEFICIENCY</td><td align=\"left\">0.559244</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">2</td><td align=\"left\">complement_compon_7_defici</td><td align=\"left\">0.554464</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">3</td><td align=\"left\">c7_defici</td><td align=\"left\">0.549911</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">4</td><td align=\"left\">complement_component_7_deficiency</td><td align=\"left\">0.540654</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">5</td><td align=\"left\">C7_DEFICIENCY</td><td align=\"left\">0.533657</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">6</td><td align=\"left\">c7_deficiency</td><td align=\"left\">0.525014</td></tr><tr><td align=\"left\"/><td align=\"left\">7</td><td align=\"left\">antibodi_defici_syndrom</td><td align=\"left\">0.510718</td></tr><tr><td align=\"left\"/><td align=\"left\">8</td><td align=\"left\">Immunologic_Deficiency_Syndromes</td><td align=\"left\">0.499981</td></tr><tr><td align=\"left\"/><td align=\"left\">9</td><td align=\"left\">immunolog_defici_syndrom</td><td align=\"left\">0.492753</td></tr><tr><td align=\"left\">\u2217</td><td align=\"left\">10</td><td align=\"left\">c7d</td><td align=\"left\">0.491925</td></tr></tbody></table>\n"}, "text": "\tRanks\tCandidate names\tCosine\t \t\t\t\tsimilarity\t \t\u2217\t1\tCOMPLEMENT_COMPONENT_7_DEFICIENCY\t0.559244\t \t\u2217\t2\tcomplement_compon_7_defici\t0.554464\t \t\u2217\t3\tc7_defici\t0.549911\t \t\u2217\t4\tcomplement_component_7_deficiency\t0.540654\t \t\u2217\t5\tC7_DEFICIENCY\t0.533657\t \t\u2217\t6\tc7_deficiency\t0.525014\t \t\t7\tantibodi_defici_syndrom\t0.510718\t \t\t8\tImmunologic_Deficiency_Syndromes\t0.499981\t \t\t9\timmunolog_defici_syndrom\t0.492753\t \t\u2217\t10\tc7d\t0.491925\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 25089, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table_foot"}, "text": "The concept id of \u201cC7 defects\u201d is \u201cOMIM:610102\u201d, and the asterisk mark (\u2217) in the first column indicates that candidate names belong to \u201cOMIM:610102\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 25253, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To measure the performance of the disease name normalization tools, we compared highly ranked predicted concepts with manually mapped concepts in the test corpus. Table\u00a02 shows an example of normalized disease names from the NCBI test set. \u201cC7 defects\u201d is the synonym of \u201cCOMPLEMENT COMPONENT 7 DEFICIENCY\u201d as a disease mention in the NCBI disease test corpus, and the corresponding concept identifier is \u201cOMIM:610102\u201d. For a given mention, other names were ranked according to cosine similarities with the mention in the vector representation. Because a concept identifier includes several disease synonyms, asterisks in the first column indicate that these words are synonyms for the concept identifier, meaning that they are correctly recommended answers. In Table\u00a02, the candidate mentions ranked first, second, third, fourth, fifth, sixth, and tenth are the correct results.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 26149, "infons": {"file": "12859_2017_1857_Fig3_HTML.jpg", "id": "Fig3", "section_type": "FIG", "type": "fig_caption"}, "text": "An example of lists of candidate concepts and accuracies. When the rank threshold is third, we consider concepts ranked from first to third as positives. \u201cO\u201d indicates that the predicted concepts are correct and \u201cX\u201d indicates that they are incorrect", "sentences": [], "annotations": [], "relations": []}, {"offset": 26407, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We measured the performance of the normalization model for all mentions in the test set for each rank threshold. For the given rank threshold, the predicted names (or their corresponding concept IDs) that ranked higher than the threshold were considered positively predicted. True positives (TP) were correct positive predictions, false positives (FP) were incorrect positive predictions, and false negatives (FN) were mentions that are not positively predicted. For the case in which an extracted mention was exactly matched to a concept name, only a single concept ID was assigned, and it was a correct normalization. Therefore, when calculating the performance for each rank threshold, this exact match was treated as a true positive. Figure\u00a03 shows an example of the candidate lists and TP, FP, and FN. The precision (p), recall (r), and F-score (f) are calculated as follows:    ", "sentences": [], "annotations": [], "relations": []}, {"offset": 27293, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "Results", "sentences": [], "annotations": [], "relations": []}, {"offset": 27301, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Disease name normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 27328, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "To measure the performance of disease name normalization tools using the test corpus, we first extracted disease mentions in the 100 test abstracts using BANNER, and then, we manually curated correct disease mentions, thereby generating 843 test mentions. Note that because DNorm applied BANNER to extract candidate disease mentions from test abstracts, we also applied BANNER to compare normalization results under the same condition as DNorm.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27773, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For disease name normalization, we constructed six models: (1) \u201cSSL-all abstracts\u201d with 1,167,886 word vectors from 13,408,565 PubMed abstracts, (2) \u201cSSL-entity abstracts\u201d with 756,089 word vectors from 7,980,370 disease-related \u201centity-specific abstracts\u201d, (3) \u201cSSL-evidences\u201d with 350,011 word vectors from 4,758,992 disease-related \u201cevidence sentences\u201d, (4) \u201cSSL-modified evidences\u201d with 740,353 word vectors, (5) \u201cSSL-only modified evidences\u201d with 714,575 word vectors, and (6) \u201cSL-only training data\u201d with 51,619 word vectors from the 592 NCBI disease training corpus.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28379, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_caption"}, "text": "Comparison of F-score of our disease normalization models using four biomedical text groups", "sentences": [], "annotations": [], "relations": []}, {"offset": 28471, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">Models</th><th align=\"left\">Win</th><th align=\"left\">Dim</th><th align=\"left\">Method</th><th align=\"left\">Precision</th><th align=\"left\">Recall</th><th align=\"left\">F-score</th></tr></thead><tbody><tr><td align=\"left\">SSL-all abstracts</td><td align=\"left\">8</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.627</td><td align=\"left\">0.832</td><td align=\"left\">0.715</td></tr><tr><td align=\"left\">SSL-entity abstracts</td><td align=\"left\">8</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.633</td><td align=\"left\">0.838</td><td align=\"left\">0.721</td></tr><tr><td align=\"left\">SSL-evidences</td><td align=\"left\">8</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.633</td><td align=\"left\">0.840</td><td align=\"left\">0.722</td></tr><tr><td align=\"left\">SSL-modified evidences</td><td align=\"left\">8</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">\n<bold>0.706</bold>\n</td><td align=\"left\">\n<bold>0.891</bold>\n</td><td align=\"left\">\n<bold>0.788</bold>\n</td></tr></tbody></table>\n"}, "text": "Models\tWin\tDim\tMethod\tPrecision\tRecall\tF-score\t \tSSL-all abstracts\t8\t200\tCBOW\t0.627\t0.832\t0.715\t \tSSL-entity abstracts\t8\t200\tCBOW\t0.633\t0.838\t0.721\t \tSSL-evidences\t8\t200\tCBOW\t0.633\t0.840\t0.722\t \tSSL-modified evidences\t8\t200\tCBOW\t0.706\t0.891\t0.788\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 28721, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_foot"}, "text": "The bold font denotes the best result for each column", "sentences": [], "annotations": [], "relations": []}, {"offset": 28775, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Table\u00a03 shows the comparison results of the four semi-supervised models, which combine training data and unlabeled data, for normalizing 843 disease mentions. To construct the word vectors used in the models, we used the default parameter values for the CBOW algorithm in Word2Vec: window size = 8 and vector dimension = 200. When \u201cSSL-all abstracts\u201d was used, the precision and F-score were the lowest. However, the model\u2019s performance was similar to that of \u201cSSL-evidences\u201d and \u201cSSL-entity abstracts\u201d. Although more unlabeled data may increase the model performance in general, the results show that unlabeled data that are more relevant to entities led to slightly better results. \u201cSSL-modified evidences\u201d was the most powerful normalization tool, showing that the direct incorporation of entity synonyms in unlabeled data improved the normalization performance.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 29662, "infons": {"file": "Tab4.xml", "id": "Tab4", "section_type": "TABLE", "type": "table_caption"}, "text": "Performance comparison of disease normalization models using various parameters", "sentences": [], "annotations": [], "relations": []}, {"offset": 29742, "infons": {"file": "Tab4.xml", "id": "Tab4", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"4\"/><th align=\"left\" colspan=\"3\">Development set</th><th align=\"left\" colspan=\"3\">Test set</th></tr><tr><th align=\"left\">Parameters</th><th align=\"left\">Win</th><th align=\"left\">Dim</th><th align=\"left\">Method</th><th align=\"left\">Precision</th><th align=\"left\">Recall</th><th align=\"left\">F-score</th><th align=\"left\">Precision</th><th align=\"left\">Recall</th><th align=\"left\">F-score</th></tr></thead><tbody><tr><td align=\"left\">5_200_CBOW</td><td align=\"left\">5</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.740</td><td align=\"left\">0.918</td><td align=\"left\">0.819</td><td align=\"left\">0.684</td><td align=\"left\">0.896</td><td align=\"left\">0.776</td></tr><tr><td align=\"left\">5_200_skip</td><td align=\"left\">5</td><td align=\"left\">200</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.730</td><td align=\"left\">0.909</td><td align=\"left\">0.809</td><td align=\"left\">0.719</td><td align=\"left\">0.890</td><td align=\"left\">0.795</td></tr><tr><td align=\"left\">5_300_CBOW</td><td align=\"left\">5</td><td align=\"left\">300</td><td align=\"left\">CBOW</td><td align=\"left\">0.738</td><td align=\"left\">0.918</td><td align=\"left\">0.818</td><td align=\"left\">0.674</td><td align=\"left\">0.892</td><td align=\"left\">0.767</td></tr><tr><td align=\"left\">5_300_skip</td><td align=\"left\">5</td><td align=\"left\">300</td><td align=\"left\">Skip-gram</td><td align=\"left\">\n<bold>0.746</bold>\n</td><td align=\"left\">0.916</td><td align=\"left\">\n<bold>0.822</bold>\n</td><td align=\"left\">0.722</td><td align=\"left\">0.893</td><td align=\"left\">0.798</td></tr><tr><td align=\"left\">5_400_CBOW</td><td align=\"left\">5</td><td align=\"left\">400</td><td align=\"left\">CBOW</td><td align=\"left\">0.730</td><td align=\"left\">0.918</td><td align=\"left\">0.813</td><td align=\"left\">0.661</td><td align=\"left\">0.878</td><td align=\"left\">0.754</td></tr><tr><td align=\"left\">5_400_skip</td><td align=\"left\">5</td><td align=\"left\">400</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.732</td><td align=\"left\">0.916</td><td align=\"left\">0.813</td><td align=\"left\">0.730</td><td align=\"left\">\n<bold>0.905</bold>\n</td><td align=\"left\">\n<bold>0.808</bold>\n</td></tr><tr><td align=\"left\">7_200_CBOW</td><td align=\"left\">7</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.738</td><td align=\"left\">\n<bold>0.919</bold>\n</td><td align=\"left\">0.819</td><td align=\"left\">0.676</td><td align=\"left\">0.891</td><td align=\"left\">0.769</td></tr><tr><td align=\"left\">7_200_skip</td><td align=\"left\">7</td><td align=\"left\">200</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.719</td><td align=\"left\">0.900</td><td align=\"left\">0.799</td><td align=\"left\">0.698</td><td align=\"left\">0.882</td><td align=\"left\">0.780</td></tr><tr><td align=\"left\">7_300_CBOW</td><td align=\"left\">7</td><td align=\"left\">300</td><td align=\"left\">CBOW</td><td align=\"left\">0.709</td><td align=\"left\">0.911</td><td align=\"left\">0.798</td><td align=\"left\">0.662</td><td align=\"left\">0.880</td><td align=\"left\">0.756</td></tr><tr><td align=\"left\">7_300_skip</td><td align=\"left\">7</td><td align=\"left\">300</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.683</td><td align=\"left\">0.895</td><td align=\"left\">0.775</td><td align=\"left\">\n<bold>0.776</bold>\n</td><td align=\"left\">0.769</td><td align=\"left\">0.772</td></tr><tr><td align=\"left\">7_400_CBOW</td><td align=\"left\">7</td><td align=\"left\">400</td><td align=\"left\">CBOW</td><td align=\"left\">0.702</td><td align=\"left\">0.898</td><td align=\"left\">0.788</td><td align=\"left\">0.632</td><td align=\"left\">0.850</td><td align=\"left\">0.725</td></tr><tr><td align=\"left\">7_400_skip</td><td align=\"left\">7</td><td align=\"left\">400</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.690</td><td align=\"left\">0.896</td><td align=\"left\">0.779</td><td align=\"left\">0.667</td><td align=\"left\">0.887</td><td align=\"left\">0.761</td></tr><tr><td align=\"left\">8_200_CBOW</td><td align=\"left\">8</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.710</td><td align=\"left\">0.907</td><td align=\"left\">0.797</td><td align=\"left\">0.706</td><td align=\"left\">0.891</td><td align=\"left\">0.788</td></tr></tbody></table>\n"}, "text": "\tDevelopment set\tTest set\t \tParameters\tWin\tDim\tMethod\tPrecision\tRecall\tF-score\tPrecision\tRecall\tF-score\t \t5_200_CBOW\t5\t200\tCBOW\t0.740\t0.918\t0.819\t0.684\t0.896\t0.776\t \t5_200_skip\t5\t200\tSkip-gram\t0.730\t0.909\t0.809\t0.719\t0.890\t0.795\t \t5_300_CBOW\t5\t300\tCBOW\t0.738\t0.918\t0.818\t0.674\t0.892\t0.767\t \t5_300_skip\t5\t300\tSkip-gram\t0.746\t0.916\t0.822\t0.722\t0.893\t0.798\t \t5_400_CBOW\t5\t400\tCBOW\t0.730\t0.918\t0.813\t0.661\t0.878\t0.754\t \t5_400_skip\t5\t400\tSkip-gram\t0.732\t0.916\t0.813\t0.730\t0.905\t0.808\t \t7_200_CBOW\t7\t200\tCBOW\t0.738\t0.919\t0.819\t0.676\t0.891\t0.769\t \t7_200_skip\t7\t200\tSkip-gram\t0.719\t0.900\t0.799\t0.698\t0.882\t0.780\t \t7_300_CBOW\t7\t300\tCBOW\t0.709\t0.911\t0.798\t0.662\t0.880\t0.756\t \t7_300_skip\t7\t300\tSkip-gram\t0.683\t0.895\t0.775\t0.776\t0.769\t0.772\t \t7_400_CBOW\t7\t400\tCBOW\t0.702\t0.898\t0.788\t0.632\t0.850\t0.725\t \t7_400_skip\t7\t400\tSkip-gram\t0.690\t0.896\t0.779\t0.667\t0.887\t0.761\t \t8_200_CBOW\t8\t200\tCBOW\t0.710\t0.907\t0.797\t0.706\t0.891\t0.788\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 30659, "infons": {"file": "Tab4.xml", "id": "Tab4", "section_type": "TABLE", "type": "table_foot"}, "text": "The bold font denotes the best result for each column", "sentences": [], "annotations": [], "relations": []}, {"offset": 30713, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Next, to find the optimal hyperparameters to learn word vectors, we applied different hyperparameters to the \u201cSSL-modified evidences\u201d model. When the NCBI disease development set was used to select hyperparameters, window size = 5 and vector dimension = 300, and a skip-gram method were selected (Table\u00a04). The performance of the test set with these parameters was also close to the highest performance. Thus, these values were used in the following comparison.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 31182, "infons": {"file": "12859_2017_1857_Fig4_HTML.jpg", "id": "Fig4", "section_type": "FIG", "type": "fig_caption"}, "text": "Performance comparison between DNorm and our models for disease name normalization with and without the abbreviation resolution step. In (a) and (b), dark-aqua bars indicate \u201cDNorm\u201d and the gray, dark-gray, and red bars indicate the \u201cSL-only training data\u201d, \u201cSSL-only modified evidences\u201d and \u201cSSL-modified evidences\u201d models, respectively. The x-axis represents the thresholds for ranks, and the y-axis indicates the F-scores of the models for each rank. c The precision, recall, and F-scores are shown for the four models", "sentences": [], "annotations": [], "relations": []}, {"offset": 31720, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Moreover, we compared \u201cSSL-modified evidences\u201d with two additional cases: (1) \u201cSL-only training data\u201d and (2) \u201cSSL-only modified evidences\u201d with 714,575 word vectors. In addition, we compared DNorm with our approach. Figure\u00a04 shows performance comparisons with and without the abbreviation step. \u201cSL-only training data\u201d was better than \u201cSSL-only modified evidences\u201d, although \u201cSSL-modified evidences\u201d outperformed both cases. The results show that the normalization accuracies were improved when unlabeled data were incorporated with training data. The accuracy of \u201cSSL-modified evidences\u201d showed the best performance. Although the performance of our model was slightly higher than that of DNorm with the abbreviation step, it significantly outperformed DNorm without the abbreviation step. For DNorm, the F-score decreased significantly from 0.747 to 0.656 without the abbreviation step.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 32640, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Plant name normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 32665, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For plant name normalization, we constructed three plant models: (1) \u201cSL-only plant training data\u201d with 94,338 word vectors, (2) \u201cSSL-only modified plant evidences\u201d with 594,802 word vectors, and (3) \u201cSSL-modified plant evidences\u201d with 649,759 word vectors. For plant evidence sentences, we collected 2,620,684 sentences containing plant names in the NCBI taxonomy database from PubMed abstracts. Note that because \u201cSSL-modified evidences\u201d showed the best performance for disease name normalization, we tested \u201cSSL-modified plant evidences\u201d among the several SSL models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33256, "infons": {"file": "Tab5.xml", "id": "Tab5", "section_type": "TABLE", "type": "table_caption"}, "text": "Performance comparison of plant normalization models using various parameters", "sentences": [], "annotations": [], "relations": []}, {"offset": 33334, "infons": {"file": "Tab5.xml", "id": "Tab5", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"4\"/><th align=\"left\" colspan=\"3\">Development set</th><th align=\"left\" colspan=\"3\">Test set</th></tr><tr><th align=\"left\">Parameters</th><th align=\"left\">Win</th><th align=\"left\">Dim</th><th align=\"left\">Method</th><th align=\"left\">Precision</th><th align=\"left\">Recall</th><th align=\"left\">F-score</th><th align=\"left\">Precision</th><th align=\"left\">Recall</th><th align=\"left\">F-score</th></tr></thead><tbody><tr><td align=\"left\">5_200_CBOW</td><td align=\"left\">5</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">0.7284</td><td align=\"left\">0.8939</td><td align=\"left\">0.8027</td><td align=\"left\">\n<bold>0.594</bold>\n</td><td align=\"left\">\n<bold>0.824</bold>\n</td><td align=\"left\">\n<bold>0.690</bold>\n</td></tr><tr><td align=\"left\">5_200_skip</td><td align=\"left\">5</td><td align=\"left\">200</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.6821</td><td align=\"left\">0.8812</td><td align=\"left\">0.7690</td><td align=\"left\">0.524</td><td align=\"left\">0.783</td><td align=\"left\">0.628</td></tr><tr><td align=\"left\">5_300_CBOW</td><td align=\"left\">5</td><td align=\"left\">300</td><td align=\"left\">CBOW</td><td align=\"left\">0.7326</td><td align=\"left\">0.8934</td><td align=\"left\">0.8051</td><td align=\"left\">0.576</td><td align=\"left\">0.811</td><td align=\"left\">0.674</td></tr><tr><td align=\"left\">5_300_skip</td><td align=\"left\">5</td><td align=\"left\">300</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.6836</td><td align=\"left\">0.8813</td><td align=\"left\">0.7699</td><td align=\"left\">0.533</td><td align=\"left\">0.787</td><td align=\"left\">0.635</td></tr><tr><td align=\"left\">5_400_CBOW</td><td align=\"left\">5</td><td align=\"left\">400</td><td align=\"left\">CBOW</td><td align=\"left\">0.7311</td><td align=\"left\">\n<bold>0.8940</bold>\n</td><td align=\"left\">0.8044</td><td align=\"left\">0.568</td><td align=\"left\">0.809</td><td align=\"left\">0.667</td></tr><tr><td align=\"left\">5_400_skip</td><td align=\"left\">5</td><td align=\"left\">400</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.7164</td><td align=\"left\">0.8878</td><td align=\"left\">0.7929</td><td align=\"left\">0.540</td><td align=\"left\">0.790</td><td align=\"left\">0.642</td></tr><tr><td align=\"left\">7_200_CBOW</td><td align=\"left\">7</td><td align=\"left\">200</td><td align=\"left\">CBOW</td><td align=\"left\">\n<bold>0.7331</bold>\n</td><td align=\"left\">0.8934</td><td align=\"left\">\n<bold>0.8054</bold>\n</td><td align=\"left\">0.590</td><td align=\"left\">0.822</td><td align=\"left\">0.687</td></tr><tr><td align=\"left\">7_200_skip</td><td align=\"left\">7</td><td align=\"left\">200</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.7062</td><td align=\"left\">0.8840</td><td align=\"left\">0.7852</td><td align=\"left\">0.521</td><td align=\"left\">0.774</td><td align=\"left\">0.623</td></tr><tr><td align=\"left\">7_300_CBOW</td><td align=\"left\">7</td><td align=\"left\">300</td><td align=\"left\">CBOW</td><td align=\"left\">0.7320</td><td align=\"left\">0.8933</td><td align=\"left\">0.8047</td><td align=\"left\">0.589</td><td align=\"left\">0.818</td><td align=\"left\">0.685</td></tr><tr><td align=\"left\">7_300_skip</td><td align=\"left\">7</td><td align=\"left\">300</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.7067</td><td align=\"left\">0.8833</td><td align=\"left\">0.7852</td><td align=\"left\">0.528</td><td align=\"left\">0.781</td><td align=\"left\">0.630</td></tr><tr><td align=\"left\">7_400_CBOW</td><td align=\"left\">7</td><td align=\"left\">400</td><td align=\"left\">CBOW</td><td align=\"left\">0.7163</td><td align=\"left\">0.8862</td><td align=\"left\">0.7922</td><td align=\"left\">0.554</td><td align=\"left\">0.798</td><td align=\"left\">0.654</td></tr><tr><td align=\"left\">7_400_skip</td><td align=\"left\">7</td><td align=\"left\">400</td><td align=\"left\">Skip-gram</td><td align=\"left\">0.6218</td><td align=\"left\">0.8859</td><td align=\"left\">0.7706</td><td align=\"left\">0.525</td><td align=\"left\">0.786</td><td align=\"left\">0.629</td></tr></tbody></table>\n"}, "text": "\tDevelopment set\tTest set\t \tParameters\tWin\tDim\tMethod\tPrecision\tRecall\tF-score\tPrecision\tRecall\tF-score\t \t5_200_CBOW\t5\t200\tCBOW\t0.7284\t0.8939\t0.8027\t0.594\t0.824\t0.690\t \t5_200_skip\t5\t200\tSkip-gram\t0.6821\t0.8812\t0.7690\t0.524\t0.783\t0.628\t \t5_300_CBOW\t5\t300\tCBOW\t0.7326\t0.8934\t0.8051\t0.576\t0.811\t0.674\t \t5_300_skip\t5\t300\tSkip-gram\t0.6836\t0.8813\t0.7699\t0.533\t0.787\t0.635\t \t5_400_CBOW\t5\t400\tCBOW\t0.7311\t0.8940\t0.8044\t0.568\t0.809\t0.667\t \t5_400_skip\t5\t400\tSkip-gram\t0.7164\t0.8878\t0.7929\t0.540\t0.790\t0.642\t \t7_200_CBOW\t7\t200\tCBOW\t0.7331\t0.8934\t0.8054\t0.590\t0.822\t0.687\t \t7_200_skip\t7\t200\tSkip-gram\t0.7062\t0.8840\t0.7852\t0.521\t0.774\t0.623\t \t7_300_CBOW\t7\t300\tCBOW\t0.7320\t0.8933\t0.8047\t0.589\t0.818\t0.685\t \t7_300_skip\t7\t300\tSkip-gram\t0.7067\t0.8833\t0.7852\t0.528\t0.781\t0.630\t \t7_400_CBOW\t7\t400\tCBOW\t0.7163\t0.8862\t0.7922\t0.554\t0.798\t0.654\t \t7_400_skip\t7\t400\tSkip-gram\t0.6218\t0.8859\t0.7706\t0.525\t0.786\t0.629\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 34227, "infons": {"file": "Tab5.xml", "id": "Tab5", "section_type": "TABLE", "type": "table_foot"}, "text": "The bold font denotes the best result for each column", "sentences": [], "annotations": [], "relations": []}, {"offset": 34281, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For selecting proper hyperparameters, we constructed the \u201cSSL-modified plant evidences\u201d model by applying different hyperparameters to the plant development set. Table\u00a05 shows a comparison of several hyperparameters. We selected the hyperparameters as window size = 7 and vector dimension = 200, and we used the CBOW method.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 34613, "infons": {"file": "12859_2017_1857_Fig5_HTML.jpg", "id": "Fig5", "section_type": "FIG", "type": "fig_caption"}, "text": "Performance comparisons of the proposed models for plant name normalization without the abbreviation resolution step. In (a), the light-green, dark-green, and red lines indicate the \u201cSL-only plant training data\u201d, \u201cSSL-only modified plant evidences\u201d, and \u201cSSL-modified plant evidences\u201d models, respectively. The x-axis represents the thresholds for ranks, and the y-axis indicates the recall of models for each rank. b The precision, recall, and F-scores are shown for the three models for plant name normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 35139, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We tested the models using the plant corpus, for which an abbreviation dictionary was not available. Figure\u00a05 shows the normalization results of 629 plant mentions from the plant test corpus. For plant normalization, \u201cSSL-modified plant evidences\u201d showed the best performance. Unlike the disease normalization result, \u201cSSL-only modified evidences\u201d was better than \u201cSL-only training data\u201d. Because an abbreviation dictionary was not available and plant names are usually represented by several types of names depending on their context, region, or language, plant name normalization showed lower accuracy compared to disease name normalization.  ", "sentences": [], "annotations": [], "relations": []}, {"offset": 35798, "infons": {"section_type": "DISCUSS", "type": "title_1"}, "text": "Discussion", "sentences": [], "annotations": [], "relations": []}, {"offset": 35809, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "In this study, we compared the proposed approach to DNorm for disease name normalization. In the BioCreative V challenge, DNorm was used as a baseline system in the disease named entity recognition and normalization (DNER) task, and the F-score was 0.806. Therefore, we further evaluated our approach using a data set in the DNER task. Because our approach contains only the normalization step, we assumed that we already knew the correct disease mentions in the test data set of the DNER task, and then, we measured the normalization performance. In the DNER task, Lee et al.\u2019s approach ranked first with an F-score of 0.865; their approach used dictionary-based normalization by using five dictionaries with priorities in the order of CDR development/training sets from a subset of the BioCreative V corpus, MEDIC, NCBI disease corpus, and MEDIC extension lexicon. When we re-evaluated their normalization approach after assuming that all disease names were correctly recognized, the F-score was 0.982. For the purpose of comparison, we used the same dictionaries, and then applied the \u201cSSL-modified evidences\u201d model with the following parameter values: window size = 5 and vector dimension = 300 for the skip-gram algorithm in Word2Vec. As a result, we obtained an F-score of 0.986. The performances of these two systems were similar with very high accuracies; this might be due to the high-quality dictionaries used, such as the CDR development/training sets and MEDIC. Therefore, after excluding dictionaries from the CDR development/training sets, MEDIC, and MEDIC extension lexicon and by using the NCBI disease corpus, we evaluated the two systems. Note that because we excluded MEDIC, we used the \u201cSSL-evidences\u201d model in this evaluation; training data was constructed using the NCBI disease corpus and unlabeled data was constructed using sentences containing disease names from PubMed. The F-scores of the dictionary-based approach and our approach reduced to 0.324 and 0.659, respectively. This shows the importance of high-quality dictionaries; at the same time, our system can achieve better performance even without good dictionaries.", "sentences": [], "annotations": [], "relations": []}, {"offset": 37969, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "In this study, we applied the Word2Vec algorithm for the word representation. In addition to Word2Vec, several studies for word representation have succeeded in capturing fine-grained semantic meanings. GloVe is an alternative model for learning word embeddings. For comparison with Word2Vec, we generated word embeddings using the GloVe algorithm with different sets of parameters, and used them for the normalization. As a result, we obtained an F-score of 0.639 for disease name normalization using GloVe trained with the same parameters as Word2Vec as follows: window size = 5, vector dimension = 300, and iteration = 1000. Compared to Table\u00a04, word embeddings obtained by Word2Vec outperformed those generated by Glove. Indeed, several studies showed that Word2Vec outperformed GloVe on word similarity tasks although GloVe achieved the best performance on the word analogical reasoning task. In addition to GloVe, Luong et al. proposed morphological recursive neural networks (RNNs) that combine RNNs and neural language models to learn word embeddings from morphemes. Wang et al. applied the bidirectional LSTM-RNN structure to represent word vectors, which outperformed CBOW and skip-gram approaches in Word2Vec when tested for the NER task. Thus, in the future work, we will improve the normalization performance by enhancing a neural network architecture for word representation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39360, "infons": {"section_type": "CONCL", "type": "title_1"}, "text": "Conclusions", "sentences": [], "annotations": [], "relations": []}, {"offset": 39372, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "In this study, we integrated training data and unlabeled data for word representation in entity name normalization and verified that the proposed normalization model is a useful tool for disease names and plant names. For many biological entities, there is no comprehensive dictionary; therefore, our approach will be useful for normalizing various entities.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39731, "infons": {"section_type": "CONCL", "type": "title_1"}, "text": "Additional file", "sentences": [], "annotations": [], "relations": []}, {"offset": 39747, "infons": {"section_type": "ABBR", "type": "title"}, "text": "Abbreviations", "sentences": [], "annotations": [], "relations": []}, {"offset": 39761, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "All abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 39775, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "All PubMed abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 39796, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "AR", "sentences": [], "annotations": [], "relations": []}, {"offset": 39799, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Androgen receptor", "sentences": [], "annotations": [], "relations": []}, {"offset": 39817, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "AS", "sentences": [], "annotations": [], "relations": []}, {"offset": 39820, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Angelman syndrome", "sentences": [], "annotations": [], "relations": []}, {"offset": 39838, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "AS", "sentences": [], "annotations": [], "relations": []}, {"offset": 39841, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "ankylosing spondylitis", "sentences": [], "annotations": [], "relations": []}, {"offset": 39864, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "BioCreative", "sentences": [], "annotations": [], "relations": []}, {"offset": 39876, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "The Critical Assessment of Information Extraction in Biology", "sentences": [], "annotations": [], "relations": []}, {"offset": 39937, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "C7 defects", "sentences": [], "annotations": [], "relations": []}, {"offset": 39948, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Complement component 7 deficiency", "sentences": [], "annotations": [], "relations": []}, {"offset": 39982, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "CBOW", "sentences": [], "annotations": [], "relations": []}, {"offset": 39987, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Continuous bag-of-word", "sentences": [], "annotations": [], "relations": []}, {"offset": 40010, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Entity-specific abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 40036, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Biological-entity-specific abstracts that contain at least one biological entity name in the abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 40139, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Evidence sentences", "sentences": [], "annotations": [], "relations": []}, {"offset": 40158, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "sentences that include at least one biological entity name in the sentence", "sentences": [], "annotations": [], "relations": []}, {"offset": 40233, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "f", "sentences": [], "annotations": [], "relations": []}, {"offset": 40235, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "F-score", "sentences": [], "annotations": [], "relations": []}, {"offset": 40243, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "FN", "sentences": [], "annotations": [], "relations": []}, {"offset": 40246, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "False negatives", "sentences": [], "annotations": [], "relations": []}, {"offset": 40262, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "FP", "sentences": [], "annotations": [], "relations": []}, {"offset": 40265, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "False positives", "sentences": [], "annotations": [], "relations": []}, {"offset": 40281, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "IAA", "sentences": [], "annotations": [], "relations": []}, {"offset": 40285, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Inter-annotator agreement", "sentences": [], "annotations": [], "relations": []}, {"offset": 40311, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "IDs", "sentences": [], "annotations": [], "relations": []}, {"offset": 40315, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Identifiers", "sentences": [], "annotations": [], "relations": []}, {"offset": 40327, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "LSTM", "sentences": [], "annotations": [], "relations": []}, {"offset": 40332, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Long short-term memory", "sentences": [], "annotations": [], "relations": []}, {"offset": 40355, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "MEDIC", "sentences": [], "annotations": [], "relations": []}, {"offset": 40361, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Merged disease vocabulary", "sentences": [], "annotations": [], "relations": []}, {"offset": 40387, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "MeSH", "sentences": [], "annotations": [], "relations": []}, {"offset": 40392, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Medical subject headings", "sentences": [], "annotations": [], "relations": []}, {"offset": 40417, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "ML", "sentences": [], "annotations": [], "relations": []}, {"offset": 40420, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Machine learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 40437, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Modified evidence sentences", "sentences": [], "annotations": [], "relations": []}, {"offset": 40465, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "A collection of \u201cevidence sentences\u201d and modified evidence sentences", "sentences": [], "annotations": [], "relations": []}, {"offset": 40538, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "NCBI", "sentences": [], "annotations": [], "relations": []}, {"offset": 40543, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "National center for biotechnology information", "sentences": [], "annotations": [], "relations": []}, {"offset": 40589, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "NER", "sentences": [], "annotations": [], "relations": []}, {"offset": 40593, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Named entity recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 40618, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "NLP", "sentences": [], "annotations": [], "relations": []}, {"offset": 40622, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Natural language processing", "sentences": [], "annotations": [], "relations": []}, {"offset": 40650, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "OMIM", "sentences": [], "annotations": [], "relations": []}, {"offset": 40655, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Online mendelian inheritance in man", "sentences": [], "annotations": [], "relations": []}, {"offset": 40691, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "p", "sentences": [], "annotations": [], "relations": []}, {"offset": 40693, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Precision", "sentences": [], "annotations": [], "relations": []}, {"offset": 40703, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "PCa", "sentences": [], "annotations": [], "relations": []}, {"offset": 40707, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Prostate cancer", "sentences": [], "annotations": [], "relations": []}, {"offset": 40723, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "r", "sentences": [], "annotations": [], "relations": []}, {"offset": 40725, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Recall", "sentences": [], "annotations": [], "relations": []}, {"offset": 40732, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "RNNs", "sentences": [], "annotations": [], "relations": []}, {"offset": 40737, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Recursive neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 40763, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "TP", "sentences": [], "annotations": [], "relations": []}, {"offset": 40766, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "True positives", "sentences": [], "annotations": [], "relations": []}, {"offset": 40781, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SL-only training data", "sentences": [], "annotations": [], "relations": []}, {"offset": 40803, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Supervised learning model with the training corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 40854, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SSL-all abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 40872, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Semi-supervised learning with unlabeled data of \u201call abstracts\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 40940, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SSL-entity abstracts", "sentences": [], "annotations": [], "relations": []}, {"offset": 40961, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Semi-supervised learning with unlabeled data of \u201centity-specific abstracts\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 41041, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SSL-evidences", "sentences": [], "annotations": [], "relations": []}, {"offset": 41055, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Semi-supervised learning with unlabeled data of \u201cevidence sentences\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 41128, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SSL-modified evidences", "sentences": [], "annotations": [], "relations": []}, {"offset": 41151, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Semi-supervised learning with unlabeled data of \u201cmodified evidence sentences\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 41233, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "SSL-only modified evidences", "sentences": [], "annotations": [], "relations": []}, {"offset": 41261, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Semi-supervised model that used only modified evidence sentences without the training corpus", "sentences": [], "annotations": [], "relations": []}, {"offset": 41354, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "UMLS", "sentences": [], "annotations": [], "relations": []}, {"offset": 41359, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Unified medical language system", "sentences": [], "annotations": [], "relations": []}, {"offset": 41391, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "VWS", "sentences": [], "annotations": [], "relations": []}, {"offset": 41395, "infons": {"section_type": "ABBR", "type": "paragraph"}, "text": "Van der Woude syndrome", "sentences": [], "annotations": [], "relations": []}, {"offset": 41418, "infons": {"section_type": "ABBR", "type": "footnote"}, "text": "Electronic supplementary material", "sentences": [], "annotations": [], "relations": []}, {"offset": 41452, "infons": {"section_type": "ABBR", "type": "footnote"}, "text": "The online version of this article (doi:10.1186/s12859-017-1857-8) contains supplementary material, which is available to authorized users.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41592, "infons": {"section_type": "ABBR", "type": "footnote"}, "text": "Availability of data and materials", "sentences": [], "annotations": [], "relations": []}, {"offset": 41627, "infons": {"section_type": "ABBR", "type": "footnote"}, "text": "The described plant corpus and the proposed model are freely available for non-commercial use and posted on http://gcancer.org/plant and http://gcancer.org/normalization, respectively.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41812, "infons": {"section_type": "AUTH_CONT", "type": "title"}, "text": "Authors\u2019 contributions", "sentences": [], "annotations": [], "relations": []}, {"offset": 41837, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "HL designed the project, HC and HL developed the algorithm and analyzed the results, and HC and WC constructed the corpus. All authors read and approved the final manuscript.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42012, "infons": {"section_type": "AUTH_CONT", "type": "title_1"}, "text": "Ethics approval and consent to participate", "sentences": [], "annotations": [], "relations": []}, {"offset": 42055, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "Not applicable", "sentences": [], "annotations": [], "relations": []}, {"offset": 42070, "infons": {"section_type": "AUTH_CONT", "type": "title_1"}, "text": "Consent for publication", "sentences": [], "annotations": [], "relations": []}, {"offset": 42094, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "Not applicable", "sentences": [], "annotations": [], "relations": []}, {"offset": 42109, "infons": {"section_type": "COMP_INT", "type": "title_1"}, "text": "Competing interests", "sentences": [], "annotations": [], "relations": []}, {"offset": 42129, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare that they have no competing interests.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42188, "infons": {"section_type": "COMP_INT", "type": "title_1"}, "text": "Publisher\u2019s Note", "sentences": [], "annotations": [], "relations": []}, {"offset": 42207, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42326, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 42337, "infons": {"name_0": "surname:Tjong Kim Sang;given-names:EF", "name_1": "surname:De Meulder;given-names:F", "section_type": "REF", "source": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003-Volume 4", "type": "ref", "year": "2003"}, "text": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 42427, "infons": {"fpage": "130", "lpage": "50", "name_0": "surname:Hachey;given-names:B", "name_1": "surname:Radford;given-names:W", "name_2": "surname:Nothman;given-names:J", "name_3": "surname:Honnibal;given-names:M", "name_4": "surname:Curran;given-names:JR", "pub-id_doi": "10.1016/j.artint.2012.04.005", "section_type": "REF", "source": "Artif Intell", "type": "ref", "volume": "194", "year": "2013"}, "text": "Evaluating entity linking with wikipedia", "sentences": [], "annotations": [], "relations": []}, {"offset": 42468, "infons": {"name_0": "surname:Luo;given-names:G", "name_1": "surname:Huang;given-names:X", "name_2": "surname:Lin;given-names:CY", "name_3": "surname:Nie;given-names:Z", "section_type": "REF", "source": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing at Lisbon, Portugal, 17-21 September 2015", "type": "ref", "year": "2015"}, "text": "Joint named entity recognition and disambiguation", "sentences": [], "annotations": [], "relations": []}, {"offset": 42518, "infons": {"section_type": "REF", "type": "ref"}, "text": "Arnold S, Dziuba R, L\u00f6ser A. Tasty: Interactive entity linking as-you-type. In: Proceedings of the 26th International Conference on Computational Linguistics: System Demonstrations at Osaka, Japan, December 11-17 2016.2016. p. 111\u20135.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42755, "infons": {"fpage": "28", "lpage": "37", "name_0": "surname:Leaman;given-names:R", "name_1": "surname:Khare;given-names:R", "name_2": "surname:Lu;given-names:Z", "pub-id_doi": "10.1016/j.jbi.2015.07.010", "pub-id_pmid": "26187250", "section_type": "REF", "source": "J Biomed Inform", "type": "ref", "volume": "57", "year": "2015"}, "text": "Challenges in clinical natural language processing for automated disorder normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 42843, "infons": {"fpage": "1", "issue": "1", "name_0": "surname:Neves;given-names:ML", "name_1": "surname:Carazo;given-names:JM", "name_2": "surname:Pascual-Montano;given-names:A", "pub-id_doi": "10.1186/1471-2105-11-157", "pub-id_pmid": "20043860", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "11", "year": "2010"}, "text": "Moara: a java library for extracting and normalizing gene and protein mentions", "sentences": [], "annotations": [], "relations": []}, {"offset": 42922, "infons": {"fpage": "1", "issue": "8", "name_0": "surname:Wei;given-names:CH", "name_1": "surname:Kao;given-names:HY", "pub-id_pmid": "21199577", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "12", "year": "2011"}, "text": "Cross-species gene normalization by species inference", "sentences": [], "annotations": [], "relations": []}, {"offset": 42976, "infons": {"fpage": "2769", "issue": "19", "lpage": "71", "name_0": "surname:Hakenberg;given-names:J", "name_1": "surname:Gerner;given-names:M", "name_2": "surname:Haeussler;given-names:M", "name_3": "surname:Solt;given-names:I", "name_4": "surname:Plake;given-names:C", "name_5": "surname:Schroeder;given-names:M", "name_6": "surname:Gonzalez;given-names:G", "name_7": "surname:Nenadic;given-names:G", "name_8": "surname:Bergman;given-names:CM", "pub-id_doi": "10.1093/bioinformatics/btr455", "pub-id_pmid": "21813477", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "27", "year": "2011"}, "text": "The gnat library for local and remote gene mention normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 43041, "infons": {"fpage": "1633", "issue": "12", "lpage": "40", "name_0": "surname:Rockt\u00e4schel;given-names:T", "name_1": "surname:Weidlich;given-names:M", "name_2": "surname:Leser;given-names:U", "pub-id_doi": "10.1093/bioinformatics/bts183", "pub-id_pmid": "22500000", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "28", "year": "2012"}, "text": "Chemspot: a hybrid system for chemical named entity recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 43105, "infons": {"section_type": "REF", "type": "ref"}, "text": "Wei CH, Peng Y, Leaman R, Davis AP, Mattingly CJ, Li J, Wiegers TC, Lu Z. Overview of the biocreative v chemical disease relation (cdr) task. In: Proceedings of the Fifth BioCreative Challenge Evaluation Workshop.Sevilla: 2015. p. 154\u201366.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43346, "infons": {"fpage": "14", "issue": "Suppl 1", "name_0": "surname:Hanisch;given-names:D", "name_1": "surname:Fundel;given-names:K", "name_2": "surname:Mevissen;given-names:HT", "name_3": "surname:Zimmer;given-names:R", "name_4": "surname:Fluck;given-names:J", "pub-id_doi": "10.1186/1471-2105-6-S1-S14", "pub-id_pmid": "15663789", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "6", "year": "2005"}, "text": "Prominer: rule-based protein and gene entity recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 43403, "infons": {"section_type": "REF", "type": "ref"}, "text": "Aronson AR. Effective mapping of biomedical text to the umls metathesaurus: the metamap program. In: Proceedings of the AMIA Symposium. American Medical Informatics Association: 2001. p. 17.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43594, "infons": {"fpage": "1", "issue": "1", "name_0": "surname:Campos;given-names:D", "name_1": "surname:Matos;given-names:S", "name_2": "surname:Oliveira;given-names:JL", "pub-id_doi": "10.1186/1471-2105-14-1", "pub-id_pmid": "23323762", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "14", "year": "2013"}, "text": "Gimli: open source and high-performance biomedical name recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 43662, "infons": {"fpage": "1", "issue": "1", "name_0": "surname:Campos;given-names:D", "name_1": "surname:Matos;given-names:S", "name_2": "surname:Oliveira;given-names:JL", "pub-id_doi": "10.1186/1471-2105-14-1", "pub-id_pmid": "23323762", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "14", "year": "2013"}, "text": "A modular framework for biomedical concept recognition", "sentences": [], "annotations": [], "relations": []}, {"offset": 43717, "infons": {"section_type": "REF", "type": "ref"}, "text": "Lee HC, Hsu YY, Kao HY. An enhanced crf-based system for disease name entity recognition and normalization on biocreative v dner task. In: Proceedings of the Fifth BioCreative Challenge Evaluation Workshop. Sevilla: 2015. p. 226\u201333.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43952, "infons": {"fpage": "1", "issue": "1", "name_0": "surname:Leaman;given-names:R", "name_1": "surname:Wei;given-names:CH", "name_2": "surname:Lu;given-names:Z", "pub-id_doi": "10.1186/s13321-014-0049-z", "section_type": "REF", "source": "J Cheminformatics", "type": "ref", "volume": "7", "year": "2015"}, "text": "tmchem: a high performance approach for chemical named entity recognition and normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 44044, "infons": {"fpage": "2909", "issue": "22", "lpage": "17", "name_0": "surname:Leaman;given-names:R", "name_1": "surname:Do\u011fan;given-names:RI", "name_2": "surname:Lu;given-names:Z", "pub-id_doi": "10.1093/bioinformatics/btt474", "pub-id_pmid": "23969135", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "29", "year": "2013"}, "text": "Dnorm: disease name normalization with pairwise learning to rank", "sentences": [], "annotations": [], "relations": []}, {"offset": 44109, "infons": {"fpage": "1", "lpage": "10", "name_0": "surname:Do\u011fan;given-names:RI", "name_1": "surname:Leaman;given-names:R", "name_2": "surname:Lu;given-names:Z", "pub-id_doi": "10.1016/j.jbi.2013.12.006", "pub-id_pmid": "24393765", "section_type": "REF", "source": "J Biomed Inform", "type": "ref", "volume": "47", "year": "2014"}, "text": "Ncbi disease corpus: a resource for disease name recognition and concept normalization", "sentences": [], "annotations": [], "relations": []}, {"offset": 44196, "infons": {"fpage": "1", "issue": "3", "name_0": "surname:Rumelhart;given-names:DE", "name_1": "surname:Hinton;given-names:GE", "name_2": "surname:Williams;given-names:RJ", "section_type": "REF", "source": "Cogn Model", "type": "ref", "volume": "5", "year": "1988"}, "text": "Learning representations by back-propagating errors", "sentences": [], "annotations": [], "relations": []}, {"offset": 44248, "infons": {"fpage": "625", "issue": "Feb", "lpage": "60", "name_0": "surname:Erhan;given-names:D", "name_1": "surname:Bengio;given-names:Y", "name_2": "surname:Courville;given-names:A", "name_3": "surname:Manzagol;given-names:PA", "name_4": "surname:Vincent;given-names:P", "name_5": "surname:Bengio;given-names:S", "section_type": "REF", "source": "J Mach Learn Res", "type": "ref", "volume": "11", "year": "2010"}, "text": "Why does unsupervised pre-training help deep learning?", "sentences": [], "annotations": [], "relations": []}, {"offset": 44303, "infons": {"section_type": "REF", "type": "ref"}, "text": "Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C. Neural architectures for named entity recognition. arXiv preprint arXiv:1603.01360. 2016.", "sentences": [], "annotations": [], "relations": []}, {"offset": 44453, "infons": {"section_type": "REF", "type": "ref"}, "text": "Ma X, Hovy E. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354. 2016.", "sentences": [], "annotations": [], "relations": []}, {"offset": 44569, "infons": {"fpage": "065", "name_0": "surname:Davis;given-names:AP", "name_1": "surname:Wiegers;given-names:TC", "name_2": "surname:Rosenstein;given-names:MC", "name_3": "surname:Mattingly;given-names:CJ", "section_type": "REF", "source": "Database", "type": "ref", "volume": "2012", "year": "2012"}, "text": "Medic: a practical disease vocabulary used at the comparative toxicogenomics database", "sentences": [], "annotations": [], "relations": []}, {"offset": 44655, "infons": {"fpage": "136", "issue": "D1", "lpage": "43", "name_0": "surname:Federhen;given-names:S", "pub-id_doi": "10.1093/nar/gkr1178", "section_type": "REF", "source": "Nucleic Acids Res", "type": "ref", "volume": "40", "year": "2012"}, "text": "The ncbi taxonomy database", "sentences": [], "annotations": [], "relations": []}, {"offset": 44682, "infons": {"fpage": "38", "issue": "suppl 1", "lpage": "51", "name_0": "surname:Sayers;given-names:EW", "name_1": "surname:Barrett;given-names:T", "name_2": "surname:Benson;given-names:DA", "name_3": "surname:Bolton;given-names:E", "name_4": "surname:Bryant;given-names:SH", "name_5": "surname:Canese;given-names:K", "name_6": "surname:Chetvernin;given-names:V", "name_7": "surname:Church;given-names:DM", "name_8": "surname:DiCuccio;given-names:M", "name_9": "surname:Federhen;given-names:S", "pub-id_doi": "10.1093/nar/gkq1172", "section_type": "REF", "source": "Nucleic Acids Res", "type": "ref", "volume": "39", "year": "2011"}, "text": "Database resources of the national center for biotechnology information", "sentences": [], "annotations": [], "relations": []}, {"offset": 44754, "infons": {"fpage": "W518", "issue": "W1", "lpage": "22", "name_0": "surname:Wei;given-names:CH", "name_1": "surname:Kao;given-names:HY", "name_2": "surname:Lu;given-names:Z", "pub-id_doi": "10.1093/nar/gkt441", "pub-id_pmid": "23703206", "section_type": "REF", "source": "Nucleic Acids Res", "type": "ref", "volume": "41", "year": "2013"}, "text": "Pubtator: a web-based text mining tool for assisting biocuration", "sentences": [], "annotations": [], "relations": []}, {"offset": 44819, "infons": {"section_type": "REF", "type": "ref"}, "text": "Leaman R, Gonzalez G, et al.Banner: an executable survey of advances in biomedical named entity recognition. In: Pacific Symposium on Biocomputing. vol. 13. Big Island: 2008. p. 652\u201363.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45007, "infons": {"section_type": "REF", "type": "ref"}, "text": "Baldwin B, Carpenter B. LingPipe. In: Available from World Wide Web: http://alias-i.com/lingpipe: 2003.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45111, "infons": {"section_type": "REF", "type": "ref"}, "text": "Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 2013.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45249, "infons": {"name_0": "surname:McCandless;given-names:M", "name_1": "surname:Hatcher;given-names:E", "name_2": "surname:Gospodnetic;given-names:O", "section_type": "REF", "source": "Lucene in Action: Covers Apache Lucene 3.0", "type": "ref", "year": "2010"}, "text": "", "sentences": [], "annotations": [], "relations": []}, {"offset": 45250, "infons": {"section_type": "REF", "type": "ref"}, "text": "Shmanina T, Zukerman I, Yepes AJ, Cavedon L, Verspoor K. Impact of corpus diversity and complexity on ner performance. In: Proceedings of Australasian Language Technology Association Workshop. Brisbane: 2013. p. 91\u20135.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45470, "infons": {"fpage": "85", "issue": "1", "name_0": "surname:Gerner;given-names:M", "name_1": "surname:Nenadic;given-names:G", "name_2": "surname:Bergman;given-names:CM", "pub-id_doi": "10.1186/1471-2105-11-85", "pub-id_pmid": "20149233", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "11", "year": "2010"}, "text": "Linnaeus: a species name identification system for biomedical literature", "sentences": [], "annotations": [], "relations": []}, {"offset": 45543, "infons": {"fpage": "296", "issue": "2", "lpage": "8", "name_0": "surname:Rebholz-Schuhmann;given-names:D", "name_1": "surname:Arregui;given-names:M", "name_2": "surname:Gaudan;given-names:S", "name_3": "surname:Kirsch;given-names:H", "name_4": "surname:Jimeno;given-names:A", "pub-id_doi": "10.1093/bioinformatics/btm557", "pub-id_pmid": "18006544", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "24", "year": "2008"}, "text": "Text processing through web services: calling whatizit", "sentences": [], "annotations": [], "relations": []}, {"offset": 45598, "infons": {"section_type": "REF", "type": "ref"}, "text": "Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). Doha: 2014. p. 1532\u201343.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45807, "infons": {"section_type": "REF", "type": "ref"}, "text": "Ghannay S, Favre B, Esteve Y, Camelin N. Word Embedding Evaluation and Combination. In: LREC.2016.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45906, "infons": {"section_type": "REF", "type": "ref"}, "text": "Wohlgenannt G, Chernyak E, Ilvovsky D. Extracting social networks from literary text with word embedding tools. In: COLING 2016 Organizing Committee. Osaka: 2016. p. 18\u201325.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46081, "infons": {"section_type": "REF", "type": "ref"}, "text": "Luong T, Socher R, Manning CD. Better word representations with recursive neural networks for morphology. In: CoNLL. Sofia: 2013. p. 104\u201313.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46224, "infons": {"section_type": "REF", "type": "ref"}, "text": "Wang P, Qian Y, Soong FK, He L, Zhao H. Learning distributed word representations for bidirectional lstm recurrent neural network. In: Proceedings of NAACL-HLT. San Diego: 2016. p. 527\u201333.", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]