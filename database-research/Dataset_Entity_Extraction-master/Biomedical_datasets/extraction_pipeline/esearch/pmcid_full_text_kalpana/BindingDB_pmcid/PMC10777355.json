[{"source": "PMC", "date": "20240203", "key": "pmc.key", "infons": {}, "documents": [{"id": "10777355", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.1093/bioinformatics/btad778", "article-id_pmc": "10777355", "article-id_pmid": "38141210", "article-id_publisher-id": "btad778", "elocation-id": "btad778", "issue": "1", "license": "This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.", "name_0": "surname:Li;given-names:Zongquan", "name_1": "surname:Ren;given-names:Pengxuan", "name_2": "surname:Yang;given-names:Hao", "name_3": "surname:Zheng;given-names:Jie", "name_4": "surname:Bai;given-names:Fang", "name_5": "surname:Wren;given-names:Jonathan", "section_type": "TITLE", "type": "front", "volume": "40", "year": "2023"}, "text": "TEFDTA: a transformer encoder and fingerprint representation combined prediction method for bonded and non-bonded drug\u2013target affinities", "sentences": [], "annotations": [], "relations": []}, {"offset": 139, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Abstract", "sentences": [], "annotations": [], "relations": []}, {"offset": 148, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Motivation", "sentences": [], "annotations": [], "relations": []}, {"offset": 159, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The prediction of binding affinity between drug and target is crucial in drug discovery. However, the accuracy of current methods still needs to be improved. On the other hand, most deep learning methods focus only on the prediction of non-covalent (non-bonded) binding molecular systems, but neglect the cases of covalent binding, which has gained increasing attention in the field of drug development.", "sentences": [], "annotations": [], "relations": []}, {"offset": 563, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Results", "sentences": [], "annotations": [], "relations": []}, {"offset": 571, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "In this work, a new attention-based model, A Transformer Encoder and Fingerprint combined Prediction method for Drug\u2013Target Affinity (TEFDTA) is proposed to predict the binding affinity for bonded and non-bonded drug\u2013target interactions. To deal with such complicated problems, we used different representations for protein and drug molecules, respectively. In detail, an initial framework was built by training our model using the datasets of non-bonded protein\u2013ligand interactions. For the widely used dataset Davis, an additional contribution of this study is that we provide a manually corrected Davis database. The model was subsequently fine-tuned on a smaller dataset of covalent interactions from the CovalentInDB database to optimize performance. The results demonstrate a significant improvement over existing approaches, with an average improvement of 7.6% in predicting non-covalent binding affinity and a remarkable average improvement of 62.9% in predicting covalent binding affinity compared to using BindingDB data alone. At the end, the potential ability of our model to identify activity cliffs was investigated through a case study. The prediction results indicate that our model is sensitive to discriminate the difference of binding affinities arising from small variances in the structures of compounds.", "sentences": [], "annotations": [], "relations": []}, {"offset": 1903, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Availability and implementation", "sentences": [], "annotations": [], "relations": []}, {"offset": 1935, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The codes and datasets of TEFDTA are available at https://github.com/lizongquan01/TEFDTA.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2025, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "1 Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 2040, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In the realm of drug research and development, predicting drug\u2013target interactions/affinities (DTI/DTA) is an indispensable component. In early stages, researchers determined these interactions through experiments, which is time-consuming and costly. Subsequently, with advancements of computer technology, researchers started utilizing computers to predict drug\u2013target interactions and simulate the binding poses of drugs and targets using docking programs, such as GLIDE, Molegro Virtual Docker. However, this docking method also has corresponding limitations, i.e. the docking process also takes a long time to do the computation and requires the three-dimensional structures of proteins.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2736, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "With the development of machine learning and deep learning, researchers have attempted to incorporate these fields to DTI. Kronrls and SimBoost use similarity matrix to predict DTA. In recent deep learning approaches for prediction DTA, tasks are divided into two categories: binary classification and regression. The binary classification primarily aims to determine whether there is or a strong enough interaction between drugs and targets, while the regression task is more inclined to determine the strength of the combination between molecules and protein targets, which is undoubtedly more challenging.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3345, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "At present, deep-learning based methods have been extensively used. An advantage of these methods is their capability to automatically extract features. However, the initial input data, particularly the data description of proteins and small molecules, significantly influences the model's performance. Pafnucy constructs a 20\u2009\u00d7\u200920\u2009\u00d7\u200920 \u00c53 box centered as the geometric center of a binding ligand, and then discretized the positions of heavy atoms into a lattice with a bin size of 1\u2009\u00c5. RosENet combines voxelized molecular mechanics energies and molecular descriptors as input for an ensemble of three-dimensional (3D) convolutional neural networks (CNNs). OnionNet draws shells with each atom of small molecules as the center, incrementing by \u03b4 as the radius, resembling layers of onions. Subsequently, it gathers all pairs between the atoms of the central molecule and the protein atoms within the shell, utilizing them as inputs. Kdeep voxelizes the whole protein, while PointNet and PointTransformer convert the whole protein into a point cloud and take the coordinates as the input. The majority of the aforementioned models leverage CNN architectures. Masif utilizes the geometric information from the protein surface as its input and uses the GNN to perform training. These approaches consider the static 3D structural information of the protein. However, the protein structure undergoes dynamic changes. Presently, our data mostly contain singular conformation of the protein, prompting the need to assess whether relying on a single pose impacts practical results. Furthermore, not all proteins have their structures determined to date.", "sentences": [], "annotations": [], "relations": []}, {"offset": 5007, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Consequently, there are numerous advanced methods that do not directly incorporate the structure of the target protein into their inputs. DeepDTA utilizes the FASTA sequence format to represent proteins, mapping each element to an amino acid, and uses the SMILES string format to characterize small molecules. DeepCDA improves the feature extraction process of DeepDTA and utilizes LSTM to further extract features from proteins and drug molecules. However, these methods do not take into account the spatial structure of small molecules. GraphDTA and DeepGS use the two-dimensional structures of small molecules, which contain the bonding information between atoms. MolTrans and TransformerCPI also use sequence data, using Transformer (an excellent model that works well on language translation), to predict drug\u2013target binding affinity. The authors of TransformerCPI have continually made notable progress in their research endeavors, leading to the development of TransformerCPI2.0, which is a competitive sequence-based model compared to 3D structure-based methods like molecular docking. MGraphDTA introduces a multi-scale graph neural network that not only preserves the ability to extract local features but also enables the capturing of global structural information in compounds. SAM-DTA treats different proteins separately but trains them together in a multi-head strategy, which differs from the traditional models that process protein and ligand inputs separately and merge them later. DataDTA integrates multi-scale interaction information and achieves impressive predictive accuracy for binding affinity by using the dual interaction aggregation neural network strategy. However, it is worth noting that most of the aforementioned methods were primarily developed for predicting non-covalent binding affinity, and there is currently no deep learning method specifically designed for predicting covalent binding affinity.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6946, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In this paper, we present a novel model for predicting both covalent (bonded) and non-covalent (non-bonded) binding affinities in drug-protein interactions, called Fingerprint Encoder DTA (TEFDTA). Our model TEFDTA, draws inspiration from two existing models, DeepDTA and TransformerCPI. DeepDTA offers a way to extract features from sequences using 1D-CNN (1-Dimensional Convolutional Neural Network). This model focuses on extracting local pattern features from sequence information to facilitate feature extraction. While Recurrent Neural Networks (RNN) can also process one-dimensional inputs and perform feature extraction, they suffer from certain limitations. CNN, on the other hand, has constraints in capturing global features effectively. RNN, despite being able to process the entire sequence with network propagation, encounters the problem of forgetting information over time. TransformerCPI demonstrates that Transformers can effectively address the issues present in both CNN and RNN models. Transformers are built upon an encoder and a decoder. In light of this, we utilize the Transformer as a feature extractor to distill complex molecular sequences. It is important to note that a single encoder is sufficient for this task, as more complex models would require significantly longer training times due to potential convergence difficulties, without necessarily enhancing information extraction. The main contributions of this paper are concluded as follows.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8423, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "We propose a module that can extract features of protein and drug molecules separately to improve the accuracy of the model in modeling protein molecular interaction problems. Resulting in near state-of-the-art performance on the public datasets Davis and KIBA.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8685, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "We take the lead on predicting covalent binding affinity using deep learning methods, and experiments have shown that the prediction of covalent binding affinity still has certain accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8875, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "2 Materials and methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 8899, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "2.1 Datasets", "sentences": [], "annotations": [], "relations": []}, {"offset": 8912, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Our DTI model was trained and tested with three non-covalent drug target binding benchmark databases (details as below), KIBA, Davis, and BindingDB, which are widely used for DTI tasks. We then fine-tuned the model with the covalent database CovalentInDB based on the model trained with BindingDB.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9210, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "KIBA is the abbreviation for Kinase Inhibitor Bio-Activity, which combines three biochemical analysis indices Ki (inhibitory constant), Kd (dissociation constant) and IC50 (half maximal inhibitory concentration). The KIBA database contains 52\u00a0498 drugs and 467 targets, and there are 246\u00a0088 KIBA scores in total. However, in SimBoost, the database was filtered. The filtered KIBA database has a total of 2111 drugs and 229 target proteins, with a total of 118\u00a0254 scores for drug\u2013target bioactivities.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9718, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The database proposed by is mainly contains the bioactivity data in the format of Kd, with a total of 442 proteins and 68 drugs, and a total of 30\u00a0056 values. In order to make our model comparable with the results of other models, we transform Kd value to pKd [Equation (1)]. ", "sentences": [], "annotations": [], "relations": []}, {"offset": 9996, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In addition, we found that the sequence information of some proteins was not correctly recorded in the Davis database, which is provided by DeepDTA and serves as a benchmark dataset for DTA tasks. This issue further affects the accuracy of models that utilize incorrect data. In particular, mutant amino acid sequences were still represented with the wide type, which could also affect the accuracy of our prediction. Therefore, we corrected these sequences to ensure their accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10480, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "BindingDB is a public, web-accessible database of measured binding affinities. It was launched on the web in 2000 and currently contains about 2.7 million binding data for 9000 protein targets and 1.2 million small molecules. After data cleaning, including the removal of data with unclear binding affinity data and duplicate data in the database, the remaining dataset contains 80\u00a0324 compound molecules and 5561 proteins, with a total of 1\u00a0254\u00a0402 interaction data. The summary of the non-covalent datasets, and how we divide it into training set, validation set and test set are shown in the Table\u00a01.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11088, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "CovalentInDB is a comprehensive database for covalent inhibitors and their targets. It contains not only the basic information of covalent inhibitors such as warheads, reaction mechanisms, and binding sites, but also the experimental data of covalent binding affinity, i.e. IC50, Kd and Ki. The dataset contains 4511 covalent inhibitors (including 68 approved drugs) with 57 different reactive warheads for 280 protein targets. We performed a statistical analysis of our covalent data. The six most abundant targets are Kinase, Acyltransferase, Aminopeptidase, Hydrolase, Oxidoreductase, and Protease. The remaining types were categorized into a class called others, as illustrated in Fig.\u00a01A. The ratio of each type of target was marked in the figure, with the largest proportion being Kinase (61%), followed by Protease, which accounts for about 13.2%. The types of the residue types involved in the covalent reaction have been clustered, and the results show that the observed residue types with the highest percentage are cysteine (66.5%), serine (18.7%), histidine (5.2%), and tyrosine (4.7%), as illustrated in Fig.\u00a01B. The largest portion of the types of ligand warheads is michael acceptor (44.9%), followed by the types of carbonyl (5.2%), nitrile (5%), etc., as illustrated in Fig.\u00a01C. We then performed fine-tuning based on each specific warhead type.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12454, "infons": {"file": "btad778f1.jpg", "id": "btad778-F1", "section_type": "FIG", "type": "fig_caption"}, "text": "The pie charts present the distribution analysis of covalent data used for finetuning. (A) Proportion analysis of target types in the covalent dataset. (B) Proportion analysis of covalent residue types in the dataset. (C) Proportion analysis of covalent warhead types in the dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12738, "infons": {"file": "btad778f2.jpg", "id": "btad778-F2", "section_type": "FIG", "type": "fig_caption"}, "text": "The framework of the designed module TEFDTA for drug\u2013protein binding affinity prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12830, "infons": {"file": "btad778-T1.xml", "id": "btad778-T1", "section_type": "TABLE", "type": "table_caption"}, "text": "Statistical analysis of benchmark datasets and the division of training, validation, and test sets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12930, "infons": {"file": "btad778-T1.xml", "id": "btad778-T1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Dataset</th><th rowspan=\"1\" colspan=\"1\">No. of compounds</th><th rowspan=\"1\" colspan=\"1\">No. of proteins</th><th rowspan=\"1\" colspan=\"1\">No. of interactions</th><th colspan=\"2\" rowspan=\"1\">Training set</th><th rowspan=\"1\" colspan=\"1\">Validation set</th><th rowspan=\"1\" colspan=\"1\">Test set</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">KIBA</td><td rowspan=\"1\" colspan=\"1\">2111</td><td rowspan=\"1\" colspan=\"1\">229</td><td colspan=\"2\" rowspan=\"1\">118\u00a0254</td><td rowspan=\"1\" colspan=\"1\">78\u00a0836</td><td rowspan=\"1\" colspan=\"1\">19\u00a0709</td><td rowspan=\"1\" colspan=\"1\">19\u00a0709</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Davis</td><td rowspan=\"1\" colspan=\"1\">68</td><td rowspan=\"1\" colspan=\"1\">442</td><td colspan=\"2\" rowspan=\"1\">30\u00a0056</td><td rowspan=\"1\" colspan=\"1\">20\u00a0037</td><td rowspan=\"1\" colspan=\"1\">5009</td><td rowspan=\"1\" colspan=\"1\">5010</td></tr><tr><td rowspan=\"1\" colspan=\"1\">BindingDB</td><td rowspan=\"1\" colspan=\"1\">803\u00a0234</td><td rowspan=\"1\" colspan=\"1\">5561</td><td colspan=\"2\" rowspan=\"1\">1\u00a0254\u00a0402</td><td rowspan=\"1\" colspan=\"1\">1\u00a0172\u00a0682</td><td rowspan=\"1\" colspan=\"1\">81\u00a0720</td><td rowspan=\"1\" colspan=\"1\">20\u00a0001</td></tr></tbody></table>\n"}, "text": "Dataset\tNo. of compounds\tNo. of proteins\tNo. of interactions\tTraining set\tValidation set\tTest set\t \tKIBA\t2111\t229\t118\u00a0254\t78\u00a0836\t19\u00a0709\t19\u00a0709\t \tDavis\t68\t442\t30\u00a0056\t20\u00a0037\t5009\t5010\t \tBindingDB\t803\u00a0234\t5561\t1\u00a0254\u00a0402\t1\u00a0172\u00a0682\t81\u00a0720\t20\u00a0001\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 13187, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "2.2 Model Architecture", "sentences": [], "annotations": [], "relations": []}, {"offset": 13210, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To enhance the information extraction from drug and protein sequences, we have developed a novel framework called Fingerprint Encoder DTA, or TEFDTA for short. First, our model obtains FASTA sequence information of the protein and the SMILES of the drug molecule, and extracts the corresponding features. Proteins are performed label encoding and put into the embedding layer. Obtained features are fed into CNN blocks consisting of 1D-CNN layers. On the other hand, the drug molecules are converted from the SMILES format into a fingerprint type known as MACCS. These fingerprints are then fed into an embedding layer where position encoding is performed. Following this, the drug molecules' features are successfully extracted through the encoder module in the transformer. Once the features of the protein and drug molecule have been extracted, we concatenate the two representation vectors and input them into a fully connected layer. Finally, the model predicts the binding affinity score for a pair of ligand-protein. See Fig.\u00a02 for the framework of the model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14278, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "2.3 Design of the input of the network architecture", "sentences": [], "annotations": [], "relations": []}, {"offset": 14330, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "2.3.1 Representation of compounds", "sentences": [], "annotations": [], "relations": []}, {"offset": 14364, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The MACCS key is a binary fingerprint consisting of 166\u2009bits. Each bit position represents the presence 1 or absence 0 of a predefined structural feature in the drug molecule. Through our research, we have discovered that using the MACCS key as input allows for better extraction of drug molecule features compared to using the SMILES representation. This improvement can be attributed to the MACCS fingerprint's ability to capture substructure information of compounds, thereby providing more informative input for the model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14893, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Furthermore, using the MACCS fingerprint offers an additional advantage. The sequence lengths of most drug molecules are not uniform, which would require padding the sequences with zeros when using the SMILES representation. However, by utilizing the fingerprint, we can avoid this step altogether, as the fingerprint has a fixed length and does not require padding.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15260, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "2.3.2 Representation of proteins", "sentences": [], "annotations": [], "relations": []}, {"offset": 15293, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In our approach, we use integer encoding to represent amino acids in the protein sequences. We have defined 25 amino acids, and each amino acid is assigned a corresponding integer value. (e.g. \u201cA\u201d: 1, \u201cC\u201d: 2, \u201cB\u201d: 3, \u201cE\u201d: 4, etc.). The label encoding for an example of FASTA, \u201cMAAVIL\u201d is given below:", "sentences": [], "annotations": [], "relations": []}, {"offset": 15614, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "[M A V I L] = [11 1 1 22 8 12]", "sentences": [], "annotations": [], "relations": []}, {"offset": 15645, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "After encoding the drug molecules and protein sequences, the resulting vectors are passed through an embedding layer to generate a matrix or feature map. For drug molecule vectors, the transformed matrix is denoted as. Where  represents the sequence length of drug molecules and  represents the embedding dimension. For protein vectors, the transformed matrix is. Where  stands for the sequence length of the proteins, with a cutoff set at 1000.  represents the embedding dimension. Specifically, the representation of drug molecules undergoes a step called positional encoding. Although Transformers can effectively address the limitations of CNN and RNN in processing sequential information, they are unable to distinguish the same character at different positions. To overcome this, we apply positional encoding. For each different position, there is a separate feature map, which has the same dimension as the feature map of molecules. Finally, after the position encoding is completed by simple addition, it is entered into the transformer block .", "sentences": [], "annotations": [], "relations": []}, {"offset": 16698, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "2.3.3 Transformer encoder block", "sentences": [], "annotations": [], "relations": []}, {"offset": 16730, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For the encoder in the transformer, detailed description can be seen in. Briefly, the molecular feature map  is fed into a module called the multi-head attention block, which is structured as follows: first, Q, K, and V are calculated:  are query, key, and value, and generated based on the projection of the original feature map.  are projection matrices that need to be used to compute. Then do self-attention operation on Q, K, and V matrices, it is formulated as: ", "sentences": [], "annotations": [], "relations": []}, {"offset": 17199, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For , we split the dimension . In this work, we use h(head) = 8 and  = 256, so that  is formulated as a dimension in each attention operation as : ", "sentences": [], "annotations": [], "relations": []}, {"offset": 17347, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "When we extract different features from different dimensions of the input feature map, and finally concatenate them together, we project them by  to the same dimension as the original one. Adding and normalizing stand for residual connection and layer normalization respectively, which are not shown here, and feed forward is a simple MLP. Finally, the output of the transformer-encoder block has the same dimension as the input. However, the features of the drug molecules were extracted.", "sentences": [], "annotations": [], "relations": []}, {"offset": 17837, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "2.3.4 CNN block", "sentences": [], "annotations": [], "relations": []}, {"offset": 17853, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To address the memory overhead and increased parameter requirements associated with using the transformer's encoder for training due to the long length of protein's FASTA sequence, we opted for the same 1D-CNN structure as DeepDTA to extract protein features. For the input protein feature map, like 3D CNN used in the image field, we have a one-dimensional convolution kernel with a fixed field of view h and a size of , which detects and extracts the features of h amino acids according to the model. After a convolution operation, we will get a new protein feature representation. After repeating the convolution for three times, the final protein representation  is obtained by a Max pooling layer. For simplicity, we consider it as .", "sentences": [], "annotations": [], "relations": []}, {"offset": 18592, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "After obtaining the feature map of the drug molecule  and the feature map of the protein , we perform the MaxPooling operation to reduce the dimensions to one. Then do concatenation to merge the vectors , put them into three fully linked layers and finally output the binding affinity score.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18884, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "3 Experiments and results", "sentences": [], "annotations": [], "relations": []}, {"offset": 18910, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "3.1 Evaluation metrics", "sentences": [], "annotations": [], "relations": []}, {"offset": 18933, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We use three metrics to evaluate our model: mean squared Error (MSE), concordance index (CI), and a type of correlation index for prediction and ground truth .", "sentences": [], "annotations": [], "relations": []}, {"offset": 19093, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "MSE is the loss function: ", "sentences": [], "annotations": [], "relations": []}, {"offset": 19120, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The CI is another evaluation indicator for assessing the consistency in ranking the strength of binding affinities for two randomly selected pairs of drug and target between predicted and actual values. ", "sentences": [], "annotations": [], "relations": []}, {"offset": 19324, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": " N is a normalization constant equal to the number of data pairs.  is the step function and defined as:  is evaluating metrics for model external prediction performance. ", "sentences": [], "annotations": [], "relations": []}, {"offset": 19495, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The squared correlation coefficients,  and  quantify the relationship between observed and predicted values, accounting for and excluding the intercept term, respectively. Only when the  is greater than 0.5 proves the model reliable.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19729, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "3.2 Comparison of the prediction efficiency", "sentences": [], "annotations": [], "relations": []}, {"offset": 19773, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We first compared our model with the following benchmark models: KronRLS, SimBoost, DeepDTA, and DeepCDA. Among them, KronRLS and SimBoost are similarity-based models. DeepDTA and DeepCDA are sequence-based models. The test results for these methods on the test set are summarized in Tables\u00a02 and 3. As for the experimental results, we directly used the experimental data reported in the publications of these methods. We adopted this approach to avoid potential errors that may arise from implementation and training.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20293, "infons": {"file": "btad778-T2.xml", "id": "btad778-T2", "section_type": "TABLE", "type": "table_caption"}, "text": "Performance comparison of different models on Davis.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20346, "infons": {"file": "btad778-T2.xml", "id": "btad778-T2", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Model</th><th rowspan=\"1\" colspan=\"1\">CI (SD)</th><th rowspan=\"1\" colspan=\"1\">MSE</th><th rowspan=\"1\" colspan=\"1\">\n<inline-formula id=\"IE35\">\n<mml:math id=\"IM35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>\u00a0</mml:mo></mml:math>\n</inline-formula>(SD)</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">KronRLS</td><td rowspan=\"1\" colspan=\"1\">0.871 (0.001)</td><td rowspan=\"1\" colspan=\"1\">0.379</td><td rowspan=\"1\" colspan=\"1\">0.407 (0.005)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">SimBoost</td><td rowspan=\"1\" colspan=\"1\">0.872 (0.002)</td><td rowspan=\"1\" colspan=\"1\">0.282</td><td rowspan=\"1\" colspan=\"1\">0.644 (0.006)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DeepDTA</td><td rowspan=\"1\" colspan=\"1\">0.878 (0.004)</td><td rowspan=\"1\" colspan=\"1\">0.261</td><td rowspan=\"1\" colspan=\"1\">0.630 (0.017)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DeepCDA</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.891 (0.003)</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">0.248</td><td rowspan=\"1\" colspan=\"1\">0.649 (0.009)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">TEFDTA</td><td rowspan=\"1\" colspan=\"1\">0.890 (0.002)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.199</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.756 (0.008)</underline>\n</bold>\n</td></tr></tbody></table>\n"}, "text": "Model\tCI (SD)\tMSE\t(SD)\t \tKronRLS\t0.871 (0.001)\t0.379\t0.407 (0.005)\t \tSimBoost\t0.872 (0.002)\t0.282\t0.644 (0.006)\t \tDeepDTA\t0.878 (0.004)\t0.261\t0.630 (0.017)\t \tDeepCDA\t0.891 (0.003)\t0.248\t0.649 (0.009)\t \tTEFDTA\t0.890 (0.002)\t0.199\t0.756 (0.008)\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 20592, "infons": {"file": "btad778-T2.xml", "id": "btad778-T2", "section_type": "TABLE", "type": "table_footnote"}, "text": "Bold and underline corresponds to the best performance for each metric.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20664, "infons": {"file": "btad778-T3.xml", "id": "btad778-T3", "section_type": "TABLE", "type": "table_caption"}, "text": "Performance comparison of different models on KIBA.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20716, "infons": {"file": "btad778-T3.xml", "id": "btad778-T3", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Model</th><th rowspan=\"1\" colspan=\"1\">CI (SD)</th><th rowspan=\"1\" colspan=\"1\">MSE</th><th rowspan=\"1\" colspan=\"1\">\n<inline-formula id=\"IE36\">\n<mml:math id=\"IM36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math>\n</inline-formula>(SD)</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">KronRLS</td><td rowspan=\"1\" colspan=\"1\">0.782 (0.001)</td><td rowspan=\"1\" colspan=\"1\">0.411</td><td rowspan=\"1\" colspan=\"1\">0.342 (0.001)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">SimBoost</td><td rowspan=\"1\" colspan=\"1\">0.836 (0.001)</td><td rowspan=\"1\" colspan=\"1\">0.222</td><td rowspan=\"1\" colspan=\"1\">0.629 (0.007)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DeepDTA</td><td rowspan=\"1\" colspan=\"1\">0.863 (0.002)</td><td rowspan=\"1\" colspan=\"1\">0.194</td><td rowspan=\"1\" colspan=\"1\">0.673 (0.009)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DeepCDA</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.889 (0.002)</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.176</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">0.682 (0.008)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">TEFDTA</td><td rowspan=\"1\" colspan=\"1\">0.860 (0.001)</td><td rowspan=\"1\" colspan=\"1\">0.184</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.731 (0.006)</underline>\n</bold>\n</td></tr></tbody></table>\n"}, "text": "Model\tCI (SD)\tMSE\t(SD)\t \tKronRLS\t0.782 (0.001)\t0.411\t0.342 (0.001)\t \tSimBoost\t0.836 (0.001)\t0.222\t0.629 (0.007)\t \tDeepDTA\t0.863 (0.002)\t0.194\t0.673 (0.009)\t \tDeepCDA\t0.889 (0.002)\t0.176\t0.682 (0.008)\t \tTEFDTA\t0.860 (0.001)\t0.184\t0.731 (0.006)\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 20962, "infons": {"file": "btad778-T3.xml", "id": "btad778-T3", "section_type": "TABLE", "type": "table_footnote"}, "text": "Bold and underline corresponds to the best performance for each metric.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21034, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For the Davis dataset, TEFDTA and DeepCDA outperformed the other benchmark models. TEFDTA achieved superior performance compared to DeepCDA with a margin of 0.049 for mean squared error (MSE) and 0.107 for . On the other hand, DeepCDA achieved the best CI performance among all the models evaluated.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21334, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For the KIBA dataset, TEFDTA and DeepCDA also achieved the best performance metrics. DeepCDA outperformed TEFDTA by 0.002 for CI and 0.015 for MSE. However, TEFDTA achieved the best  among all the models evaluated.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21549, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In order to have a clearer understanding of the test results, Fig.\u00a03 illustrates the performance evaluation of our model in predicting binding affinities for two datasets: Davis and KIBA. The figure showcases the predicted values plotted against the measured values, emphasizing the effectiveness of our model. For the Davis dataset, the overall prediction results demonstrate a satisfactory agreement with the measured values. However, it is noteworthy that a cluster of points is concentrated around the value of five. This anomaly arises from the approximation made during data organization, where affinity values below 10\u00a0000\u2009nM were approximated to 10\u00a0000\u2009nM, introducing a certain degree of bias. On the other hand, the KIBA dataset exhibits a closer correspondence between the predicted and measured values, highlighting the model's capability to accurately predict binding affinities.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22449, "infons": {"file": "btad778f3.jpg", "id": "btad778-F3", "section_type": "FIG", "type": "fig_caption"}, "text": "Comparison of correlation between the predicted and measured values for Davis and KIBA Data.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22542, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "4 Ablation experiments", "sentences": [], "annotations": [], "relations": []}, {"offset": 22565, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The use of fingerprint transformation and Transformer encoder in our method significantly contributes to the overall performance by enhancing the understanding of molecular representation. To assess the specific impact of these components, we conducted ablation experiments where we removed or modified parts of the module. The results of these experiments are presented in Table\u00a04.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22949, "infons": {"file": "btad778-T4.xml", "id": "btad778-T4", "section_type": "TABLE", "type": "table_caption"}, "text": "Ablation experiments on feature extraction methods and model framework modifications in the Davis dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23056, "infons": {"file": "btad778-T4.xml", "id": "btad778-T4", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Different method</th><th rowspan=\"1\" colspan=\"1\">CI (SD)</th><th rowspan=\"1\" colspan=\"1\">MSE</th><th rowspan=\"1\" colspan=\"1\">\n<inline-formula id=\"IE39\">\n<mml:math id=\"IM39\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math>\n</inline-formula> (SD)</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">Only Fingerprint</td><td rowspan=\"1\" colspan=\"1\">0.877 (0.002)</td><td rowspan=\"1\" colspan=\"1\">0.235</td><td rowspan=\"1\" colspan=\"1\">0.679 (0.008)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Fingerprint with CNN</td><td rowspan=\"1\" colspan=\"1\">0.879 (0.002)</td><td rowspan=\"1\" colspan=\"1\">0.232</td><td rowspan=\"1\" colspan=\"1\">0.719 (0.003)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">SMILES with encoder</td><td rowspan=\"1\" colspan=\"1\">0.874 (0.001)</td><td rowspan=\"1\" colspan=\"1\">0.228</td><td rowspan=\"1\" colspan=\"1\">0.724 (0.004)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">\n<bold>Proposed model</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.890 (0.002)</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.199</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.756 (0.008)</underline>\n</bold>\n</td></tr></tbody></table>\n"}, "text": "Different method\tCI (SD)\tMSE\t (SD)\t \tOnly Fingerprint\t0.877 (0.002)\t0.235\t0.679 (0.008)\t \tFingerprint with CNN\t0.879 (0.002)\t0.232\t0.719 (0.003)\t \tSMILES with encoder\t0.874 (0.001)\t0.228\t0.724 (0.004)\t \tProposed model\t0.890 (0.002)\t0.199\t0.756 (0.008)\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 23311, "infons": {"file": "btad778-T4.xml", "id": "btad778-T4", "section_type": "TABLE", "type": "table_footnote"}, "text": "Bold and underline corresponds to the best performance for each metric.\u00a0\u00a0", "sentences": [], "annotations": [], "relations": []}, {"offset": 23387, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In the first modification, we removed the Transformer encoder module from the architecture. This experiment helps us evaluate the contribution of the Transformer encoder in extracting high-level drug information.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23600, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In the second modification, we replaced the Transformer encoder module with a 1D-CNN module. This alteration allows us to compare the performance of the Transformer encoder against the 1D-CNN in terms of drug representation extraction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23836, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In the third modification, we directly used SMILES as the input for the encoder, bypassing the fingerprint transformation step. This experiment helps us understand the impact of using the fingerprint transformation in capturing important substructure information of the drug molecules.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24122, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Through these ablation experiments, we can observe the advantages of using fingerprint transformation and Transformer encoder as the drug representation extraction modules. These components enable the model to capture and utilize high-level drug information effectively, leading to improving overall performance in drug\u2013target interaction prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24475, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "5 Binding affinity prediction for covalent ligands", "sentences": [], "annotations": [], "relations": []}, {"offset": 24526, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "After confirming the accurate prediction capability of our model for non-covalent binding affinity, we conducted training and testing on a larger dataset, BindingDB, and compared its performance with DeepDTA and DeepCDA (refer to Table\u00a05). Due to the lack of detailed hyperparameter information in DeepCDA's publication, we used their experimental results directly. For DeepDTA, we conducted training on the dataset while ensuring consistency with our model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24986, "infons": {"file": "btad778-T5.xml", "id": "btad778-T5", "section_type": "TABLE", "type": "table_caption"}, "text": "Different models\u2019 prediction performance on the dataset of BindingDB.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25058, "infons": {"file": "btad778-T5.xml", "id": "btad778-T5", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Model</th><th rowspan=\"1\" colspan=\"1\">CI (SD)</th><th rowspan=\"1\" colspan=\"1\">MSE</th><th rowspan=\"1\" colspan=\"1\">\n<inline-formula id=\"IE40\">\n<mml:math id=\"IM40\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math>\n</inline-formula> (SD)</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">DeepDTA</td><td rowspan=\"1\" colspan=\"1\">0.795</td><td rowspan=\"1\" colspan=\"1\">0.812</td><td rowspan=\"1\" colspan=\"1\">0.618</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DeepCDA</td><td rowspan=\"1\" colspan=\"1\">0.811</td><td rowspan=\"1\" colspan=\"1\">0.832</td><td rowspan=\"1\" colspan=\"1\">0.628</td></tr><tr><td rowspan=\"1\" colspan=\"1\">\n<bold>TEFDTA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.814</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.701</underline>\n</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>\n<underline>0.631</underline>\n</bold>\n</td></tr></tbody></table>\n"}, "text": "Model\tCI (SD)\tMSE\t (SD)\t \tDeepDTA\t0.795\t0.812\t0.618\t \tDeepCDA\t0.811\t0.832\t0.628\t \tTEFDTA\t0.814\t0.701\t0.631\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 25168, "infons": {"file": "btad778-T5.xml", "id": "btad778-T5", "section_type": "TABLE", "type": "table_footnote"}, "text": "Bold and underline corresponds to the best performance for each metric.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25240, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Subsequently, we gathered data containing covalent binding affinities from the CovalentInDB database and used the model trained on the BindingDB database. However, the experimental results revealed poor prediction performance, indicating that the model did not effectively capture the patterns associated with covalent binding.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25568, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "To address this limitation, we classified the data in CovalentInDB and identified six commonly occurring warheads. We performed fine-tuning on the model specifically for each warhead category and utilized it to predict the covalent binding data for individual warhead types. The comparison between the pre-fine-tuned and post-fine-tuned data is presented in Fig.\u00a04. Evaluation metrics such as mean squared error (MSE) and Pearson's correlation coefficient were used.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26036, "infons": {"file": "btad778f4.jpg", "id": "btad778-F4", "section_type": "FIG", "type": "fig_caption"}, "text": "Comparison of covalent binding affinity predictions before and after fine-tuning on six common warheads.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26141, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The results demonstrate that prior to fine-tuning, the model trained solely on the BindingDB dataset exhibited inadequate performance in predicting covalent binding affinity. However, after undergoing the fine-tuning process, the model became more proficient in recognizing the covalent warhead characteristics and achieved relatively accurate predictions for the corresponding covalent warheads. Fine-tuning effectively enhanced the model's ability to significantly improve the accuracy of covalent binding affinity prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26670, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "By undertaking fine-tuning on specific covalent warheads, we observed a considerable improvement in the model's prediction accuracy for covalent binding affinity. This underscores the importance of specialized training and fine-tuning when dealing with covalent binding data.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26946, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "6 Case study", "sentences": [], "annotations": [], "relations": []}, {"offset": 26959, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We conducted a case study to demonstrate the application of our proposed TEFDTA model in the field of drug design and, in particular, to investigate the ability of our method to discriminate molecules\u2019 activity cliffs. For this purpose, a set of novel heterodimeric inhibitors of epidermal growth factor receptor (EGFR)L858R, which were not included in either our training or test set, was selected them to perform the prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27392, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "These molecules have the same scaffold but different substituents in the same position, as shown in Table\u00a06. It is worth mentioning that some of these molecules possess identical sequences due to the shared backbone structure. The aim of this case study was to investigate whether our model could accurately capture subtle variations in substituents for molecules with similar sequences.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27781, "infons": {"file": "btad778-T6.xml", "id": "btad778-T6", "section_type": "TABLE", "type": "table_caption"}, "text": "Results for investigating our model\u2019s sensitivity to discriminating the difference of binding affinities (pKd) arising from the minor structure variance.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27937, "infons": {"file": "btad778-T6.xml", "id": "btad778-T6", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:xlink=\"http://www.w3.org/1999/xlink\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col valign=\"top\" align=\"left\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/><col valign=\"top\" align=\"char\" char=\".\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"1\" colspan=\"1\">Compound structure</th><th rowspan=\"1\" colspan=\"1\">\n<inline-graphic xlink:href=\"btad778ilf1.jpg\"/>\n</th><th rowspan=\"1\" colspan=\"1\">\n<inline-graphic xlink:href=\"btad778ilf2.jpg\"/>\n</th><th rowspan=\"1\" colspan=\"1\">\n<inline-graphic xlink:href=\"btad778ilf3.jpg\"/>\n</th><th rowspan=\"1\" colspan=\"1\">\n<inline-graphic xlink:href=\"btad778ilf4.jpg\"/>\n</th><th rowspan=\"1\" colspan=\"1\">\n<inline-graphic xlink:href=\"btad778ilf5.jpg\"/>\n</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">Measured value</td><td rowspan=\"1\" colspan=\"1\">8.39</td><td rowspan=\"1\" colspan=\"1\">8.22</td><td rowspan=\"1\" colspan=\"1\">7.79</td><td rowspan=\"1\" colspan=\"1\">7.63</td><td rowspan=\"1\" colspan=\"1\">6.02</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Predicted value</td><td rowspan=\"1\" colspan=\"1\">6.16</td><td rowspan=\"1\" colspan=\"1\">6.41</td><td rowspan=\"1\" colspan=\"1\">5.89</td><td rowspan=\"1\" colspan=\"1\">5.89</td><td rowspan=\"1\" colspan=\"1\">5.74</td></tr></tbody></table>\n"}, "text": "Compound structure\t\t\t\t\t\t \tMeasured value\t8.39\t8.22\t7.79\t7.63\t6.02\t \tPredicted value\t6.16\t6.41\t5.89\t5.89\t5.74\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 28049, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "As shown in Table\u00a06, results from our experiments indicate that our model can accurately rank the strength of these analogues, despite not being able to guarantee absolute accurate values for the binding affinities. This consistency between the model's prediction and the actual measurements indicates the model's sensitivity to local structural changes within molecules. Moreover, it suggests a certain level of robustness and the model's capability to approximate the strength of binding affinity.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28550, "infons": {"section_type": "CONCL", "type": "title_1"}, "text": "7 Conclusion", "sentences": [], "annotations": [], "relations": []}, {"offset": 28563, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "In this study, we propose TEFDTA, an approach for accurate prediction of drug\u2013target interaction. Our method incorporates fingerprint transformation and Transformer encoder modules to enhance molecular representation understanding. To evaluate TEFDTA's performance, we conduct experiments on the Davis, KIBA, and BindingDB dataset, and compare the results with other binding affinity prediction models, i.e. DeepDTA and DeepCDA. The results confirmed TEFDTA\u2019s performance in binding affinity prediction. In addition, this model was further optimized by fine-tuning over a dataset of bonded protein\u2013ligand interactions from the database CovalentInDB. Covalent binding data is classified based on common warheads, and individual fine-tuning is performed for each warhead category. The results demonstrate that fine-tuning process significantly improves the model's prediction accuracy for covalent binding affinity, emphasizing the importance of specialized training. Furthermore, we conduct a case study on predicting the binding affinity of drug molecules targeting the EGFR. Our results indicate that while the model may not precisely predict the exact binding affinity values for molecules with identical backbone structures but different substituents, it is able to capture the trend of affinity variance introduced by a different substituent on the molecule. This suggests the model's potential sensitivity to local structural changes and its capability to approximate binding affinity strength, which needs to be confirmed with a larger size dataset for further evaluation or training.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30160, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "In conclusion, TEFDTA, with its incorporation of fingerprint transformation and Transformer encoder modules, provides an improved approach for accurate prediction of drug\u2013target interactions. However, it is important to acknowledge the limitations of our model. While the model may successfully capture the effect of minor changes in the molecular sequence on affinity, it is not as sensitive to mutations in the protein segment, including single or few amino acid changes. For virtual screening tasks, it is very valuable to detect observable changes in affinity when mutations occur. It is also possible for us to achieve this goal. The direct extraction of features from the FASTA sequence of the protein makes it difficult to achieve this aim because single amino acid mutations are imperceptible in the embedding of the entire protein. However, with the advent of large language models, it has become possible to extract representations of proteins with unsupervised learning by pre-training these models on a large number of protein sequences. By fine-tuning the model with downstream tasks such as data having mutations, the model becomes sensitive to key amino acids. For example, in our current covalent database CovalentInDB, most of the covalent drug targets are cysteines, and the model is likely to be very sensitive to the amino acid cysteine. As soon as a change occurs, the embedding of the protein may undergo significant changes as well. Of course, this is only a hypothetical scenario that needs to be tested experimentally while the large size of data is available. In the future, we will also attempt to use large language models for the extraction of protein representation. Moreover, current prediction of covalent binding affinity requires priori knowledge of the type of covalent bond for a ligand and target pair, which may be a limitation for broad and proper application, especially not friendly for non-chemists. These aspects warrant further investigation and consideration in future research endeavors.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32196, "infons": {"section_type": "COMP_INT", "type": "title_1"}, "text": "Conflict of interest", "sentences": [], "annotations": [], "relations": []}, {"offset": 32217, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare that they have no competing interests.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32276, "infons": {"section_type": "ACK_FUND", "type": "title_1"}, "text": "Funding", "sentences": [], "annotations": [], "relations": []}, {"offset": 32284, "infons": {"section_type": "ACK_FUND", "type": "paragraph"}, "text": "This work was supported by the National Natural Science Foundation of China [82003654, 82341093], the National Key R&D Program of China [2022YFC3400501, 2022YFC3400500], Shanghai Science and Technology Development Funds [20QA1406400, 22ZR1441400], Lingang Laboratory [LG202102-01-03], start-up package from ShanghaiTech University, and Shanghai Frontiers Science Center for Biomacromolecules and Precision Medicine at ShanghaiTech University.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32727, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Data availability", "sentences": [], "annotations": [], "relations": []}, {"offset": 32745, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The codes and datasets used in this article are available on GitHub (https://github.com/lizongquan01/TEFDTA).", "sentences": [], "annotations": [], "relations": []}, {"offset": 32855, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 32866, "infons": {"fpage": "4633", "lpage": "42", "pub-id_pmid": "32462178", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "36", "year": "2020"}, "text": "DeepCDA: deep cross-domain compound\u2013protein affinity prediction through LSTM and convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 32979, "infons": {"fpage": "149", "lpage": "67", "pub-id_pmid": "31452104", "section_type": "REF", "source": "Methods Mol Biol", "type": "ref", "volume": "2053", "year": "2019"}, "text": "Molegro virtual docker for docking", "sentences": [], "annotations": [], "relations": []}, {"offset": 33014, "infons": {"fpage": "4217", "pub-id_pmid": "37452028", "section_type": "REF", "source": "Nat Commun", "type": "ref", "volume": "14", "year": "2023"}, "text": "Sequence-based drug design as a concept in computational drug design", "sentences": [], "annotations": [], "relations": []}, {"offset": 33083, "infons": {"fpage": "4406", "lpage": "14", "pub-id_pmid": "32428219", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "36", "year": "2020"}, "text": "TransformerCPI: improving compound\u2013protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments", "sentences": [], "annotations": [], "relations": []}, {"offset": 33244, "infons": {"fpage": "1046", "lpage": "51", "pub-id_pmid": "22037378", "section_type": "REF", "source": "Nat Biotechnol", "type": "ref", "volume": "29", "year": "2011"}, "text": "Comprehensive analysis of kinase inhibitor selectivity", "sentences": [], "annotations": [], "relations": []}, {"offset": 33299, "infons": {"fpage": "D1122", "lpage": "9", "pub-id_pmid": "33068433", "section_type": "REF", "source": "Nucleic Acids Res", "type": "ref", "volume": "49", "year": "2021"}, "text": "CovalentInDB: a comprehensive database facilitating the discovery of covalent inhibitors", "sentences": [], "annotations": [], "relations": []}, {"offset": 33388, "infons": {"fpage": "1739", "lpage": "49", "pub-id_pmid": "15027865", "section_type": "REF", "source": "J Med Chem", "type": "ref", "volume": "47", "year": "2004"}, "text": "Glide: a new approach for rapid, accurate docking and scoring. 1. Method and assessment of docking accuracy", "sentences": [], "annotations": [], "relations": []}, {"offset": 33496, "infons": {"fpage": "184", "lpage": "92", "pub-id_pmid": "31819266", "section_type": "REF", "source": "Nat Methods", "type": "ref", "volume": "17", "year": "2020"}, "text": "Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 33595, "infons": {"fpage": "2791", "lpage": "802", "pub-id_pmid": "32392050", "section_type": "REF", "source": "J Chem Inf Model", "type": "ref", "volume": "60", "year": "2020"}, "text": "RosENet: improving binding affinity prediction by leveraging molecular mechanics energies with an ensemble of 3D convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 33738, "infons": {"fpage": "24", "pub-id_pmid": "29086119", "section_type": "REF", "source": "J Cheminform", "type": "ref", "volume": "9", "year": "2017"}, "text": "SimBoost: a read-across approach for predicting drug\u2013target binding affinities using gradient boosting machines", "sentences": [], "annotations": [], "relations": []}, {"offset": 33852, "infons": {"fpage": "bbac533", "pub-id_pmid": "36545795", "section_type": "REF", "source": "Brief Bioinform", "type": "ref", "volume": "24", "year": "2023"}, "text": "SAM-DTA: a sequence-agnostic model for drug\u2013target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 33933, "infons": {"fpage": "830", "lpage": "6", "pub-id_pmid": "33070179", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "37", "year": "2021"}, "text": "MolTrans: molecular interaction transformer for drug\u2013target interaction prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34018, "infons": {"fpage": "287", "lpage": "96", "section_type": "REF", "source": "J Chem Inform Model", "type": "ref", "volume": "58", "year": "2018"}, "text": "KDEEP: protein\u2013ligand absolute binding affinity pre diction via 3D-convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 34117, "infons": {"fpage": "1301", "lpage": "8", "section_type": "REF", "type": "ref", "year": "2020"}, "text": "", "sentences": [], "annotations": [], "relations": []}, {"offset": 34118, "infons": {"fpage": "D198", "lpage": "201", "pub-id_pmid": "17145705", "section_type": "REF", "source": "Nucleic Acids Res", "type": "ref", "volume": "35", "year": "2007"}, "text": "BindingDB: a web-accessible database of experimentally determined protein\u2013ligand binding affinities", "sentences": [], "annotations": [], "relations": []}, {"offset": 34220, "infons": {"fpage": "1140", "lpage": "7", "pub-id_pmid": "33119053", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "37", "year": "2021"}, "text": "GraphDTA: predicting drug\u2013target binding affinity with graph neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 34299, "infons": {"fpage": "13052", "lpage": "73", "pub-id_pmid": "36178776", "section_type": "REF", "source": "J Med Chem", "type": "ref", "volume": "65", "year": "2022"}, "text": "Discovery of novel allosteric EGFR L858R inhibitors for the treatment of non-small-cell lung cancer as a single agent or in combination with osimertinib", "sentences": [], "annotations": [], "relations": []}, {"offset": 34452, "infons": {"fpage": "i821", "lpage": "i829", "pub-id_pmid": "30423097", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "34", "year": "2018"}, "text": "DeepDTA: deep drug\u2013target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34508, "infons": {"fpage": "325", "lpage": "37", "pub-id_pmid": "24723570", "section_type": "REF", "source": "Brief Bioinform", "type": "ref", "volume": "16", "year": "2015"}, "text": "Toward more realistic drug-target interaction predictions", "sentences": [], "annotations": [], "relations": []}, {"offset": 34566, "infons": {"fpage": "3666", "lpage": "74", "pub-id_pmid": "29757353", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "34", "year": "2018"}, "text": "Development and evaluation of a deep learning model for protein\u2013ligand binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34667, "infons": {"fpage": "735", "lpage": "43", "section_type": "REF", "source": "J Chem Inform Model", "type": "ref", "volume": "54", "year": "2014"}, "text": "Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis", "sentences": [], "annotations": [], "relations": []}, {"offset": 34774, "infons": {"fpage": "6000", "lpage": "10", "section_type": "REF", "source": "Advances in Neural Information Processing Systems", "type": "ref", "volume": "30", "year": "2017"}, "text": "Attention is all you need", "sentences": [], "annotations": [], "relations": []}, {"offset": 34800, "infons": {"fpage": "bbab474", "pub-id_pmid": "34849569", "section_type": "REF", "source": "Brief Bioinform", "type": "ref", "volume": "23", "year": "2022"}, "text": "A point cloud-based deep learning strategy for protein\u2013ligand binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34892, "infons": {"fpage": "816", "lpage": "33", "pub-id_pmid": "35173947", "section_type": "REF", "source": "Chem Sci", "type": "ref", "volume": "13", "year": "2022"}, "text": "MGraphDTA: deep multiscale graph neural network for explainable drug\u2013target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34998, "infons": {"fpage": "15956", "lpage": "65", "pub-id_pmid": "31592466", "section_type": "REF", "source": "ACS Omega", "type": "ref", "volume": "4", "year": "2019"}, "text": "OnionNet: a multiple-layer intermolecular-contact-based convolutional neural network for protein\u2013ligand binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 35132, "infons": {"fpage": "btad560", "pub-id_pmid": "37688568", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "39", "year": "2023"}, "text": "DataDTA: a multi-feature and dual-interaction aggregation framework for drug\u2013target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]