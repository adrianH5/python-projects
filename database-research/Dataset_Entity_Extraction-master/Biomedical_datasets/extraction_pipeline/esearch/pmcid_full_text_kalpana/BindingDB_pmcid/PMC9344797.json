[{"source": "PMC", "date": "20220806", "key": "pmc.key", "infons": {}, "documents": [{"id": "9344797", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.1038/s41598-022-16493-9", "article-id_pmc": "9344797", "article-id_pmid": "35918366", "article-id_publisher-id": "16493", "elocation-id": "13237", "kwd": "Computational models Machine learning Target identification", "license": "Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.", "name_0": "surname:Dick;given-names:Kevin", "name_1": "surname:Kyrollos;given-names:Daniel G.", "name_10": "surname:Nasir;given-names:Muneeb A.", "name_11": "surname:Qiu;given-names:Chang", "name_12": "surname:Robinson;given-names:Aisha S.", "name_13": "surname:Shao;given-names:Derek", "name_14": "surname:Siromahov;given-names:Boyan R.", "name_15": "surname:Starlight;given-names:Evening", "name_16": "surname:Tran;given-names:Christophe", "name_17": "surname:Wang;given-names:Christopher", "name_18": "surname:Yang;given-names:Yu-Kai", "name_19": "surname:Green;given-names:James R.", "name_2": "surname:Cosoreanu;given-names:Eric D.", "name_3": "surname:Dooley;given-names:Joseph", "name_4": "surname:Fryer;given-names:Joshua S.", "name_5": "surname:Gordon;given-names:Shaun M.", "name_6": "surname:Kharbanda;given-names:Nikhil", "name_7": "surname:Klamrowski;given-names:Martin", "name_8": "surname:LaCasse;given-names:Patrick N. L.", "name_9": "surname:Leung;given-names:Thomas F.", "section_type": "TITLE", "title": "Subject terms", "type": "front", "volume": "12", "year": "2022"}, "text": "Reciprocal perspective as a super learner improves drug-target interaction prediction (MUSDTI)", "sentences": [], "annotations": [], "relations": []}, {"offset": 95, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The identification of novel drug-target interactions (DTI) is critical to drug discovery and drug repurposing to address contemporary medical and public health challenges presented by emergent diseases. Historically, computational methods have framed DTI prediction as a binary classification problem (indicating whether or not a drug physically interacts with a given protein target); however, framing the problem instead as a regression-based prediction of the physiochemical binding affinity is more meaningful. With growing databases of experimentally derived drug-target interactions (e.g. Davis, Binding-DB, and Kiba), deep learning-based DTI predictors can be effectively leveraged to achieve state-of-the-art (SOTA) performance. In this work, we formulated a DTI competition as part of the coursework for a senior undergraduate machine learning course and challenged students to generate component DTI models that might surpass SOTA models and effectively combine these component models as part of a meta-model using the Reciprocal Perspective (RP) multi-view learning framework. Following 6 weeks of concerted effort, 28 student-produced component deep-learning DTI models were leveraged in this work to produce a new SOTA RP-DTI model, denoted the Meta Undergraduate Student DTI (MUSDTI) model. Through a series of experiments we demonstrate that (1) RP can considerably improve SOTA DTI prediction, (2) our new double-cold experimental design is more appropriate for emergent DTI challenges, (3) that our novel MUSDTI meta-model outperforms SOTA models, (4) that RP can improve upon individual models as an ensembling method, and finally, (5) RP can be utilized for low computation transfer learning. This work introduces a number of important revelations for the field of DTI prediction and sequence-based, pairwise prediction in general.", "sentences": [], "annotations": [], "relations": []}, {"offset": 1946, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 1959, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The elucidation of drug-target interactions (DTIs) are needed to characterize the physiochemical binding affinity of potential drug compounds to a given protein target. The determination of DTI interactions enable applications such as drug repurposing and screening which are of great importance in light of emergent diseases and viruses as exemplified in the recent and ongoing COVID19 pandemic. The computational identification of novel DTIs accelerates the drug discovery process and enables the rapid discovery of putative interactions representing candidates for experimental validation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2552, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Historically, many computational DTI methods have formulated the problem as a binary classification where predictions between a given drug and protein target are expressed as either binding or not. Such a formulation does not necessarily capture the multitudinous continuous-value factors that results in a DTI, such as respective molecular concentrations. When treated instead as a regression-type problem, where the input drug-target pair representation are predicted as a continuous drug-target affinity (DTA) value, the DTI prediction produces a more nuanced representation of relative binding affinity. Many of the contemporary DTI benchmark datasets (namely, BindingDB, Davis, and KIBA) express drug-target binding affinity in a quantitative measure; however, the measures used in each dataset are not necessarily compatible.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3384, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Binding affinity is quantified and expressed in various ways. For example, it may be represented as a dissociation constant (), or as an inhibition constant (), or the half maximal inhibitory concentration (). When the  value is low, it indicates high binding affinity. Similarly, a low  value indicates a high binding affinity. Generally,  and  values are expressed in terms of  and  respectively, which stand for the negative logarithm of  and .", "sentences": [], "annotations": [], "relations": []}, {"offset": 3832, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The research community has focused on the training and testing of deep machine learning DTI methods on independent datasets with their uniquely expressed definitions of binding affinity. Three datasets have formed the benchmark basis for the development of SOTA DTI methods. The Davis and BindingDB, while smaller than the KIBA dataset, each express DTI pairs using a traditional DTI binding affinity () that, for consistency, we will refer to as  in this work. Complimentarily, the KIBA dataset defines DTI binding affinity using an aggregate \u2018KIBA Score\u2019 (we denote it as  in this work) that combines several DTI metrics into a single meta-score. While the  value of certain DTI pairs contribute to the overall  score, numerically, there is no simple, linear mapping between the two metrics.", "sentences": [], "annotations": [], "relations": []}, {"offset": 4630, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "The combination of benchmark datasets for double-cold evaluation", "sentences": [], "annotations": [], "relations": []}, {"offset": 4695, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Given the importance of these three benchmark datasets to DTI prediction, we here define a new experimental method to integrate the three benchmark datasets such that they might be leveraged to train new models on the maximal number of available pairs to advance the DTI SOTA, while providing a framework for fair comparison with existing methods. To that end, we focus on the development of models trained on the combined BindingBD and Davis datasets (given their consistent definition of DTI scoring with binding affinity, ) and then independently evaluate performance over the larger KIBA dataset. Most importantly, in considering all three datasets, we additionally define a novel evaluation dataset, denoted the \u2018double-cold dataset\u2019, where no one drug SMILE or protein amino acid sequence appears in either the training or validation datasets. Since this final evaluation dataset is completely independent of the training and validation datasets, it correctly reflects model performance when applied to completely new drug SMILES and/or completely new proteins. Consequently, it is an evaluation framework that better represents how performant various SOTA models would be in generating predictions for new emergent diseases (such as is the case for emergent diseases like COVID-19 caused by the SARS-CoV-2 proteins).", "sentences": [], "annotations": [], "relations": []}, {"offset": 6023, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In an effort to formalize this research project as part of a collective project tailored to senior-undergraduate students to generate models that might surpass SOTA DTI models, we challenged a cohort of students to generate deep machine learning models that may surpass SOTA DTI models. In the following sections, we lay the conceptual foundations that encouraged the generation of novel DTI SOTA models. This initiative was predominantly inspired from global-centric challenges seen within distributed competitive frameworks. In the following sections, we describe how the regression-based formulation of the DTI task within a peer vs. peer challenge framework enabled the generation of competitive SOTA DTI models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6740, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Computational competition breeds innovation", "sentences": [], "annotations": [], "relations": []}, {"offset": 6784, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The big data and artificial intelligence era has enabled the establishment of computational variants of traditional fields of research (and their pedagogical frameworks) including computational biology, computational chemistry, and computational physics. However, it is within global and/or community-level competition contexts that many state-of-the-art (SOTA) methods emerged. The framing of grand challenges in ways that engage the broad research community enables the consistent use of benchmark data, the cross-pollination of methodologies, and progressive iteration of achievable performance. In the following subsections, we describe how computational competitive contexts enabled this work.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7483, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "First, we introduce the Netflix Competition as it arguably represents the initial demonstration of galvanizing both academic and industry research groups in the pursuit of a large-scale and multi-year challenge. Secondly, the success of the Netflix Competition gave rise to the Kaggle online competitive framework. Finally, we describe how these frameworks are increasingly used as the basis of grand challenge competitions within biomedical research to advance the frontier of domain knowledge.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7979, "infons": {"section_type": "INTRO", "type": "title_3"}, "text": "The Netflix competition", "sentences": [], "annotations": [], "relations": []}, {"offset": 8003, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "While specific academic research communities enjoyed the burgeoning of computational compliments over the lat decade(s), it is the nascence of the Netflix competition circa. 2006\u20132009 that initiated the research community and global community to a benchmark-based innovation challenge. In 2006, Netflix publicly released a dataset comprising a hundred million anonymized movie ratings on a five-point scale as part of a million-dollar challenge to the global computer science and machine learning communities to beat its existing recommendation system, denoted Cinematch.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8577, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The Netflix competition represents one of the original framings of an international competition soliciting the efforts of teams of researchers applying themselves to improve (at the time, by a substantial margin) the state-of-the-art method for a specific task. In providing a high-quality and structured dataset from which teams could base their solutions, numerous advances in the research of recommendation systems were achieved.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9010, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Most notably, the top-ranking competitors incrementally generated large ensembled methods from individual component predictors; the best performing models resulted from the combination of complimentary methods and from multi-scale views. These large-scale ensemble methods integrated through a cascaded linear model are typically referred to as a \u201cblended\u201d model in machine learning literature. Through the use of k-fold cross-validation for creating a weighted combination of many candidate learners, these ensemble models are referred to as \u201csuper learners\u201d. The last decade has increasingly seen the use of ensemble methods as part of online competitions since the completion of the Netflix Competition. The official winners of the multi-phasic competition (under the pseudonym \u201cBellKor Pragmatic Chaos\u201d) achieved the ambitious minimum Root Mean Squared (RMSE) improvement of +10% over the Cinematch and other baseline solutions on September 18, 2009. The popularization of this competition gave rise to a trend that has since shaped the landscape of crowd-sourced solutions to otherwise challenging open industrial and research questions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10165, "infons": {"section_type": "INTRO", "type": "title_3"}, "text": "Kaggle: online competition environment", "sentences": [], "annotations": [], "relations": []}, {"offset": 10204, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Through the decade following the Netflix competition (2010\u20132020), a paradigm shift in the crowd-sourced problem-solving space ensued. Open innovation and crowd-sourcing organizations, such as InnoCentive, offered monetary rewards to selected \u201cSolvers\u201d for proposed solutions to posted \u201cChallenges\u201d of unsolved problems. However, it is the fully online competition frameworks, such as Kaggle, that fostered communities of machine learning practitioners and data scientists to crowd-source solutions to open problems. The Kaggle platform enables users to leverage published datasets, contribute models, and collaborate broadly to solve machine learning/data science problems. The crowdsourcing of solutions be collaborative and/or competitive depending on the challenge outcome. A collaborative challenge seeks to focus contributors towards an objective outcome that is achieved incrementally and rewards shared contribution. A strictly competitive problem formulation (e.g. InnoCentive) often seeks to collect diverse and independently generated solutions from which the crowdsourcer selects a winner.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11315, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Furthermore, the platform enables individuals to establish credentials on open datasets in a structured environment. This formal extra-university framework for establishing one\u2019s expertise in machine learning represents a new form of credentialization that can lead to employment opportunities both within the Kaggle ecosystem and beyond. Much like the Netflix competition, submitted solutions are automatically evaluated against a benchmark enabling the ranking of teams in near-real-time.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11808, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In essence, contemporary problems and corresponding structured datasets are made broadly available to the global community during a competition period and top-ranking solutions receive monetary prizes for their solutions. As an online collaborative environment, participants are also rewarded for contributing open programmatic content that is up-/down-voted by other participants based upon its utility to the community. In summary, Kaggle has fundamentally transformed the data science and applied machine learning landscape through democratization of datasets and methods in a fully collaborative digital environment available to all, expert to student alike, in contrast to the global competitions directed by dedicated research communities to tackle fundamental research questions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12595, "infons": {"section_type": "INTRO", "type": "title_3"}, "text": "Critical assessment of <Insert Task>", "sentences": [], "annotations": [], "relations": []}, {"offset": 12632, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "With a more dedicated research focus, international competitions to address grand challenges are often run at biennial intervals. For example, in the pursuit of advances at the frontier of molecular biology, a series of competitions templated by the convention \u201cCritical Assessment of <Insert Task>\u201d are hosted to encourage the development of new methods and derive novels insight towards the resolution of each challenge. Examples include the \u201cCritical Assessment of Structure Prediction\u201d (CASP; now in its 15th iteration), the \u201cCritical Assessment of Genome Interpretation\u201d (CAGI; now in its 6th iteration), and the \u201cCritical Assessment of Functional Annotation\u201d (CAFA; now in its 4th iteration) competitions, among others. CASP was the first of such competitions after which other fields modelled themselves. These international competitions are held regularly to galvanize teams within the research community to develop methods in an effort to establish and advance the state-of-the-art. Similar to the Kaggle challenges, these competitions provide research groups with an opportunity to establish their excellence in a fair and open competition, with some groups participating to solve the challenge, while others participate to establish the superiority of their underlying machine learning methods.", "sentences": [], "annotations": [], "relations": []}, {"offset": 13954, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Teams are not exclusive to academic research labs; in a notable example, during the 2020 CASP competition, DeepMind developed AlphaFold resulting in a tremendous improvement in performance over competing methods and benchmarked across previous years. The competition assessors declared that AlphaFold 2 had succeeded in solving the 50-year grand challenge demonstrating the promise of machine learning when used in conjunction with massive-scale computational resources.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14425, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "As with Kaggle competitions, these international competitions represent excellent learning opportunities for participating students (whether at the graduate or undergraduate level) to establish credibility in the sub-field, or more broadly, within machine learning. (Under)graduate student-based teams with the guidance of a seasoned mentor can, at times, be successful within these competitions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14822, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "MetaStudent: a student-centric case study", "sentences": [], "annotations": [], "relations": []}, {"offset": 14864, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Competitions, whether tailored to research or industry applications, represent excellent learning opportunities and, consequently, may be leveraged for pedagogical goals. In an impressive demonstration of utilizing graduate student bioinformatic pedagogy for the advancement of research in the domain of protein function prediction, as part of a 2013 Master\u2019s-level bioinformatics course, Dr. Burkhard Rost (Technical University of Munich) assigned the project of generating solutions for the recently-run CAFA competition. The 16 students participating in the course were divided into three groups and each applied themselves to generating a homology-based model capable of predicting protein functions, resulting in three solutions denoted \u201cStudentA\u201d, \u201cStudentB\u201d, and \u201cStudentC\u201d. Two of these three methods were determined to be competitive in CAFA and outperformed related methods prompting their combination as part of a single meta-predictor.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15826, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The post-CAFA evaluation of the meta-model (denoted MetaStudent as a weighted ensemble based on confidence scores of each model) was evaluated to have been among the top-10 methods of the competition; a notable feat for student-only teams producing their models over an 8-week period. This work represents a promising demonstration that tailoring student pedagogy to include projects focused on addressing open research questions may lead to surprisingly successful outcomes that contribute to the research community well beyond the ephemerality of that course; MetaStudent, to date, is incorporated in the PredictProtein software developed by the Rost lab.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16484, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "This work inspired the generalized framework presented in this article to tailor the pedagogy of a senior undergraduate coursework to address open research questions. The following sections describe how this was achieved to support the global COVID19 pandemic research initiative to encourage students to contribute meaningfully to contemporary problems, as outlined in.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16855, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Related work and ensembling student-generated DTI methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 16913, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The related DTI work covers a broad collection of methods as summarized in recent reviews. Recent (deep) machine learning approaches have formulated the drug-target prediction problem so as to predict the continuous DTA value directly.", "sentences": [], "annotations": [], "relations": []}, {"offset": 17149, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "One of the first such methods was proposed by Pahikkala et al. that combined a drug compound similarity-based representation and Smith-Waterman similarity representation of targets in conjunction with the Kronecker regularized least-squared method. The work of Zhao et al. introduced a DTA model based on an adversarial neural network (GAN) architechture, denoted GANsDTA, capable of learning drug and protein sequence features for both labelled and unlabelled data. In the work of Abbasi et al., denoted DeepCDA, the combined use of convolutional neural networks (CNNs) and long-short-term memory (LSTM) blocks produced a two-sided attention mechanism that learns a better representation of drug and protein sequences. In the work of \u00d6zt\u00fcrk et al., describing the SOTA DeepDTA model, a one dimensional CNN encoding of the drug SMILES and proteins sequences are concatenated into a single representation and fed in multiple deep fully connected layers to produce a final output prediction. Shim et al. proposes a similarity-based model that generates DTA predictions from two dimensional CNNs applied to the outer products between column vectors of two similarity matrices for the drugs and targets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18351, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Beyond the use of a single (deep) machine learning model for DTI prediction tasks, there is additionally a growing usage of an ensembled collection of individual models to achieve and/or surpass SOTA-level performance. For example, the EnsembleDLM method embeds sequence information of chemical compounds and proteins and generates predictions based on the aggregation of predictions from multiple deep neural networks; this method achieved SOTA performance over the Davis and KIBA datasets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18843, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Most relevant to the work presented herein is the concept of Super Learners that integrate the predicted output of numerous individual component learning models within a cascaded learning model that generates a final prediction. Super learners are conventionally trained through k-fold cross-validation and benefit from the complimentary input of numerous diversely-defined component predictors. Most importantly, their training, validation, and test datasets are specifically defined to guarantee that no training sample is present in the test sets to ensure a fair comparison and integration of methods.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19449, "infons": {"file": "41598_2022_16493_Fig1_HTML.jpg", "id": "Fig1", "section_type": "FIG", "type": "fig_caption"}, "text": "Conceptual overview of the proposed MUSDTI predictor.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19503, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In Fig. 1 we illustrate how the three DTI benchmark datasets were leveraged to enable the training and evaluation of student-contributed deep machine learning DTI models and how these were combined using the Reciprocal Perspective (RP) framework to form a meta-DTI model that was evaluated against SOTA DTI models. In combining multiple component student models as part of an ensembled meta-model, we demonstrate that significant improvement in performance over SOTA models can be achieved and the experimental design employed should serve as the basis for future DTI model development to adequately report expected model performance on double-cold DTI pairs (where neither the drug SMILES nor target amino acid sequence have previously been seen in training or validation data).", "sentences": [], "annotations": [], "relations": []}, {"offset": 20283, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Data and methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 20300, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In the following section, we describe how three of the benchmark DTI datasets were integrated into training, validation, and the proposed double-cold evaluation dataset. The training data were then made available to students as part of an senior undergraduate machine learning course project. We describe the software framework, programming environment, and hardware available to the students. We then describe how each of these student contributed component models were combined using the RP cascaded machine learning method and finally evaluated and compared to SOTA DTI methods on the double-cold dataset as well as the test dataset defined in the DeepDTA work, for a fair comparison. An overview figure of the end-to-end processing pipeline is visualized in Fig. 1.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21070, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Structured project materials for both didactics and contributing to contemporary research", "sentences": [], "annotations": [], "relations": []}, {"offset": 21160, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Students enrolled in the Fall 2020 course offering of Introduction to Machine Learning at Carleton University (SYSC4906) were instructed as part of their course-long project to produce machine learning models competitive with SOTA DTI models. To that end, students formed groups of two and were provided with the BindingDB and Davis datasets as well as Jupyter Notebooks utilizing the DeepPurpose framework demonstrating how a DTI model could be trained and evaluated.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21629, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To ensure all students shared equal access to hardware resources with which to train and evaluate their models, Google\u2019s Collaboratory environment (Google Colab) was used by each student group. The adoption of notebook environments and freely accessible cloud-based high-performance computing infrastructure for scientific discovery is emergent and democratizing the scientific process. While access to high-capacity GPUs varies by session, anecdotally, all student groups were able to train and evaluate multiple iterations of their proposed component DTI models over the project duration. A discussion of the many lessons learned in formulating an open research project as part of (under)graduate course didactics are available in our related work. The following sections detail the dataset, software framework, RP methodology, and experimental design with a comparison against SOTA DTI methods (Fig. 1).", "sentences": [], "annotations": [], "relations": []}, {"offset": 22538, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "The Davis, BindingDB, and KIBA datasets", "sentences": [], "annotations": [], "relations": []}, {"offset": 22578, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In this work we leverage three benchmark datasets to train and evaluate the component student models. The smallest of the three, Davis, includes binding affinities (expressed in ) for 26,000 pairs involving 68 unique drugs and 442 unique targets. The second database, BindingDB, is a a publicly accessible database of experimentally measured binding affinities for 55,000 pairs (as of time of writing,Feb. 6, 2022, the dataset now contains 41,296 unique entries representing 8661 protein targets and 1,039,940 small molecules). Similar to the Davis dataset, the recorded binding affinities are expressed as a  value. Finally, the most unique benchmark dataset of the three is KIBA that integrates kinase inhibitor bioactivity from various affinity measurements including , , and  in a uniquely defined \u201cKIBA score\u201d, denoted as  in this work. The KIBA dataset comprises 118,000 observations (involving 52,498 drugs and 467 targets). Table 1 summarizes the sizes of each dataset for use in each stage of the model generation pipeline illustrated in Fig. 1 and according to the experimental design outlined in Fig. 2. Finally, while prior work may have considered the direct  values from Davis or BindingDB, we followed from the work of \u00d6zt\u00fcrk et al., He et al., and as implemented in Shim et al. to log-transform the values into a  value as follows:", "sentences": [], "annotations": [], "relations": []}, {"offset": 23932, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_caption"}, "text": "DTI dataset sizes and their combined usage in defining the two test datasets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24010, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">Dataset descriptor</th><th align=\"left\">Num. DTI pairs</th></tr></thead><tbody><tr><td align=\"left\">Davis</td><td align=\"left\">25,772</td></tr><tr><td align=\"left\">BindingDB</td><td align=\"left\">55,148</td></tr><tr><td align=\"left\">KIBA</td><td align=\"left\">117,657</td></tr><tr><td align=\"left\">Training data (D+BDB)</td><td align=\"left\">80,920</td></tr><tr><td align=\"left\">Numerical Map data (KIBA)</td><td align=\"left\">108,436</td></tr><tr><td align=\"left\">Test size (double cold)</td><td align=\"left\">8178</td></tr><tr><td align=\"left\">Test size (DTA-defined)</td><td align=\"left\">19,550</td></tr></tbody></table>\n"}, "text": "Dataset descriptor\tNum. DTI pairs\t \tDavis\t25,772\t \tBindingDB\t55,148\t \tKIBA\t117,657\t \tTraining data (D+BDB)\t80,920\t \tNumerical Map data (KIBA)\t108,436\t \tTest size (double cold)\t8178\t \tTest size (DTA-defined)\t19,550\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 24227, "infons": {"file": "41598_2022_16493_Fig2_HTML.jpg", "id": "Fig2", "section_type": "FIG", "type": "fig_caption"}, "text": "Experimental design to evaluate the proposed MUSDTI predictor.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24290, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Importantly, we note that a linear mapping between  as used in the Davis and BindingDB datasets and the  used in the KIBA dataset does not exist. In the majority of DTI model development, methods are trained and evaluated on independent datasets. Promisingly, the public availability of these three benchmark datasets in developing DTI predictors (in a safe and efficient way) within this work may ultimately lead to improved DTI SOTA models. To that end, students were tasked with producing a DTI regression model trained from both the Davis and BindingDB datasets to then leverage the KIBA dataset. Subsequently, their models were further refined and improved using a cascaded RP model. As previously described, students were provided with access to these two benchmark datasets and the ability to recreate and evaluate existing SOTA DTI models through the DeepPurpose framework which, ultimately, enabled them to generate their own custom DTI models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25244, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "The DeepPurpose framework", "sentences": [], "annotations": [], "relations": []}, {"offset": 25270, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "\u201c15+ powerful encodings for drugs and proteins, ranging from deep neural network on classic cheminformatics fingerprints, CNN, transformers to message passing graph neural network, with 50+ combined models! Most of the combinations of the encodings are not yet in existing works. All of these under 10 lines but with lots of flexibility! Switching encoding is as simple as changing the encoding names!\u201d", "sentences": [], "annotations": [], "relations": []}, {"offset": 25677, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Several models produced with the DeepPurpose framework were compared with KronRLS, and GraphDTA, and DeepDTA, each state-of-the-art DTI methods. It was concluded that some methods using DeepPurpose achieved comparable predictive performance on two benchmark datasets, DAVIS and KIBA; this work integrates multiple benchmark datasets and fairly investigates how the DeepPurpose framework enables the extension and integration of these studies. The DeepPurpose framework, as originally introduced by Huang et al. in the Fall of 2020 is a deep learning-based molecular modeling and prediction toolkit that provides a programmatic framework enabling the rapid prototypting of DTI predictors and related molecular computational applications including protein function prediction, protein-protein interaction prediction, and compound property prediction. In abstracting away much of the low-level programming required to load, process, and manipulate drug SMILES and protein amino acid sequences, DeepPurpose makes readily accessible the implementation of seven protein encoders, eight compound encoders, over 50 deep learning models. Huang et al. empirically determined that models implemented in DeepPurpose and evaluated against SOTA DTI predictors achieved similar or improved performance on DTI benchmark datasets. Promisingly, the DeepPurpose documentation highlights among its features that numerous combinations of drug-target encoding and deep learning models have yet to be trained and evaluated, leaving considerable room for individual and ensembled models to be produced:", "sentences": [], "annotations": [], "relations": []}, {"offset": 27256, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To that end, the approximately 39 student teams in the course were introduced to DTI prediction and provided notebooks using the DeepPurpose framework to implement and retrain an existing SOTA model, notably the DeepDTA model by \u00d6zt\u00fcrk et al. and then challenged to produce their own model in an attempt to improve performance over this existing model, replicating a Kaggle-like competition.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27650, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Development and comparison of component models", "sentences": [], "annotations": [], "relations": []}, {"offset": 27697, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Instructed only to make use of either or both the Davis and BindingDB datasets to develop their models, students trained their component models (CM) using the DeepPurpose framework and Google Colab for access to GPU resources. For the small number of duplicate pairs in the two training datasets () the label was set to the average of those scores. We distinguish individual student models as \u201ccomponent models\u201d to differentiate them from subsequent ensemble models. Each component model was assigned a unique identification templated as G<id> where <id> . With this naming convention, we refer to the component model produced by group 9 and \u201cG9-CM\u201d.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28356, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In Fig. 2A we illustrate the relative sizes of each of the three datasets considered in this work and emphasize their binding affinity measure (i.e.  for training,  for testing); a Venn diagram of how each dataset overlaps and relates to the others is visualized, noting that the two test datasets (the \u2019double-cold\u2019 and \u2019DTA-defined\u2019) are subsets of KIBA, as illustrated in Fig. 2.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28747, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For a fair comparison against the DeepDTA SOTA DTI model, we considered the same evaluation dataset as defined in \u00d6zt\u00fcrk et al. that we denote as the \u201cDTA-Defined Test Dataset\u201d in Fig. 2C. The second test dataset is a considerably more challenging \u201cDouble-Cold Test Dataset\u201d given that it comprises the set of pairs where neither the drug compound or target protein appear in the either thge BindingDB  Davis training dataset (Fig. 2D). Consequently, this dataset represents the most challenging evaluation task and reflects the model performance when predicting completely novel and/or unseen drug targets and drug compounds. For novel organisms or emergent pathogens/viruses, top-performing models evaluated under this proposed scheme are ideally suited.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29514, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Reciprocal perspective for transfer learning over KIBA to generate RP-DTI models", "sentences": [], "annotations": [], "relations": []}, {"offset": 29595, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "As described above, there is no simple linear mapping between  and , such that the three benchmark datasets cannot be easily combined for model training and evaluation. We hypothesized that a cascaded model could be trained to learn the nonlinear mapping between these DTA definitions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29881, "infons": {"file": "41598_2022_16493_Fig3_HTML.jpg", "id": "Fig3", "section_type": "FIG", "type": "fig_caption"}, "text": "Example paired one-to-all score curves. An example pair demonstrating dramatically differing distributions is depicted to emphasize that even though a given drug scores relatively low in the given protein target perspective, that protein is the top-scoring target for that specific drug.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30169, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For similar bioinformatic tasks, the Reciprocal Perspective (RP) framework has been leveraged in a cascade to improve initial model prediction results. In subsequent work, RP was used for the cascaded combination of multiple experts (), and we here hypothesize that RP can be used not only for the combination of multiple experts () but also as a means of learning a domain transfer model. We depict in Fig. 3 the various RP-based features extracted for any pair of drug and protein target.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30660, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Leveraging all three benchmark datasets with two differing DTA measures connected through a non-linear mapping function provides the opportunity to use the RP framework to generate a cascaded learning model to learn this non-linear mapping and further improve the component model performance. In previous work, RP was demonstrated to significantly improve the predictive performance of protein-protein interaction predictors and the performance of microRNA target predictors. RP has also been shown capable of fusing component models () as an ensembling method. In this work, we propose to utilize RP as both a transfer learning method (to learn an approximated mapping from  to ) as well as a many-CM ensembling method ().", "sentences": [], "annotations": [], "relations": []}, {"offset": 31384, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "As illustrated in Fig. 1, the prototypical prediction pipeline will consider a given data source to train and evaluate a model and generate a set predictions, however RP cascades beyond these initial results. The RP method makes use of the comprehensive set of all predicted scores (denoted the \u201cComprehensive Prediction Matrix\u201d [CPM]) as a data source (i.e. in a cascade) to train and evaluate a downstream RP model to generate the final set of predictions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 31847, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "This methodological framework was originally introduced as a cascaded, semi-supervised learning algorithm to improve the pair-wise predictive performance of existing learning algorithms. Most interestingly, in considering these output scores generated by various initial learning algorithms as a combined input to the RP method, those model-specific scores are cast into a new rank-order domain denoted a One-to-All score curve (O2A) where, in the case of DTI provides two complimentary views, a drug-based \u201cperspective\u201d and a protein-based \u201cperspective\u201d. That is, for an n-numbered set of drugs and m-numbered proteins, the CPM containing all nm/2 predictions could be utilized through RP. For a given query pair (x, y), RP examines the pair\u2019s predicted score in the context of all predicted scores for all pairs involving either (x,) or (,y). The RP method differentiates itself from other cascaded predictors in that it is domain-agnostic (the features leveraged are derived from a domain entirely independent of the context of the original problem) and it therefore serves as a prediction error-correction layer.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32974, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Intuitively, it is the recent development of high-throughput (deep) learning models that have enabled the generation of the comprehensive scoring of all possible pairs of elements (i.e., CPM). This has given rise to context where we can appraise the relative value of one element with respect to all others (e.g., how does the score of one potential protein target compare to the scores of all possible targets?). RP extends this further by examining each perspective reciprocally (e.g., in-context score of the target and in-context score of the drug), such that the score of the pair can be placed in context. The RP framework estimates from all predicted scores a localized baseline on a per-element basis (e.g. per-drug and/or per-protein) enabling the computation of a number of rank-order metrics.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33778, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table_caption"}, "text": "The 14 RP features derived from DTI pair-specific one-to-all score curves.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33853, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">Feature generic name</th><th align=\"left\">Short name</th><th align=\"left\">Type</th><th align=\"left\">Description</th></tr></thead><tbody><tr><td align=\"left\">Y-in-X-percentile</td><td align=\"left\"><italic>ryx</italic></td><td align=\"left\">Rank</td><td align=\"left\">Percentile of target Y among all the predictions for drug X</td></tr><tr><td align=\"left\">X-in-Y-percentile</td><td align=\"left\"><italic>rxy</italic></td><td align=\"left\">Rank</td><td align=\"left\">Percentile of drug X among all the predictions for target Y</td></tr><tr><td align=\"left\">Adjusted reciprocal rank order</td><td align=\"left\">ARRO</td><td align=\"left\">Rank</td><td align=\"left\">Reciprocal product of <italic>rxy</italic> and <italic>ryx</italic></td></tr><tr><td align=\"left\">X-percentile-baseline</td><td align=\"left\"><italic>rxt</italic></td><td align=\"left\">Rank</td><td align=\"left\">Percentile rank of the target nearest to the local cutoff value of drug X</td></tr><tr><td align=\"left\">X-baseline</td><td align=\"left\"><italic>sxt</italic></td><td align=\"left\">Score</td><td align=\"left\">Score at the local cutoff value of drug X</td></tr><tr><td align=\"left\">Y-percentile-baseline</td><td align=\"left\"><italic>ryt</italic></td><td align=\"left\">Rank</td><td align=\"left\">Percentile rank of the drug nearest to the local cutoff value of target Y</td></tr><tr><td align=\"left\">Y-baseline</td><td align=\"left\"><italic>syt</italic></td><td align=\"left\">Score</td><td align=\"left\">Score at the local cutoff value of target Y</td></tr><tr><td align=\"left\">Percentile-difference-from-baseline-X</td><td align=\"left\"><italic>pdx</italic></td><td align=\"left\">Fold</td><td align=\"left\">Difference between <italic>rxy</italic> and <italic>rxt</italic></td></tr><tr><td align=\"left\">Percentile-difference-from- baseline-Y</td><td align=\"left\"><italic>pdy</italic></td><td align=\"left\">Fold</td><td align=\"left\">Difference between <italic>ryx</italic> and <italic>ryt</italic></td></tr><tr><td align=\"left\">Fold-difference-from-baseline-X</td><td align=\"left\"><italic>fdx</italic></td><td align=\"left\">Fold</td><td align=\"left\">Fold-difference of target Y score in drug X from baseline <italic>sxt</italic></td></tr><tr><td align=\"left\">Fold-difference-from-baseline-Y</td><td align=\"left\"><italic>fdy</italic></td><td align=\"left\">Fold</td><td align=\"left\">Fold-difference of drug X score in target Y from baseline <italic>syt</italic></td></tr><tr><td align=\"left\">SD-distance-from-mean-X</td><td align=\"left\"><italic>Stdx</italic></td><td align=\"left\">Stats</td><td align=\"left\">The number of standard deviations from the mean score in drug X</td></tr><tr><td align=\"left\">SD-distance-from-mean-Y</td><td align=\"left\"><italic> Stdy</italic></td><td align=\"left\">Stats</td><td align=\"left\">The number of standard deviations from the mean score in target Y</td></tr><tr><td align=\"left\">Original-Score</td><td align=\"left\"><inline-formula id=\"IEq46\"><alternatives><tex-math id=\"M93\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G&lt;id&gt;$$\\end{document}</tex-math><mml:math id=\"M94\"><mml:mrow><mml:mi>G</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"41598_2022_16493_Article_IEq46.gif\"/></alternatives></inline-formula></td><td align=\"left\">Score</td><td align=\"left\">The original predicted score from the component model</td></tr></tbody></table>\n"}, "text": "Feature generic name\tShort name\tType\tDescription\t \tY-in-X-percentile\tryx\tRank\tPercentile of target Y among all the predictions for drug X\t \tX-in-Y-percentile\trxy\tRank\tPercentile of drug X among all the predictions for target Y\t \tAdjusted reciprocal rank order\tARRO\tRank\tReciprocal product of rxy and ryx\t \tX-percentile-baseline\trxt\tRank\tPercentile rank of the target nearest to the local cutoff value of drug X\t \tX-baseline\tsxt\tScore\tScore at the local cutoff value of drug X\t \tY-percentile-baseline\tryt\tRank\tPercentile rank of the drug nearest to the local cutoff value of target Y\t \tY-baseline\tsyt\tScore\tScore at the local cutoff value of target Y\t \tPercentile-difference-from-baseline-X\tpdx\tFold\tDifference between rxy and rxt\t \tPercentile-difference-from- baseline-Y\tpdy\tFold\tDifference between ryx and ryt\t \tFold-difference-from-baseline-X\tfdx\tFold\tFold-difference of target Y score in drug X from baseline sxt\t \tFold-difference-from-baseline-Y\tfdy\tFold\tFold-difference of drug X score in target Y from baseline syt\t \tSD-distance-from-mean-X\tStdx\tStats\tThe number of standard deviations from the mean score in drug X\t \tSD-distance-from-mean-Y\t Stdy\tStats\tThe number of standard deviations from the mean score in target Y\t \tOriginal-Score\t\tScore\tThe original predicted score from the component model\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 35160, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "By considering a putative interaction of elements from the perspective of each of the elements within the pair, the RP framework extracts 14 pair-specific features (tabulated in Table 2) as input features to train a cascaded super learner model. These features contextualise each pair among all other predicted pairs thus making use of semi-supervised distributions and a variety of features types (including rank, fold, statistics, and score-types). Thus RP rescores the predicted DTI pair as part of a cascaded super learning model, typically an eXtreme Gradient Boosting (XGBoost) model as in Kyrollos et al. and Dick et al..", "sentences": [], "annotations": [], "relations": []}, {"offset": 35789, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To generate the cascaded RP model for each student-generated CM, each CM was used to generate the comprehensive predictions of all n(n+1)/ 2 drug-target pairs of the KIBA dataset, denoted the CM comprehensive prediction matrix (CPM), and thereby producing a predicted  affinity score for each pair (originally expressed as  when available). This large-scale prediction task made use of the high-performance compute infrastructure provided by Compute Canada to massively parallelize the prediction generation over KIBA. For the benefit of the broader research community, we release the complete set of our CM predictions over KIBA for reproducibility and from which subsequent projects might benefit (discussed in the Future Directions below).", "sentences": [], "annotations": [], "relations": []}, {"offset": 36532, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_caption"}, "text": "MUSDTI hyperparameter values.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36562, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">MUSDTI model parameter</th><th align=\"left\">DeepDTA dataset</th><th align=\"left\">DeepDTA* dataset</th><th align=\"left\">Double-cold dataset</th><th align=\"left\">Double-cold* dataset</th></tr></thead><tbody><tr><td align=\"left\">Colsample by tree</td><td align=\"left\">0.9362</td><td align=\"left\">0.7991</td><td align=\"left\">0.8988</td><td align=\"left\">0.9278</td></tr><tr><td align=\"left\">Gamma</td><td align=\"left\">1.1306</td><td align=\"left\">2.918</td><td align=\"left\">1.9723</td><td align=\"left\">4.342</td></tr><tr><td align=\"left\">Learning rate</td><td align=\"left\">0.2637</td><td align=\"left\">0.095</td><td align=\"left\">1.748</td><td align=\"left\">0.093</td></tr><tr><td align=\"left\">Max depth</td><td align=\"left\">15.0</td><td align=\"left\">13.0</td><td align=\"left\">12.0</td><td align=\"left\">12.0</td></tr><tr><td align=\"left\">Min child weight</td><td align=\"left\">2.0</td><td align=\"left\">9.000</td><td align=\"left\">4.0</td><td align=\"left\">1.0</td></tr></tbody></table>\n"}, "text": "MUSDTI model parameter\tDeepDTA dataset\tDeepDTA* dataset\tDouble-cold dataset\tDouble-cold* dataset\t \tColsample by tree\t0.9362\t0.7991\t0.8988\t0.9278\t \tGamma\t1.1306\t2.918\t1.9723\t4.342\t \tLearning rate\t0.2637\t0.095\t1.748\t0.093\t \tMax depth\t15.0\t13.0\t12.0\t12.0\t \tMin child weight\t2.0\t9.000\t4.0\t1.0\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 36854, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_foot"}, "text": "The * designation denotes the model parameters used for the ensembled MUSDTI* model prior to the cascaded application of RP.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36979, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Once predicted, the 14 RP features for each component model as defined in Table 2 were computed and a cascaded RP XGBoost model (with hyperparameters defined in Table 3) for each CM was trained. We optimized, through grid-search, five particular hyperparameters in the in the training of the MUSDTI and the MUSDTI* contributing models (colsample by tree, gamma, learning rate, max depth, and minimum child weight); these values were determined through the use of the validation sets available to each model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 37487, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To differentiate the performance attributable to the application of the context-leveraging RP to each component model versus the performance attributable to the numerical mapping procedure alone (mapping  to ), we also trained CM-specific numerical mapping (NM) models. The NM models were trained using a single feature, the predicted  of a KIBA pair, with the goal of effectively learning the non-linear mapping between  and . The numerical mapping models are a simple XGBoost model (with default hyperparameters) used to learn the non-linear mapping between two numerical domains to effectively \u201ctranslate\u201d  values to  values. This non-linear mapping procedure is expected to be learned by the cascaded MUSDTI meta-model, however, to fairly compare the performance of the meta-model with the component models, they must each be evaluated based on the output score generated for the evaluation dataset. Consequently, the CM-NM models are produced using the same meta-model training samples however only take as input the CM-generated  score and learns to map it to the ground-truth  value. The application of RP to each CM (the CM-RP models) leverages all 14 RP features.", "sentences": [], "annotations": [], "relations": []}, {"offset": 38664, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Finally, to demonstrate RP as a many-CM ensembling method with the goal of producing the best performing model (a super learner), we iteratively fused the RP-features of all CMs and trained new multi-CM-RP models, re-evaluating the performance of each. We selected the order of progressively included models based on the rank-order performance of each CM over the validation dataset. This work sought to determine the trade-off between the performance increase from progressively including CM and the computational expense in adding each. The inference rate for each model was therefore reported to express the relative runtime as an inference rate (i.e. number of predicted pairs per unit of time).", "sentences": [], "annotations": [], "relations": []}, {"offset": 39364, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "All models were fairly evaluated over the two test datasets where we considered two complimentary performance metrics; the first focused on the agreement between prediction and ground-truth, and the second on the ordering of predictions. The first evaluation metric considered is the Root Mean Squared Error (RMSE) defined as:where  is the predicted value and  is the ground-truth value. Smaller RMSE values represent better models, and vice versa.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39813, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Given that this work considers multiple datasets with differing metrics expressing DTA, the second magnitude-independent evaluation metric considered is the Concordance Index (CI), or c-score. In considering this complementary metric, we could better evaluate the specific influence of the numerical mapping and RP performance contributions. The CI is defined as the proportion of concordant pairs divided by the total number of possible evaluation pairs. Intuitively, the CI focuses on the order of the predictions rather than the magnitude of the predictions themselves. Specifically, the CI over a set of paired data expresses the probability that the predictions for two randomly drawn drug-target pairs with different labels are in the correct order, that is, that the prediction  for the larger affinity  is larger than the prediction  for the smaller affinity value . Formally:where Z represents a normalization constant and h(x) represents the step function:", "sentences": [], "annotations": [], "relations": []}, {"offset": 40780, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The CI ranges between 0.5 and 1.0, where 0.5 corresponds to a random predictor and 1.0 corresponds to perfect prediction accuracy, thus larger CI values represent better models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 40958, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Comparison with SOTA DTI methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 40991, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Finally, in order to fairly compare our proposed methods to the SOTA DTI methods, we recreated the implementation of the DeepDTA by \u00d6zt\u00fcrk et al.. This ensured that our proposed model(s) and DeepDTA had accesses to the same information through all stages of the evaluation pipeline enabling direct comparison of our results. Moreover, the DeepDTA model could also be evaluated with a cascade NM and RP layer to demonstrate improved performance resulting from these cascaded approaches.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41479, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The DeepDTA model architecture comprises two independent CNN blocks (encoders) to learn a drug SMILES representation and a protein amino acid sequence representation. Both encoders are composed of three consecutive 1D convolutional layers that feed into a max-pooling layer. The two CNN encoder outputs are then concatenated into a single vector that is passed into three fully connected layers, before ultimately producing the output affinity prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41935, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In this work, we consider three variants of the DeepDTA model, denoted DTA-BD, DTA-D, and DTA-DBD to represent whether the model was originally trained on the BindingDB dataset, Davis dataset, and joint BindingDB & Davis datasets, respectively. Each of these SOTA models, much as with the CMs, were treated independently of each other through the prediction pipeline (that is, each model was trained on either BindingDB and or the Davis datasets). For each of the experiments considered in this work the three DeepDTA- models are ranked among all CMs and visually depicted differently to emphasize their performance in relation to the student CMs. ", "sentences": [], "annotations": [], "relations": []}, {"offset": 42584, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "Results and discussion", "sentences": [], "annotations": [], "relations": []}, {"offset": 42607, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In this work, we propose several adaptations of the experimental design for producing and evaluating DTI predictors. Building upon recent deep learning advances for DTI prediction, we propose a novel evaluation framework that makes use of three commonly used benchmark DTI datasets to maximally utilize the available DTI data and incorporate a cascaded transfer learning layer to accommodate the use of differing measurements of binding affinity (i.e.  & ). Consequently, the use of a cascaded learning algorithm (such as RP) to not only learn the  non-linear mapping (i.e. numerical mapping models), but to leverage the context enabled by transfer learning these DTA measures (i.e. CM-RP), promises exciting results given that the application of RP to related bioinformatic problems has led to statistically significant improvements of predictor performance. ", "sentences": [], "annotations": [], "relations": []}, {"offset": 43468, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In formulating this project as part of senior undergraduate course pedagogy to replicate similar competitive programmatic environments to solve open or active problems, student groups generated individually unique solutions (Supplementary Materials, Table 1), each amenable to evaluation against one another and a fair SOTA DTI prediction model. Conveniently, this formulation also enabled the strict assignment of KIBA data (given it\u2019s relatively large and non-leverageble size) as part of data for use in a cascaded framework. With limited compute infrastucture, students were unable to use large-scale datasets. Given this experimental assignment of DTI pairs, our experimental design could assign the union of Davis and BindingDB pairs as a component model training set, and then define the independent KIBA dataset for the transfer learning task with specifically withheld pairs to represent the test dataset (either as a predefined DeepDTA hold-out test, or the highly conservative double-cold subset).", "sentences": [], "annotations": [], "relations": []}, {"offset": 44479, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Student-generated DTI models outperform SOTA DTI models", "sentences": [], "annotations": [], "relations": []}, {"offset": 44535, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Excitingly, in providing students with SOTA DTI models as a starting point in their own development of novel DTI predictors, they had a definitive baseline upon which they might inform their own model development and ultimately compare themselves. As described in Dick et al., establishing the project baseline as an existing SOTA DTI model spurred innovations to advance the frontier of knowledge.", "sentences": [], "annotations": [], "relations": []}, {"offset": 44934, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Ultimately, student-submitted CM models outperformed the three DeepDTA- models as reported in Tables 4 and 5. Excitingly, a number of of these student-contributed models also generate predictions on considerably faster time-frames than SOTA models as depicted in Fig. 4; models G27, G9, G37, and G25 all appear to produce predictions faster than all other models that perform similarly.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45321, "infons": {"file": null, "id": "Tab4", "section_type": "TABLE", "type": "table_caption"}, "text": "Component and MUSDTI model performance evaluated over the validation and test datasets using concordance index.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45433, "infons": {"file": null, "id": "Tab5", "section_type": "TABLE", "type": "table_caption"}, "text": "Component and MUSDTI model performance evaluated over the validation and test datasets using root mean squared error.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45551, "infons": {"file": "41598_2022_16493_Fig4_HTML.jpg", "id": "Fig4", "section_type": "FIG", "type": "fig_caption"}, "text": "Inference rates of each component model measured over random subsets of 1 million pairs.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45640, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In considering the reported CM performance of the validation and test sets for the DTA-defined and double-cold tests sets, numerous student CM models outperformed the DeepDTA- models (Tables 4, 5). For example, in Table 4 summarizing CI results (higher values are better) the highest DeepDTA- model performance in the CM column is 0.5656 for DTA-DBD (as expected) while several other student-contributed CM models achieve G3:0.5978, G10:0.5787, G14:0.6017, G31:0.5801, amongst others (Table 4).", "sentences": [], "annotations": [], "relations": []}, {"offset": 46135, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Similarly, the results measured by RMSE over each of the test datasets corroborate the previous findings: numerous student-contributed findings achieve a smaller RMSE over the DeepDTA- models. However, given that the reported RMSE is obtained from the  prediction of a  ground-truth value, these values with differing DTA definition are not meaningful. Rather, the numerically-mapped RMSE values (i.e. CM-NM & CM-RP) are of greater significance (Table 4).", "sentences": [], "annotations": [], "relations": []}, {"offset": 46591, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We note that the student-defined CM models effectively represent a large-scale search of hyperparameter space including variable protein amino acid and drug SMILES sequence encodings, variable fusion of single or multiple models, and optimized hyperparameter values according to specific training strategies. Collectively, the exploration of these parameters allo us to draw a various of conclusions based on the selected parameters. Foremost, we note that 9/28 (approximately one third) of student models chose to utilize the same protein and drug 1-D CNN encodings as in the DeepDTA model, however, each group opted to vary other aspects of their prediction pipeline. One notable example, the G3 model, leveraged a fusion-based prediction framework integrating multiple encoding paradigms as part of their methods (differing from the CNN-only approach proposed by \u00d6zt\u00fcrk et al.) and this model consistently demonstrated one of the highest performances amongst the CMs over both datasets. This finding is consistent with the generally known utility of integrating a fusion of multiple complimentary data representations to train a given learning algorithm. Excitingly, various insights can be drawn based on the wide-scale overview of this explored feature space and, to that end, we list the complete CM implemetation details in Supplementary Table 1 and performance results in Tables 4 and 5.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47989, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Learning a numerical-mapper is sufficient for domain transfer", "sentences": [], "annotations": [], "relations": []}, {"offset": 48051, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The three benchmark datasets considered in this work expressed DTA in either  (BindingDB & Davis) or  (KIBA), and to integrate all three in a combined end-to-end framework requires a non-linear mapping to express predicted values in the appropriate domain and can be learned through an additional learned machine learning layer. For each of the KIBA pairs, each CM (and DTA model) predicted a  score for the original  value enabling the learning of a CM-specific numerical mapper translating the  prediction to it\u2019s  counterpart. This has considerable demonstrated impact on the magnitude-specific RMSE results reported in Table 5. For each of the CM models over the validation and test datasets of both the DeepDTA-Defined and Double-Cold datasets, the CM-NM values show a marked improvement (Table 5 where darker values indicate improved performance).", "sentences": [], "annotations": [], "relations": []}, {"offset": 48907, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Conversely, when measured using CI, the application of the numerical mapping layer has little to no effect on performance. Since the CI ignores the magnitude of the predicted binding affinity and rather reports the relative ordering of predictions, the remapping of predicted binding affinities to an alternative numerical domain contributes little to the model performance since no additional information has been incorporated (Table 4). However, when the RP layer is applied, which incorporates the context-derived features, a considerable improvement of performance is observed (Table 4).", "sentences": [], "annotations": [], "relations": []}, {"offset": 49499, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Reciprocal perspective improves all component model performance", "sentences": [], "annotations": [], "relations": []}, {"offset": 49563, "infons": {"file": "41598_2022_16493_Fig5_HTML.jpg", "id": "Fig5", "section_type": "FIG", "type": "fig_caption"}, "text": "Component model performance improvement from the reciprocal perspective cascaded layer over the double-cold dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 49680, "infons": {"file": "41598_2022_16493_Fig6_HTML.jpg", "id": "Fig6", "section_type": "FIG", "type": "fig_caption"}, "text": "Component model performance improvement from the reciprocal perspective cascaded layer over the DeepDTA-defined dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 49801, "infons": {"file": "41598_2022_16493_Fig7_HTML.jpg", "id": "Fig7", "section_type": "FIG", "type": "fig_caption"}, "text": "Experimental results over the DeepDTA-defined datasets when incrementally incorporating reciprocal perspective component models compared to the SOTA DeepDTA models. The top-performing combined models were circled in the figure (top-20 models) and the first (top-10 models) represent the performance of the proposed MUSDTI model even when the later combined models represent a marginally higher performance. We opted for the component model ensemble that represented the plateaued performance of component models.", "sentences": [], "annotations": [], "relations": []}, {"offset": 50314, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The application of a cascaded RP layer to all student and SOTA models resulted in a considerable improvement in performance. For both the DeepDTA-Defined and Double-Cold validation and test datasets, the CM-RP model results produced a notable increase in performance, whether measured by RMSE or CI (Tables 4, 5; Figs. 5, 6, 7). Most notably, among all reported results, several student-produced models outperform the best ranked DeepDTA model. Interestingly, all models benefit from RP to achieve an approximately similar level of performance, regardless of the metric used. Most importantly, the relative performance between the CM-NM and CM-RP results suggest that the observed performance gain is attributable to the leveraging of context-specific features and not to the domain transfer alone.", "sentences": [], "annotations": [], "relations": []}, {"offset": 51113, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Promisingly, these results suggest that any existing SOTA DTI model could benefit from the application of RP as a cascaded layer. These findings corroborate the findings presented in Kyrollos et al. applying RP to miRNA-target prediction where 26 unique SOTA predictors were significantly improved. In an extension of prior work, we sought to not only demonstrate RP as a means of learning a domain transform and to improve performance, but to additionally leverage the CM-specific RP features as part of an ensembled method.", "sentences": [], "annotations": [], "relations": []}, {"offset": 51639, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "The MUSDTI model outperforms the state-of-the-art", "sentences": [], "annotations": [], "relations": []}, {"offset": 51689, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Prior DTI literature suggests that the incorporation of individual DTI predictors into an ensemble will outperform those individual models. The work of Dick et al. on predicting protein-protein interactions (PPI) between SARS-CoV-2 and humans demonstrated that RP could be used to ensemble two PPI predictors (the Protein-protein Interaction Prediction Engine [PIPE4] and the Scoring PRotein INTeractions [SPRINT] models) to produce an RP fusion model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 52142, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Here, we sought to expand upon this work to produce a multi-CM DTI ensemble model using Reciprocal Perspective. In the pursuit of the highest-possible performing model, we sought to define the Meta-Undergraduate Student DTI (MUSDTI) model as a meta-model fusing the top-performing CM-RP models. To that end, we incrementally incorporated the derived RP features of each component model (ranked according to their relative performance over the validation dataset) and visualised the performance increase in Fig. 7.", "sentences": [], "annotations": [], "relations": []}, {"offset": 52656, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Most excitingly, we note that the inclusion of a single student CM model (without the RP cascade) outperforms the DeepDTA- and, as expected, the iterative incorporation of CM-RP features results in the progressive performance increase of the ensembled model. The incorporation of the  top-ranking student-produced CM models within the RP ensemble appears to have plateaued however we additionally note that the model produced at  achieved the maximum recorded performance (Fig. 7). As a trade-off between performance increase (seemingly within random variation at the plateau) and computational expense, we determined that the MUSDTI model would be defined by the ensemble of these top-10 ranking models and all subsequent performance evaluation is made with this model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 53427, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The MUSDTI model outperformed not only the student CM and SOTA DeepDTA- models, but all of their CM-NM and CM-RP variants, supported over the two datasets and performance metrics (Tables 4, 5; Figs. 5, 7). Our findings suggest that ensembled RP-DTI models establish a new SOTA for DTI prediction and provide quality predictions for even the most challenging datasets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 53795, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Defining a double-cold evaluation framework for future DTI studies", "sentences": [], "annotations": [], "relations": []}, {"offset": 53862, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In this work, we proposed a novel experimental design that integrates the three most common DTI benchmark datasets, despite their differing DTA measures. This experimental design dedicated two benchmark datasets (Davis and BindingDB) with compatible DTA measures (i.e. ), which when combined, represent a sizable training dataset that is (by definition) greater than what they represent individually. The third (KIBA) dataset, with  values, is then used for the training of a cascaded model by means of transfer learning with the context-leveraging RP framework to further improve performance. This experimental framework that maximally incorporates the three benchmark datasets enabled the definition of a highly conservative double-cold dataset for which none of the pairs contained a drug compound or protein sequence occurring within the training dataset (also known as \u201cDouble-Cold\u201d). To our knowledge, this is the first definition of such a conservative test dataset while leveraging the Davis, BindingDB, and KIBA datasets combined in such a cascade to report performance. We recommend that the training and evaluation framework introduced within this work serve as the basis for future DTI work and we further support this initiative by providing the data and component models generated from this work to the research community.", "sentences": [], "annotations": [], "relations": []}, {"offset": 55203, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We would like to emphasize that, by definition, the double-cold formulation (where neither the drug SMILES nor protein amino acid sequence in the test set is present in the training or validation sets) is the most conservative experimental design for pairwise model evaluation. In fact, this conservative experimental design was initially proposed for protein-protein interaction prediction tasks in the work of Park and Marcotte. Three levels of difficulty are defined in this critique of evaluation schemes: the easiest has both elements of a test pair appearing in the training dataset, the intermediate has either element of a test pair appearing in the training set, and the hardest has neither of the elements appearing in the training set. The hardest experimental design represents the most authentic evaluation of a model\u2019s performance given that it expresses the model\u2019s expected performance when used to make predictions for completely novel elements (never seen protein amino acid sequences and/or never seen drug SMILES).", "sentences": [], "annotations": [], "relations": []}, {"offset": 56242, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "To quantitatively express the diversity of the double-cold dataset, we measured the cosine distance between the centroid of the SMILES samples in the training and test set when numerically embedded in their latent space. As expected, the similarity between the double-cold test and training sets with (mean similarity 0.1694 and variance 0.0047) is smaller than the similarity between the DeepDTA-defined test and training sets (mean similarity 0.1711 and variance 0.0714) since higher values are more similar and lower values are more dissimilar. Thus, the double-cold dataset represents an adequately diverse dataset with which to benchmark other DTI prediction methods.", "sentences": [], "annotations": [], "relations": []}, {"offset": 56915, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "As summarized in Tables 4 and 5, when comparing the performance between the DeepDTA-Defined and Double-Cold datasets, we note that typically the CM-RP validation results outperforms the CM-RP test results (as expected) and that the ensembled MUSDTI model outperformed all models. We note that the CM-NM entry for MUSDTI is the reported result of the ensembled model using only a numerical mapping component and not the full context-leveraging feature set that the RP framework provides.", "sentences": [], "annotations": [], "relations": []}, {"offset": 57402, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Reciprocal perspective enables low computation transfer learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 57467, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In this work we demonstrate that domain transfer learning can be easily achieved by learning a CM-NM model to re-score predicted DTI predictions to alternative domains. As expected, the simple non-linear translation from one distribution to another (e.g. ) while impactful on a magnitude-focused measure, such as RMSE, doesn\u2019t actually incorporate additional learning-specific information, as exemplified in our rank-focused measurement, CI.", "sentences": [], "annotations": [], "relations": []}, {"offset": 57911, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "However, the application of the context-leveraging Reciprocal Perspective framework not only provides domain translation but also considerably improves the performance of the original model through a downstream cascade. With 14 computed features contributing to this improved model, RP effectively enables a low computation transfer learning layer that may ultimately be used for other domain translation tasks. Excitingly, this work represents the third bioinformatic-related application with demonstrated improvement due to the Reciprocal Perspective framework. Promisingly, the domain-agnostic nature of RP suggests that it may be broadly applied to numerous pair-wise applications, even beyond bioinformatics.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58625, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "On the potential for overfitting", "sentences": [], "annotations": [], "relations": []}, {"offset": 58658, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "An important consideration for the use of the multiple dataset and multiple contributing models is the potential risk of overfitting. In this work, careful steps to de-duplicate datasets were taken to ensure that no replicated samples appeared in multiple independent sets. In fact, the formulation of a super learner training protocol (which relates to stacked generalization in general) avoid the possibility of overfitting by splitting all data into training, validation, and test sets. The CMs are initially created using explicit training data and the input of the downstream meta-model is the out-of-sample predictions. By then training the meta-model on out-of-sample predictions of the CMs, the meta-modal learns how to simultaneously \u201ccorrect\u201d the out-of-sample predictions and how to best integrate these predictions from multiple CMs to produce its final prediction. Finally, to fully assess the capabilities of the meta-model, it is then evaluated on a final independent test set not used for either the training of the CMs nor the meta-model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 59718, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In interpreting the quantitative results of the CM and MUDSTI on the validation and test sets for both the double-cold and DeepDTA-defined sets, we note that test set results are typically lower than the validation set results and within the general range of performance suggesting that these models are not likely to be overfit (Tables 4, 5).", "sentences": [], "annotations": [], "relations": []}, {"offset": 60062, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "SHAP analysis reveals variable-contribution CMs and RP features", "sentences": [], "annotations": [], "relations": []}, {"offset": 60126, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "This work sought to demonstrate the utility of using RP for combining multiple deep learning component models into a single, high-performing MUSDTI model, however, we note that the use of (deep) learning models typically represent black box from which little actionable knowledge may be derived. Thankfully, the machine learning research community is actively engaged in developing explanatory artificial intelligence (XAI) methods that help describe what it is that a particular model focuses upon when generating a given prediction.", "sentences": [], "annotations": [], "relations": []}, {"offset": 60661, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "One such XAI framework is the SHAPley Additive exPlanations (SHAP) visualization tool that can make a machine learning model more explainable by visualizing the model output. Shapley values are a concept originating from the field of cooperative game theory whose objective is to quantify a given player\u2019s contribution to a game. Shapley values are derived from gameplay contexts where n players collectively seek to obtain a reward p which is intended for fair distribution among the n players according to the individual contribution; such a contribution is known as the Shapley value. In the context of XAI, Shapley values are determined through a heuristic game-theoretic framework to quantify the level of contribution a given feature has on a particular model prediction and to determine these contributions on average.", "sentences": [], "annotations": [], "relations": []}, {"offset": 61489, "infons": {"file": "41598_2022_16493_Fig8_HTML.jpg", "id": "Fig8", "section_type": "FIG", "type": "fig_caption"}, "text": "Shapeley additive features analysis. The x-axis is sorted left-to-right in increasing magnitude of SHAP value summed over the column while the y-axis is sorted top-down in increasing magnitude of SHAP value summed over the row. Emanating out from the bottom-right are the models and features with increasingly lesser impact on the model decision. Only the top-10 models contributing to the MUSDTI model are depicted along all 14 RP features.", "sentences": [], "annotations": [], "relations": []}, {"offset": 61931, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "For the purposes of this work, we are interested in better understanding which of the RP features are most impactful on model performance as well as which of the CMs contributes most to the MUSDTI prediction. Thus, we can conceptualize visualizing the impact of individual features and individual CMs as a matrix/heatmap representing the average SHAP value between that feature and the specific model (Fig. 8). We note that no one model (column-wise) nor RP feature (row-wise) appears to dominate all others and rather, a mixture in the diversity of both the contributing CMs and the features that they provide to support the meta-model output are necessary (Fig. 8). Nonetheless, there are a subset of RP features and component models that appear to contribute more than others. The precise definitions for each feature is listed in Table 1 and the implementation details for each of the CM models are explicitly stated within a Table 1 in the supplementary material. For any high-contributing component model, the specific drug and protein encoding method can be determined from the table and the hyperparameters used for its training are listed. Ultimately, this analysis demonstrates that different models will variably rely on different features to inform the final prediction which is consistent with the utility of ensemble methods that seek to integrate multiple learners.", "sentences": [], "annotations": [], "relations": []}, {"offset": 63312, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Didactics tailored towards resolving real-world problems using limited computational environments", "sentences": [], "annotations": [], "relations": []}, {"offset": 63410, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Finally, this work represents an impactful application of senior undergraduate student pedagogy to an open research application. We demonstrated that teams of undergraduate students can, with very limited (and free) cloud-based resources, produce DTI models that rival the state-of-the-art. Most excitingly is the prospect that (under)graduate pedagogy represents a seemingly untapped resource from which advances at the frontier of knowledge may be gleaned. Given the success reported from the MetaStudent work of Dr. Burkhard Rost and exemplified in the generalized Kaggle framework, there exists ample opportunity to engage (under)graduate students in meaningful ways to advance the state-of-the-art of various applications.", "sentences": [], "annotations": [], "relations": []}, {"offset": 64138, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "To that end, we strongly encourage researchers and fellow educators to follow from this work and tailor course-specific didactics to promote engagement in projects that may advance the frontier of knowledge. MUSDTI, while benefiting from the application of RP to student-contributed models, could serve in a similar way as a pedagogical example to student- or SOTA-generated models in other application domains. With a well-defined computational framework from which students might explore and evaluate their work, our experiences may be translated to other open research questions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 64721, "infons": {"section_type": "CONCL", "type": "title_1"}, "text": "Conclusion", "sentences": [], "annotations": [], "relations": []}, {"offset": 64732, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "The identification of novel DTI is critical to drug discovery and drug repurposing, and represents an open research question for which the research community is actively seeking novel solutions. Various databases contributed to experimentally derived DTI predictors that can be effectively leveraged to achieve SOTA performance, even if different measures of interaction are used in each of the databases.", "sentences": [], "annotations": [], "relations": []}, {"offset": 65138, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "In this work, we formulated a DTI competition as part of the coursework for an senior undergraduate machine learning course and challenged students to generate component DTI models that might surpass SOTA models and ultimately combined these component models as part of a meta-mode (denoted MUDTI) using the Reciprocal Perspective framework. Consequently, our proposed MUSDTI model represents the new SOTA DTI model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 65555, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "Our work demonstrated that RP can considerably improve SOTA DTI predictors, that our novel double-cold experimental test dataset (in theory) is better suited to emergent DTI models, that our novel MUSDTI model outperforms SOTA models, that generally, the RP framework can improve individual models as an ensembling method, that RP can be effectively leveraged for the combination of multiple experts (for ), and that RP can be used in a domain-mapping strategy. Ultimately, we demonstrate that, much like Hamp et al., student didactics can be tailored to open research applications. Ultimately, this work introduces a novel DTI predictor and revelations for the bioinformatics community in general. This work will share the double-cold test dataset as well as the component models and their domain-transfered predictions of the KIBA dataset to enable future research.", "sentences": [], "annotations": [], "relations": []}, {"offset": 66423, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 66449, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Publisher's note", "sentences": [], "annotations": [], "relations": []}, {"offset": 66466, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "sentences": [], "annotations": [], "relations": []}, {"offset": 66585, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "These authors contributed equally: Eric D. Cosoreanu, Joseph Dooley, Joshua S. Fryer, Shaun M. Gordon, Nikhil Kharbanda, Martin Klamrowski, Patrick N. L. LaCasse, Thomas F. Leung, Muneeb A. Nasir, Chang Qiu, Aisha S. Robinson, Derek Shao, Boyan R. Siromahov, Evening Starlight, Christophe Tran, Christopher Wang, Yu-Kai Yang.", "sentences": [], "annotations": [], "relations": []}, {"offset": 66911, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 66937, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The online version contains supplementary material available at 10.1038/s41598-022-16493-9.", "sentences": [], "annotations": [], "relations": []}, {"offset": 67029, "infons": {"section_type": "AUTH_CONT", "type": "title"}, "text": "Author contributions", "sentences": [], "annotations": [], "relations": []}, {"offset": 67050, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "K.D., D. G. K., and J.R.G conceived of the study; K.D. and D.G.K collected the data and prepared all figures; K. D., D. G. K., E. D. C., J. D., J. S. F., S. M. G., N. K., M. K., P. N. L. L., T. F. L., M. A. N., C. Q., A. S. R., D. S., B. R. S., E. S., C. T., C. W., and Y. Y. developed the algorithms and analysed the results. All authors reviewed and approved the final manuscript.", "sentences": [], "annotations": [], "relations": []}, {"offset": 67433, "infons": {"section_type": "SUPPL", "type": "title"}, "text": "Data availability", "sentences": [], "annotations": [], "relations": []}, {"offset": 67451, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The materials provided to students in support of this work are available in the following Github repository: https://github.com/jrgreen7/SYSC4906/tree/master/F2020.", "sentences": [], "annotations": [], "relations": []}, {"offset": 67616, "infons": {"section_type": "COMP_INT", "type": "title"}, "text": "Competing interests", "sentences": [], "annotations": [], "relations": []}, {"offset": 67636, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare no competing interests.", "sentences": [], "annotations": [], "relations": []}, {"offset": 67680, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 67691, "infons": {"section_type": "REF", "type": "ref"}, "text": "Beck, B.\u00a0R., Shin, B., Choi, Y., Park, S. & Kang, K. Predicting commercially available antiviral drugs that may act on the novel coronavirus (sars-cov-2) through a drug-target interaction deep learning model. Comput. Struct. Biotechnol. J. (2020).", "sentences": [], "annotations": [], "relations": []}, {"offset": 67940, "infons": {"fpage": "3036", "lpage": "3043", "name_0": "surname:Van Laarhoven;given-names:T", "name_1": "surname:Nabuurs;given-names:SB", "name_2": "surname:Marchiori;given-names:E", "pub-id_pmid": "21893517", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "27", "year": "2011"}, "text": "Gaussian interaction profile kernels for predicting drug-target interaction", "sentences": [], "annotations": [], "relations": []}, {"offset": 68016, "infons": {"fpage": "1", "lpage": "10", "name_0": "surname:Cao;given-names:D-S", "pub-id_pmid": "23101647", "section_type": "REF", "source": "Anal. Chim. Acta", "type": "ref", "volume": "752", "year": "2012"}, "text": "Large-scale prediction of drug-target interactions using protein sequences and drug topological structures", "sentences": [], "annotations": [], "relations": []}, {"offset": 68123, "infons": {"fpage": "2304", "lpage": "2310", "name_0": "surname:G\u00f6nen;given-names:M", "pub-id_pmid": "22730431", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "28", "year": "2012"}, "text": "Predicting drug-target interactions from chemical and genomic kernels using bayesian matrix factorization", "sentences": [], "annotations": [], "relations": []}, {"offset": 68229, "infons": {"fpage": "3399", "lpage": "3409", "name_0": "surname:Cobanoglu;given-names:MC", "name_1": "surname:Liu;given-names:C", "name_2": "surname:Hu;given-names:F", "name_3": "surname:Oltvai;given-names:ZN", "name_4": "surname:Bahar;given-names:I", "pub-id_pmid": "24289468", "section_type": "REF", "source": "J. Chem. Inf. Model.", "type": "ref", "volume": "53", "year": "2013"}, "text": "Predicting drug-target interactions using probabilistic matrix factorization", "sentences": [], "annotations": [], "relations": []}, {"offset": 68306, "infons": {"fpage": "669", "lpage": "681", "name_0": "surname:Cao;given-names:D-S", "section_type": "REF", "source": "Mol. Inf.", "type": "ref", "volume": "33", "year": "2014"}, "text": "Computational prediction of drug target interactions using chemical, biological, and network features", "sentences": [], "annotations": [], "relations": []}, {"offset": 68408, "infons": {"fpage": "1", "lpage": "11", "name_0": "surname:\u00d6zt\u00fcrk;given-names:H", "name_1": "surname:Ozkirimli;given-names:E", "name_2": "surname:\u00d6zg\u00fcr;given-names:A", "pub-id_pmid": "26817711", "section_type": "REF", "source": "BMC Bioinformatics", "type": "ref", "volume": "17", "year": "2016"}, "text": "A comparative study of smiles-based compound similarity functions for drug-target interaction prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 68513, "infons": {"fpage": "D198", "lpage": "D201", "name_0": "surname:Liu;given-names:T", "name_1": "surname:Lin;given-names:Y", "name_2": "surname:Wen;given-names:X", "name_3": "surname:Jorissen;given-names:RN", "name_4": "surname:Gilson;given-names:MK", "pub-id_pmid": "17145705", "section_type": "REF", "source": "Nucleic Acids Res.", "type": "ref", "volume": "35", "year": "2007"}, "text": "Bindingdb: a web-accessible database of experimentally determined protein-ligand binding affinities", "sentences": [], "annotations": [], "relations": []}, {"offset": 68613, "infons": {"fpage": "1046", "lpage": "1051", "name_0": "surname:Davis;given-names:MI", "pub-id_pmid": "22037378", "section_type": "REF", "source": "Nat. Biotechnol.", "type": "ref", "volume": "29", "year": "2011"}, "text": "Comprehensive analysis of kinase inhibitor selectivity", "sentences": [], "annotations": [], "relations": []}, {"offset": 68668, "infons": {"fpage": "735", "lpage": "743", "name_0": "surname:Tang;given-names:J", "pub-id_pmid": "24521231", "section_type": "REF", "source": "J. Chem. Inf. Model.", "type": "ref", "volume": "54", "year": "2014"}, "text": "Making sense of large-scale kinase inhibitor bioactivity data sets: A comparative and integrative analysis", "sentences": [], "annotations": [], "relations": []}, {"offset": 68775, "infons": {"fpage": "473", "lpage": "491", "name_0": "surname:Kasabov;given-names:N", "name_1": "surname:Sidorov;given-names:IA", "name_2": "surname:Dimitrov;given-names:DS", "section_type": "REF", "source": "J. Comput. Theor. Nanosci.", "type": "ref", "volume": "2", "year": "2005"}, "text": "Computational intelligence, bioinformatics and computational biology: A brief overview of methods, problems and perspectives", "sentences": [], "annotations": [], "relations": []}, {"offset": 68900, "infons": {"section_type": "REF", "type": "ref"}, "text": "Dykstra, C., Frenking, G., Kim, K. & Scuseria, G. Theory and applications of computational chemistry: the first forty years (Elsevier) (2011).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69043, "infons": {"section_type": "REF", "type": "ref"}, "text": "Martin, R.\u00a0F. Undergraduate computational physics education: uneven history and promising future. J. Phys. Conf. Ser.759, 012005 (organizationIOP Publishing) (2016).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69210, "infons": {"fpage": "75", "lpage": "79", "name_0": "surname:Bell;given-names:RM", "name_1": "surname:Koren;given-names:Y", "section_type": "REF", "source": "ACM SIGKDD Explor. Newsl.", "type": "ref", "volume": "9", "year": "2007"}, "text": "Lessons from the netflix prize challenge", "sentences": [], "annotations": [], "relations": []}, {"offset": 69251, "infons": {"section_type": "REF", "type": "ref"}, "text": "Bennett, J. & Lanning, S. The netflix prize. In Proceedings of KDD Cup and Workshop, 2007, 35 (organizationNew York, NY, USA) (2007).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69385, "infons": {"section_type": "REF", "type": "ref"}, "text": "Zhou, Y., Wilkinson, D., Schreiber, R. & Pan, R. Large-scale parallel collaborative filtering for the netflix prize. In International conference on algorithmic applications in management, 337\u2013348 (organizationSpringer) (2008).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69614, "infons": {"section_type": "REF", "type": "ref"}, "text": "Amatriain, X. & Basilico, J. Recommender systems in industry: A netflix case study. In Recommender systems handbook, 385\u2013419 (Springer) (2015).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69760, "infons": {"section_type": "REF", "type": "ref"}, "text": "T\u00f6scher, A., Jahrer, M. & Bell, R.\u00a0M. The bigchaos solution to the netflix grand prize. Netflix prize documentation 1\u201352 (2009).", "sentences": [], "annotations": [], "relations": []}, {"offset": 69893, "infons": {"section_type": "REF", "type": "ref"}, "text": "Piotte, M. & Chabbert, M. The pragmatic theory solution to the netflix grand prize. Netflix prize documentation (2009).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70013, "infons": {"section_type": "REF", "type": "ref"}, "text": "Kittler, J. & Roli, F. Multiple Classifier Systems: First International Workshop, MCS 2000 Cagliari, Italy, June 21\u201323, 2000 Proceedings (Springer) (2003).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70171, "infons": {"section_type": "REF", "type": "ref"}, "text": "Van\u00a0der Laan, M.\u00a0J., Polley, E.\u00a0C. & Hubbard, A.\u00a0E. Super learner. Statistical applications in genetics and molecular biology 6 (2007).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70311, "infons": {"fpage": "1", "lpage": "10", "name_0": "surname:Koren;given-names:Y", "section_type": "REF", "source": "Netflix Prize Docum.", "type": "ref", "volume": "81", "year": "2009"}, "text": "The bellkor solution to the netflix grand prize", "sentences": [], "annotations": [], "relations": []}, {"offset": 70359, "infons": {"section_type": "REF", "type": "ref"}, "text": "Allio, R.\u00a0J. Ceo interview: the innocentive model of open innovation. Strategy & Leadership (2004).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70460, "infons": {"section_type": "REF", "type": "ref"}, "text": "Tauchert, C., Buxmann, P. & Lambinus, J. Crowdsourcing data science: A qualitative analysis of organizations\u2019 usage of kaggle competitions. In Proceedings of the 53rd Hawaii international conference on system sciences (2020).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70688, "infons": {"section_type": "REF", "type": "ref"}, "text": "Friedberg, I., Wass, M.\u00a0N., Mooney, S.\u00a0D. & Radivojac, P. Ten simple rules for a community computational challenge (2015).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70813, "infons": {"section_type": "REF", "type": "ref"}, "text": "Pereira, J. et\u00a0al. High-accuracy protein structure prediction in casp14. Proteins: Structure, Function, and Bioinformatics (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 70945, "infons": {"fpage": "1197", "lpage": "1201", "name_0": "surname:Andreoletti;given-names:G", "name_1": "surname:Pal;given-names:L.\u00a0R", "name_2": "surname:Moult;given-names:J", "name_3": "surname:Brenner;given-names:S.\u00a0E", "pub-id_pmid": "31334884", "section_type": "REF", "source": "Hum. Mutat.", "type": "ref", "volume": "40", "year": "2019"}, "text": "Reports from the fifth edition of cagi: The critical assessment of genome interpretation", "sentences": [], "annotations": [], "relations": []}, {"offset": 71034, "infons": {"fpage": "1", "lpage": "23", "name_0": "surname:Zhou;given-names:N", "pub-id_pmid": "30606230", "section_type": "REF", "source": "Genome Biol.", "type": "ref", "volume": "20", "year": "2019"}, "text": "The cafa challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens", "sentences": [], "annotations": [], "relations": []}, {"offset": 71180, "infons": {"section_type": "REF", "type": "ref"}, "text": "Callaway, E. \u2019it will change everything\u2019: Deepmind\u2019s ai makes gigantic leap in solving protein structures. Nature 203\u2013204 (2020).", "sentences": [], "annotations": [], "relations": []}, {"offset": 71318, "infons": {"section_type": "REF", "type": "ref"}, "text": "Jumper, J. et\u00a0al. Highly accurate protein structure prediction with alphafold. Nature 1\u201311 (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 71420, "infons": {"section_type": "REF", "type": "ref"}, "text": "Hamp, T. et\u00a0al. Homology-based inference sets the bar high for protein function prediction. BMC Bioinf.14, 1\u201310 (2013).", "sentences": [], "annotations": [], "relations": []}, {"offset": 71543, "infons": {"section_type": "REF", "type": "ref"}, "text": "Dick, K., Kyrollos, D.\u00a0G. & Green, J.\u00a0R. Machine learning pedagogy to support the research community. In Proceedings of the 2021 ACM SIGPLAN International Symposium on SPLASH-E, 43\u201348 (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 71739, "infons": {"section_type": "REF", "type": "ref"}, "text": "Bernhofer, M. et\u00a0al. Predictprotein-predicting protein structure and function for 29 years. bioRxiv (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 71848, "infons": {"fpage": "2208", "name_0": "surname:Chen;given-names:R", "name_1": "surname:Liu;given-names:X", "name_2": "surname:Jin;given-names:S", "name_3": "surname:Lin;given-names:J", "name_4": "surname:Liu;given-names:J", "section_type": "REF", "source": "Molecules", "type": "ref", "volume": "23", "year": "2018"}, "text": "Machine learning for drug-target interaction prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 71904, "infons": {"fpage": "325", "lpage": "337", "name_0": "surname:Pahikkala;given-names:T", "pub-id_pmid": "24723570", "section_type": "REF", "source": "Brief. Bioinform.", "type": "ref", "volume": "16", "year": "2015"}, "text": "Toward more realistic drug-target interaction predictions", "sentences": [], "annotations": [], "relations": []}, {"offset": 71962, "infons": {"section_type": "REF", "type": "ref"}, "text": "Goodfellow, I. et\u00a0al. Generative adversarial nets. Adv. Neural Inf. Process. Syst.27 (2014).", "sentences": [], "annotations": [], "relations": []}, {"offset": 72056, "infons": {"fpage": "1243", "name_0": "surname:Zhao;given-names:L", "name_1": "surname:Wang;given-names:J", "name_2": "surname:Pang;given-names:L", "name_3": "surname:Liu;given-names:Y", "name_4": "surname:Zhang;given-names:J", "pub-id_pmid": "31993067", "section_type": "REF", "source": "Front. Genet.", "type": "ref", "volume": "10", "year": "2020"}, "text": "Gansdta: Predicting drug-target binding affinity using gans", "sentences": [], "annotations": [], "relations": []}, {"offset": 72116, "infons": {"fpage": "4633", "lpage": "4642", "name_0": "surname:Abbasi;given-names:K", "pub-id_pmid": "32462178", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "36", "year": "2020"}, "text": "Deepcda: deep cross-domain compound-protein affinity prediction through lstm and convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 72227, "infons": {"fpage": "i821", "lpage": "i829", "name_0": "surname:\u00d6zt\u00fcrk;given-names:H", "name_1": "surname:\u00d6zg\u00fcr;given-names:A", "name_2": "surname:Ozkirimli;given-names:E", "pub-id_pmid": "30423097", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "34", "year": "2018"}, "text": "Deepdta: deep drug-target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 72281, "infons": {"fpage": "1", "lpage": "9", "name_0": "surname:Shim;given-names:J", "name_1": "surname:Hong;given-names:Z-Y", "name_2": "surname:Sohn;given-names:I", "name_3": "surname:Hwang;given-names:C", "pub-id_pmid": "33414495", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "11", "year": "2021"}, "text": "Prediction of drug-target binding affinity using similarity-based convolutional neural network", "sentences": [], "annotations": [], "relations": []}, {"offset": 72376, "infons": {"section_type": "REF", "type": "ref"}, "text": "Kao, P.-Y., Kao, S.-M., Huang, N.-L. & Lin, Y.-C. Toward robust drug-target interaction prediction via ensemble modeling and transfer learning. arXiv preprint arXiv:2107.00719 (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 72560, "infons": {"fpage": "1", "lpage": "12", "name_0": "surname:Dick;given-names:K", "name_1": "surname:Green;given-names:JR", "pub-id_pmid": "29311619", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "8", "year": "2018"}, "text": "Reciprocal perspective for improved protein-protein interaction prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 72635, "infons": {"fpage": "1", "lpage": "13", "name_0": "surname:Kyrollos;given-names:DG", "name_1": "surname:Reid;given-names:B", "name_2": "surname:Dick;given-names:K", "name_3": "surname:Green;given-names:JR", "pub-id_pmid": "31913322", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "10", "year": "2020"}, "text": "Rpmirdip: Reciprocal perspective improves mirna targeting prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 72704, "infons": {"section_type": "REF", "type": "ref"}, "text": "Huang, K. et\u00a0al. Deeppurpose: A deep learning library for drug-target interaction prediction. Bioinformatics (2020).", "sentences": [], "annotations": [], "relations": []}, {"offset": 72822, "infons": {"section_type": "REF", "type": "ref"}, "text": "Randles, B.\u00a0M., Pasquetto, I.\u00a0V., Golshan, M.\u00a0S. & Borgman, C.\u00a0L. Using the jupyter notebook as a tool for open science: An empirical study. In 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL), 1\u20132 (organizationIEEE) (2017).", "sentences": [], "annotations": [], "relations": []}, {"offset": 73063, "infons": {"fpage": "1", "lpage": "14", "name_0": "surname:He;given-names:T", "name_1": "surname:Heidemeyer;given-names:M", "name_2": "surname:Ban;given-names:F", "name_3": "surname:Cherkasov;given-names:A", "name_4": "surname:Ester;given-names:M", "section_type": "REF", "source": "J. Cheminf.", "type": "ref", "volume": "9", "year": "2017"}, "text": "Simboost: A read-across approach for predicting drug-target binding affinities using gradient boosting machines", "sentences": [], "annotations": [], "relations": []}, {"offset": 73175, "infons": {"fpage": "1140", "lpage": "1147", "name_0": "surname:Nguyen;given-names:T", "pub-id_pmid": "33119053", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "37", "year": "2021"}, "text": "Graphdta: Predicting drug-target binding affinity with graph neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 73252, "infons": {"fpage": "e11117", "name_0": "surname:Dick;given-names:K", "name_1": "surname:Chopra;given-names:A", "name_2": "surname:Biggar;given-names:K.\u00a0K", "name_3": "surname:Green;given-names:J.\u00a0R", "pub-id_pmid": "33868814", "section_type": "REF", "source": "PeerJ", "type": "ref", "volume": "9", "year": "2021"}, "text": "Multi-schema computational prediction of the comprehensive sars-cov-2 vs. human interactome", "sentences": [], "annotations": [], "relations": []}, {"offset": 73344, "infons": {"fpage": "1", "lpage": "4", "name_0": "surname:Chen;given-names:T", "section_type": "REF", "source": "R package version 0.4-2", "type": "ref", "volume": "1", "year": "2015"}, "text": "Xgboost: extreme gradient boosting", "sentences": [], "annotations": [], "relations": []}, {"offset": 73379, "infons": {"section_type": "REF", "type": "ref"}, "text": "Pu, Y., Li, J., Tang, J. & Guo, F. Deepfusiondta: drug-target binding affinity prediction with information fusion and hybrid deep-learning ensemble model. IEEE/ACM Trans. Comput. Biol. Bioinf. (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 73580, "infons": {"fpage": "1", "lpage": "15", "name_0": "surname:Dick;given-names:K", "pub-id_pmid": "31913322", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "10", "year": "2020"}, "text": "Pipe4: Fast ppi predictor for comprehensive inter-and cross-species interactomes", "sentences": [], "annotations": [], "relations": []}, {"offset": 73661, "infons": {"fpage": "1", "lpage": "11", "name_0": "surname:Li;given-names:Y", "name_1": "surname:Ilie;given-names:L", "section_type": "REF", "source": "BMC Bioinf.", "type": "ref", "volume": "18", "year": "2017"}, "text": "Sprint: ultrafast protein-protein interaction prediction of the entire human interactome", "sentences": [], "annotations": [], "relations": []}, {"offset": 73750, "infons": {"fpage": "1134", "lpage": "1136", "name_0": "surname:Park;given-names:Y", "name_1": "surname:Marcotte;given-names:EM", "pub-id_pmid": "23223166", "section_type": "REF", "source": "Nat. Methods", "type": "ref", "volume": "9", "year": "2012"}, "text": "Flaws in evaluation schemes for pair-input computational predictions", "sentences": [], "annotations": [], "relations": []}, {"offset": 73819, "infons": {"fpage": "82", "lpage": "115", "name_0": "surname:Arrieta;given-names:AB", "section_type": "REF", "source": "Inf. fusion", "type": "ref", "volume": "58", "year": "2020"}, "text": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "sentences": [], "annotations": [], "relations": []}, {"offset": 73935, "infons": {"section_type": "REF", "type": "ref"}, "text": "Shapley, L.\u00a0S. A Value for n-Person Games, II, 307\u2013318 (Princeton University Press) (1953).", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]