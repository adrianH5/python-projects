[{"source": "PMC", "date": "20230128", "key": "pmc.key", "infons": {}, "documents": [{"id": "9879964", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.1038/s41598-022-27029-6", "article-id_pmc": "9879964", "article-id_pmid": "36702869", "article-id_publisher-id": "27029", "elocation-id": "1474", "kwd": "Computer science Computational science Human behaviour", "license": "Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.", "name_0": "surname:Citraro;given-names:Salvatore", "name_1": "surname:Vitevitch;given-names:Michael S.", "name_2": "surname:Stella;given-names:Massimo", "name_3": "surname:Rossetti;given-names:Giulio", "section_type": "TITLE", "title": "Subject terms", "type": "front", "volume": "13", "year": "2023"}, "text": "Feature-rich multiplex lexical networks reveal mental strategies of early language learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 92, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "Knowledge in the human mind exhibits a dualistic vector/network nature. Modelling words as vectors is key to natural language processing, whereas networks of word associations can map the nature of semantic memory. We reconcile these paradigms\u2014fragmented across linguistics, psychology and computer science\u2014by introducing FEature-Rich MUltiplex LEXical (FERMULEX) networks. This novel framework merges structural similarities in networks and vector features of words, which can be combined or explored independently. Similarities model heterogenous word associations across semantic/syntactic/phonological aspects of knowledge. Words are enriched with multi-dimensional feature embeddings including frequency, age of acquisition, length and polysemy. These aspects enable unprecedented explorations of cognitive knowledge. Through CHILDES data, we use FERMULEX networks to model normative language acquisition by 1000 toddlers between 18 and 30 months. Similarities and embeddings capture word homophily via conformity, which measures assortative mixing via distance and features. Conformity unearths a language kernel of frequent/polysemous/short nouns and verbs key for basic sentence production, supporting recent evidence of children\u2019s syntactic constructs emerging at 30 months. This kernel is invisible to network core-detection and feature-only clustering: It emerges from the dual vector/network nature of words. Our quantitative analysis reveals two key strategies in early word learning. Modelling word acquisition as random walks on FERMULEX topology, we highlight non-uniform filling of communicative developmental inventories (CDIs). Biased random walkers lead to accurate (75%), precise (55%) and partially well-recalled (34%) predictions of early word learning in CDIs, providing quantitative support to previous empirical findings and developmental theories.", "sentences": [], "annotations": [], "relations": []}, {"offset": 1973, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 1986, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The mental lexicon is the part of memory that stores information about a word\u2019s meanings, syntactic features, pronunciation and more. Although often described as being like a mental dictionary, the mental lexicon is not static, and is instead a complex system, whose structure influences language processing and has been investigated across fields like psychology, linguistics, computer science and artificial intelligence. Decades of multidisciplinary research have gathered evidence that words in the mental lexicon have a dual representation, analogous to the particle/wave duality of light in physics. Psycholinguistics and distributional semantics posit that words in the lexicon possess both a networked organisation and a vector-space nature. On the one hand, networks capture conceptual relationships (as links) between words (as nodes). On the other hand, vector-spaces identify alignment and distances between vectors, whose components represent word features. The network aspects of the mental lexicon started with seminal work by Quillian and by Collins and Loftus. These works showed how in a network of words linked through semantic associations, e.g. possessing a common attribute or overlapping in meaning, the length of the shortest path separating concepts was predictive of retrieval times from semantic memory and sentence understanding. The advent of network science has revived interest in this approach, with several recent works examining how the structure of semantic networks, phonological networks, and their multiplex combination influence language acquisition and processing.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3593, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In parallel, distributional semantics postulates that semantic memory possesses a vector space structure, where concepts are vectors whose components express either interpretable features (e.g. possessing a semantic feature, being in a category or being acquired at a certain age) or latent aspects of language (e.g. overlap in meaning due to word co-occurrence in the same context). Although latent aspects of language limit the understanding of cognitive processing, models like Latent Semantic Analysis and the Hyperspace Analogue to Language were used extensively in cognitive inquiries of information processing, mainly due to their ability to extract semantic features without human intervention. More recently, transformer neural networks like BERT enabled vector representations for words depending on their context. This enhancement revolutionised the field of natural language processing and predicted successfully semantic tasks like entity recognition or word meaning disambiguation. Although powerful predictors, these approaches provide relatively little access to the organisation of words in the human mind and can thus benefit from network models and interpretable distributional semantics. Reconciling the non-latent, interpretable vector/network duality of words in the mental lexicon is the focus of this work.", "sentences": [], "annotations": [], "relations": []}, {"offset": 4924, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "We introduce FEature-Rich MUltiplex LEXical - FERMULEX - networks, a framework combining the vector-based and multiplex network aspects of words and their associations in the mental lexicon. Rather than merely building networks out of similarities between vectors of features, we view structure and feature similarities as two independent building blocks, whose contribution to represent words in the mind can be explored in parallel. Hence in FERMULEX networks, network structure remains and can be explored even when word similarities are switched off, and vice versa. This possibility does not exist in networks built from vector similarities (cf.). We achieve this advancement by using the recent measure of conformity, an enhancement of assortative mixing estimation through non-adjacent nodes.", "sentences": [], "annotations": [], "relations": []}, {"offset": 5724, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "As outlined in Fig.\u00a01A\u2013C, FERMULEX starts from a given multiplex network structure, where nodes represent concepts/words linked by different types of conceptual associations (Fig.\u00a01A). We focus on layers that were found to predict early word learning in toddlers and consider semantic, syntactic and phonological associations between words (see and \u201cMethods\u201d). Each word is also endowed with a vector of psycholinguistic features, i.e. features of relevance for lexical acquisition, processing and storage. We here endow words with vectors of interpretable features, like frequency, length and polysemy (Fig.\u00a01B). In FERMULEX, merging network structure with vectorial similarities means measuring how similar any two nodes/vectors can be according to their vector similarity, weighted through network connectivity. This is quantitatively implemented via conformity, which measures a tendency for nodes/words with similar vectors to be separated by shorter distances (i.e. fewer links). Each node receives its conformity score, leading to a richer (in terms of nodes features) multiplex representation (Fig.\u00a01C) of conceptual knowledge in the mental lexicon.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6892, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "We show that the dual network/vector representation of words is crucial for understanding key aspects of the mental lexicon that would go undetected by considering features\u2014or networks\u2014only. Using normative word learning norms and phonological/semantic/syntactic data in 1000 English toddlers, FERMULEX networks reveal a language kernel progressively built in the mental lexicon of toddlers and undetectable by either network core detection or clustering in vector spaces. This mental kernel contains general yet simple nouns and verbs that can build diverse sentences, with crucial relevance to children\u2019s communication. The identification of this kernel via FERMULEX provides quantitative evidence and modelling insights as to how can young children produce early sentences, as recently observed.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7697, "infons": {"file": "41598_2022_27029_Fig1_HTML.jpg", "id": "Fig1", "section_type": "FIG", "type": "fig_caption"}, "text": "(A\u2013C) Combining multiplex topology (A) and vector spaces (B) results in FERMULEX network (C); (D) kernel density estimates (KDEs) and ridgeline plots highlight conformity distribution for the frequency, length, and polysemy features in toddlers\u2019 mental lexicon and the randomised variants; (E) Above\u2014two-dimensional scatter plot of conformity vector space, where each point is colored according to the cluster the point belongs to (K-means algorithm); Below\u2014distribution of word features within each cluster, where a kernel language emerges, i.e. the cluster labeled as D; (F) content characterisation of the kernel compared to a competitor from a k-core decomposition.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8375, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Modelling word acquisition as increasingly biased random walkers over the network/vectorial FERMULEX representation leads to more insights. We adopted this approach inspired by past work using random walkers over cognitive networks for investigating the mental lexicon. We find that predicting word learning in the language kernel crucially depends on: (i) network/vectorial conformity and (ii) the filling of communicative developmental inventories (CDIs), i.e. lists of words sharing a semantic category and commonly used for measuring early cognitive development. We find that CDIs display a rich filling dynamic in word learning, which can be predicted by our biased random walkers. The results are statistically significant with respect to a baseline random learner. Without combining structural and attributive information as well as CDI filling levels, in fact, predictions of word learning in the language kernel are equivalent to random guessing. Since the language kernel stores words crucial for producing early sentences, our results indicate that the documented ability for young toddlers to communicate via early sentences around month 30 crucially depends on network, vector, and categorical aspects of the mental lexicon. Our approach with FERMULEX can encompass them all and thus represents a powerful tool for future cognitive research of various aspects of language.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9761, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "Results", "sentences": [], "annotations": [], "relations": []}, {"offset": 9769, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "FERMULEX characterisation", "sentences": [], "annotations": [], "relations": []}, {"offset": 9795, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "A combination of a multiplex network structure (Fig.\u00a01A) and a vector space of interpretable features (Fig.\u00a01B) results in a FERMULEX network (Fig.\u00a01C). Conformity assesses structure-feature relationships on the aggregated topology. For each node and with respect to each feature, conformity quantifies the node assortative mixing, by extending this estimation to the non-adjacent but still reachable neighbors of a node. Studying conformity distributions, we can capture heterogeneous patterns between nodes.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10308, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Figure\u00a01D sums up these patterns on the real data representing toddlers\u2019 mental lexicon (see \u201cMethods\u201d for details on network layers and vectors of word features). Conformity with respect to frequency highlights an assortative mixing pattern but limited only to highly frequent words, i.e. only words occurring many times in child-directed speech tend to connect with each other in children\u2019s FERMULEX network. This effect is absent in lower-frequency words and it was not detected in single-layer semantic networks of adults. Conformity of word length highlights an assortative mixing pattern of very short words only. These two effects are expected to be related as shorter words tend to be more frequent in language.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11037, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Interestingly, conformity quantifies that polysemous words are likely to connect to each other to a smaller extent than most frequent and shortest words. This indicates an organisation of concepts where unambiguous/less polysemous words are linked to ambiguous/more polysemous words. This heterogeneous mixing by polysemy could be beneficial in providing context and differentiating among possible meanings of a polysemous word, as suggested by previous studies. If all ambiguous words were grouped together, sense disambiguation could not rely on links including less polysemous/unambiguous words and this homogeneity would ultimately violate the frequency-meaning law.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11708, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The above assortative mixing patterns are not a consequence of feature/distance distributions, because reshuffling node labels (Null Model 1) and rewiring links (Null Model 2) disrupt the heterogeneous mixing behaviour among classes (see \u201cMethods\u201d and SI). Hence, the above patterns indicate the presence of a core-periphery organisation in the dualistic multiplex/feature-rich structure of the mental lexicon: A set of highly frequent/shorter/polysemous words linked with each other creates a network core highlighted by conformity and invisible to previous inquiries. This preliminary evidence calls for further analysis of the core.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12348, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Figure \u00a01E introduces an analysis of the core performed on: (i) dualistic network/vector and (ii) individual aspects of words in the mental lexicon of toddlers (see \u201cMethods\u201d and SI). We aim to find a language core that contains groups of words sharing similar structure-feature relationships. Among the six optimal clusters found (see \u201cMethods\u201d and SI), groups A and B (blue and gold) contain mostly short words. Cluster F (cyan) contains highly frequent words. Cluster D contains short, highly frequent and a relevant portion of polysemous words. Sets of clustered words with such features are known as language kernels in cognitive network science. Language kernels facilitate communication through a small set of simple words suitable for expressing ideas in multiple contexts. The conformity core (cluster D) satisfies this definition. In fact, 13% of the core is made of nouns, 33% of verbs and the other 54% include adjectives, adverbs and pronouns, which make it more likely to assemble syntactically well-formed sentences by random sampling compared to other word clusters (cf. Fig.\u00a01F). Identifying a network core via k-core decomposition shows almost no meaning organisation and more expressions that are syntactically unrelated . See two random samples in Fig.\u00a01F: The conformity cluster can form syntactically coherent trigrams such as \u201cGet Back Here\u201d and \u201cBoy Take Toy\u201d, whereas the same does not happen in the only network-based core. Analogously, K-Modes attribute-only clusters are unable to form syntactically coherent bigrams. See SI for an analysis centered on computing the internal syntactic coherence of the cores. These comparisons provide unprecedented evidence showing a syntactically advantageous organisation of words in early children\u2019s lexicon. This phenomenon goes undetected unless both the network and vector nature of words in the mind is considered.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14254, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Topology and cognitive relevance of the conformity core FERMULEX", "sentences": [], "annotations": [], "relations": []}, {"offset": 14319, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We further compare the conformity core with the k-core decomposition (where similarities are switched off) and with the most relevant K-Modes cluster (where network structure is switched off). Interestingly, the conformity core appears to be a synthesis of the other two potential language kernels. Figure\u00a02C characterises the three cores with several qualitative functions assessing intra-cluster connectivity and inter-cluster distinctiveness (cf. \u201cMethods\u201d and the SI). The K-Modes core contains a rich set of short, highly frequent and polysemous words compared to the conformity core. The conformity core contains a more homogeneous set of words, which is crucial for syntactic sentences mixing specific and more general concepts. The structural k-core has high transitivity, but the conformity core has a more cliquish configuration due to higher hub dominance score. Cliquishness was recently shown to correlate with better recall from memory due to the concentration of spreading activation in the clique. These recent studies suggest that the higher cliquishness found here for the conformity core might be beneficial for language processing in toddlers. The conformity core also displays high values of conductance and cut ratio: this language kernel possess a dense internal structure but it is also strongly connected to the rest of the graph as well, considerably more than the other competitors. In other words, the conformity core is strongly internally connected (more than k-core) and homogeneous with respect to the features (more than k-mode). This higher connectivity might reflect an advantage in accessing and producing items from the language kernel in view of activation spreading models of the mental lexicon.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16059, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Language kernel entanglement", "sentences": [], "annotations": [], "relations": []}, {"offset": 16088, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We aim to further investigate the multiplex structure of the conformity core even through layers. To this aim, we leverage the concept of layer entanglement, assessing how much the layers overlap and are balanced in the multiplex core against the whole multiplex structure. In detail, layer entanglement can be captured by two measures: Entanglement intensity I, that computes how much layers overlap with other layers, and entanglement homogeneity H, that measures how much nodes are connected in a balanced way across layers. In the whole multiplex structure, we find that  and , while in the conformity core/language kernel we find higher intensity and lower homogeneity values,  and . A higher entanglement intensity in the kernel () demonstrates that such group of words, highlighted by network/vector conformity, acts as a core in the multiplex network: Layers are more entangled, i.e. concentrate more links, within this core rather than in the whole multiplex structure. A lower entanglement homogeneity in the kernel () indicates that one or more layers are over-represented in the kernel itself. Looking at the counts of links from different layers with both endpoints within the kernel, we notice that co-occurrences constitute most of the links in the kernel (0.76% of co-occurrences against 0.19% of associations and 0.05% of phonological similarities). This finding provides additional evidence that the observed language kernel is crucial for syntactic relationships, which are best captured by child-directed co-occurrences. Interestingly, excluding the layer of co-occurrences from the multiplex network does not alter the presence of the kernel (see SI) nor its entanglement: Entanglement values of the kernel without co-occurrence links do not drastically change, i.e.  and . Moreover, layer entanglement can be computed on a temporal network as well. By creating subgraphs of the original multiplex network with the first 200, 300 and 400 learned words, we registered values of H and I analogous to the ones of the full multiplex network. Interestingly, the above findings indicate that the language kernel highlighted by the interplay of vector and network features is highly entangled across semantic, phonological and syntactic aspects of the mental lexicon and it persists over time. These patterns further suggest the kernel/core might play a relevant role for supporting cognitive processing (see \u201cDiscussion\u201d).", "sentences": [], "annotations": [], "relations": []}, {"offset": 18530, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Normative word learning as random walks on FERMULEX", "sentences": [], "annotations": [], "relations": []}, {"offset": 18582, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "To investigate how the conformity core and the whole FERMULEX structure emerge over time, we adopt a random walk framework. Random walks on cognitive network structures have successfully modelled phenomena like Zipf\u2019s law or semantic priming. Here, we use structure-feature biased random walks to explore normative language learning, as reported in Fig.\u00a02.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18942, "infons": {"file": "41598_2022_27029_Fig2_HTML.jpg", "id": "Fig2", "section_type": "FIG", "type": "fig_caption"}, "text": "(A) Random walks combining progressively structure and vector information (Graph Walk 1\u20133) and CDIs integration (Graph Walk 4); (B) Above\u2014CDIs filling in CHILDES normative learning; bars show that CDIs are not uniformly filled over time, e.g. more than half of Body and Vehicle categories are learned during early stage acquisition, whereas Questions and Time emerge later; Below\u2014precision-recall evaluation over selected CDIs; solid bars identify statistically significant scores compared to a random learning baseline; (C) Left\u2014precision-recall evaluation with respect to early acquisition of kernel words; Right\u2014kernel characterisations using several quality measures.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19624, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "The simplest idea is to limit the walk to network structure only (Graph Walk 1). To explore the interplay between topology and features of words, we can weigh network links with the similarity between vectors representing adjacent words (Graph Walk 2). Let us consider an example. In Fig. 2, at , Graph Walk 1 should choose to learn either cat or daddy after the current word mommy. Because of network/vectorial similarities, Graph Walk 2 will select daddy as the next-to-be-learned word. We can expand the set of next-to-be-learned candidate words: Graph Walk 3 encodes this parallel word learning process by considering as potential candidates all neighbors of already learned words. With reference to Fig.\u00a02A, at , Graph Walk 2 can only move to and learn friend, while Graph Walk 3 can also activate and learn cat after mommy. Focus is given to considering how these models can predict the assembly over time of: (i) the conformity core, and of (ii) Communicative Development Inventories (CDIs), which are commonly used by psycholinguists to measure a child\u2019s communicative, receptive and expressive abilities. CDIs are clusters of words from the same semantic category\u2014e.g. a list of words all relative to time\u2014and thus represent a portion of the whole vocabulary available to children.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20922, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "CDIs are not filled uniformly under normative learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 20977, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In the CHILDES data, toddlers are found not to learn CDIs uniformly over time (cf. Fig. 2B). This means that some semantic domains of toddlers\u2019 lexicon are filled earlier during normative learning. However, the above random walkers do not include information about the semantic category a word belongs to. Graph Walk 4 proposes a CDI-based similarity integrating information about CDIs\u2019 availability and attractiveness. In the figure, at , Graph Walk 4 moves from mommy to cat, because Animal-CDI is relatively emptier than People-CDI, i.e. People already contains mommy. However, at , the model learns friend from daddy, because the feature similarity equation term is stronger than the CDI-based ones (see \u201cMethods\u201d and SI).", "sentences": [], "annotations": [], "relations": []}, {"offset": 21712, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Toddler\u2019s language kernel rises from CDI density and network/vector dualities", "sentences": [], "annotations": [], "relations": []}, {"offset": 21792, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Figure 2C, left reports precision and recall in reconstructing the conformity core early on during cognitive development. Performance metrics statistically higher than random learning (significance of 0.05, see SIB) are highlighted with full bars. Non-significant results are visualised as dashed bars. The normative growth of children\u2019s language kernel was captured with a precision higher than random learning only by our most advanced model, combining CDI density, multiplex network structure and feature similarities. This provides strong evidence that semantic spheres and their filling over time provide insights additional to network/vector duality for capturing how early production of syntactically coherent sentences is achieved. Compared to other CDIs (see next section), our walkers achieved a relatively lower precision in predicting the assembly of the conformity core. This indicates that the language kernel does not emerge all at once during early cognitive development, unlike other kernels highlighted in older children. The emergence of the conformity core is thus a gradual phenomenon, that is not strongly biased by similarities and cannot thus be captured by biased random walks only.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23002, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Random walks highlight different strategies at work in different CDIs", "sentences": [], "annotations": [], "relations": []}, {"offset": 23072, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Random walks produce word ordering lists that we evaluate with respect to CHILDES normative ordering, i.e. the order in which most children produced words over time (Fig. 2B-above). Random learning is used as a baseline to test whether walks considering word topology and feature predict more words as correctly learned over time. See \u201cMethods\u201d and SI for details of our statistical approach.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23469, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_caption"}, "text": "Model performances over each bin of acquisition.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23518, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\"/><th align=\"left\">Accuracy</th><th align=\"left\">Relevant CDIs</th><th align=\"left\">Precision</th><th align=\"left\">Relevant CDIs</th><th align=\"left\">Recall</th><th align=\"left\">Relevant CDIs</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"6\"><bold>AoA &lt; 21</bold></td><td align=\"left\"/></tr><tr><td align=\"left\">Random learning</td><td align=\"left\">0.67</td><td align=\"left\">\u2013</td><td align=\"left\">0.17</td><td align=\"left\">\u2013</td><td align=\"left\">0.19</td><td align=\"left\">\u2013</td></tr><tr><td align=\"left\">Struct</td><td align=\"left\">0.70</td><td align=\"left\">0.26</td><td align=\"left\">0.40</td><td align=\"left\">0.64</td><td align=\"left\">0.30</td><td align=\"left\">0.58</td></tr><tr><td align=\"left\">MaxSim</td><td align=\"left\">0.76</td><td align=\"left\">0.26</td><td align=\"left\">0.37</td><td align=\"left\">0.70</td><td align=\"left\">0.34</td><td align=\"left\">0.52</td></tr><tr><td align=\"left\">ExtCand</td><td align=\"left\">0.65</td><td align=\"left\">0.21</td><td align=\"left\">0.55</td><td align=\"left\">0.76</td><td align=\"left\">0.30</td><td align=\"left\">0.58</td></tr><tr><td align=\"left\">CDI-MaxSim</td><td align=\"left\">0.75</td><td align=\"left\">0.42</td><td align=\"left\">0.25</td><td align=\"left\">0.58</td><td align=\"left\">0.34</td><td align=\"left\">0.47</td></tr><tr><td align=\"left\" colspan=\"6\"><bold>&lt; AoA &lt; 23</bold></td><td align=\"left\"/></tr><tr><td align=\"left\">Random learning</td><td align=\"left\">0.71</td><td align=\"left\">\u2013</td><td align=\"left\">0.17</td><td align=\"left\">\u2013</td><td align=\"left\">0.19</td><td align=\"left\">\u2013</td></tr><tr><td align=\"left\">Struct</td><td align=\"left\">\u2013</td><td align=\"left\">0.00</td><td align=\"left\">0.24</td><td align=\"left\">0.64</td><td align=\"left\">0.24</td><td align=\"left\">0.71</td></tr><tr><td align=\"left\">MaxSim</td><td align=\"left\">0.82</td><td align=\"left\">0.36</td><td align=\"left\">0.28</td><td align=\"left\">0.57</td><td align=\"left\">0.25</td><td align=\"left\">0.64</td></tr><tr><td align=\"left\">ExtCand</td><td align=\"left\">0.83</td><td align=\"left\">0.26</td><td align=\"left\">0.24</td><td align=\"left\">0.42</td><td align=\"left\">0.24</td><td align=\"left\">0.50</td></tr><tr><td align=\"left\">CDI-MaxSim</td><td align=\"left\">0.66</td><td align=\"left\">0.10</td><td align=\"left\">0.26</td><td align=\"left\">0.71</td><td align=\"left\">0.26</td><td align=\"left\">0.71</td></tr><tr><td align=\"left\" colspan=\"6\"><bold>&lt; AoA &lt; 24</bold></td><td align=\"left\"/></tr><tr><td align=\"left\">Random learning</td><td align=\"left\">0.69</td><td align=\"left\">\u2013</td><td align=\"left\">0.17</td><td align=\"left\">\u2013</td><td align=\"left\">0.19</td><td align=\"left\">\u2013</td></tr><tr><td align=\"left\">Struct</td><td align=\"left\">0.73</td><td align=\"left\">0.21</td><td align=\"left\">0.19</td><td align=\"left\">0.42</td><td align=\"left\">0.21</td><td align=\"left\">0.52</td></tr><tr><td align=\"left\">MaxSim</td><td align=\"left\">0.75</td><td align=\"left\">0.36</td><td align=\"left\">0.17</td><td align=\"left\">0.42</td><td align=\"left\">0.23</td><td align=\"left\">0.42</td></tr><tr><td align=\"left\">ExtCand</td><td align=\"left\">0.73</td><td align=\"left\">0.31</td><td align=\"left\">0.20</td><td align=\"left\">0.21</td><td align=\"left\">0.21</td><td align=\"left\">0.52</td></tr><tr><td align=\"left\">CDI-MaxSim</td><td align=\"left\">0.69</td><td align=\"left\">0.21</td><td align=\"left\">0.19</td><td align=\"left\">0.52</td><td align=\"left\">0.23</td><td align=\"left\">0.52</td></tr><tr><td align=\"left\" colspan=\"6\"><bold>24 &lt; AoA &lt; 26</bold></td><td align=\"left\"/></tr><tr><td align=\"left\">Random learning</td><td align=\"left\">0.70</td><td align=\"left\">\u2013</td><td align=\"left\">0.17 </td><td align=\"left\">\u2013</td><td align=\"left\">0.19</td><td align=\"left\">\u2013</td></tr><tr><td align=\"left\">Struct</td><td align=\"left\">0.73</td><td align=\"left\">0.31</td><td align=\"left\">0.20</td><td align=\"left\">0.44</td><td align=\"left\">0.22</td><td align=\"left\">0.61</td></tr><tr><td align=\"left\">MaxSim</td><td align=\"left\">0.72</td><td align=\"left\">0.31</td><td align=\"left\">0.21</td><td align=\"left\">0.38</td><td align=\"left\">0.26</td><td align=\"left\">0.44</td></tr><tr><td align=\"left\">ExtCand</td><td align=\"left\">0.71</td><td align=\"left\">0.42</td><td align=\"left\">0.18</td><td align=\"left\">0.33</td><td align=\"left\">0.22</td><td align=\"left\">0.50</td></tr><tr><td align=\"left\">CDI-MaxSim</td><td align=\"left\">0.72</td><td align=\"left\">0.31</td><td align=\"left\">0.23</td><td align=\"left\">0.38</td><td align=\"left\">0.22</td><td align=\"left\">0.44</td></tr><tr><td align=\"left\" colspan=\"6\"><bold>AoA &gt; 26</bold></td><td align=\"left\"/></tr><tr><td align=\"left\">Random learning</td><td align=\"left\">0.61</td><td align=\"left\">\u2013</td><td align=\"left\">0.24</td><td align=\"left\">\u2013</td><td align=\"left\">0.24</td><td align=\"left\">\u2013</td></tr><tr><td align=\"left\">Struct</td><td align=\"left\">0.68</td><td align=\"left\">0.78</td><td align=\"left\">0.32</td><td align=\"left\">0.72</td><td align=\"left\">0.36</td><td align=\"left\">0.61</td></tr><tr><td align=\"left\">MaxSim</td><td align=\"left\">0.70</td><td align=\"left\">0.84</td><td align=\"left\">0.33</td><td align=\"left\">0.77</td><td align=\"left\">0.35</td><td align=\"left\">0.66</td></tr><tr><td align=\"left\">ExtCand</td><td align=\"left\">0.64</td><td align=\"left\">0.52</td><td align=\"left\">0.28</td><td align=\"left\">0.44</td><td align=\"left\">0.29</td><td align=\"left\">0.66</td></tr><tr><td align=\"left\">CDI-MaxSim</td><td align=\"left\">0.79</td><td align=\"left\">0.31</td><td align=\"left\">0.33</td><td align=\"left\">0.77</td><td align=\"left\">0.41</td><td align=\"left\">0.66</td></tr></tbody></table>\n"}, "text": "\tAccuracy\tRelevant CDIs\tPrecision\tRelevant CDIs\tRecall\tRelevant CDIs\t \tAoA < 21\t\t \tRandom learning\t0.67\t\u2013\t0.17\t\u2013\t0.19\t\u2013\t \tStruct\t0.70\t0.26\t0.40\t0.64\t0.30\t0.58\t \tMaxSim\t0.76\t0.26\t0.37\t0.70\t0.34\t0.52\t \tExtCand\t0.65\t0.21\t0.55\t0.76\t0.30\t0.58\t \tCDI-MaxSim\t0.75\t0.42\t0.25\t0.58\t0.34\t0.47\t \t< AoA < 23\t\t \tRandom learning\t0.71\t\u2013\t0.17\t\u2013\t0.19\t\u2013\t \tStruct\t\u2013\t0.00\t0.24\t0.64\t0.24\t0.71\t \tMaxSim\t0.82\t0.36\t0.28\t0.57\t0.25\t0.64\t \tExtCand\t0.83\t0.26\t0.24\t0.42\t0.24\t0.50\t \tCDI-MaxSim\t0.66\t0.10\t0.26\t0.71\t0.26\t0.71\t \t< AoA < 24\t\t \tRandom learning\t0.69\t\u2013\t0.17\t\u2013\t0.19\t\u2013\t \tStruct\t0.73\t0.21\t0.19\t0.42\t0.21\t0.52\t \tMaxSim\t0.75\t0.36\t0.17\t0.42\t0.23\t0.42\t \tExtCand\t0.73\t0.31\t0.20\t0.21\t0.21\t0.52\t \tCDI-MaxSim\t0.69\t0.21\t0.19\t0.52\t0.23\t0.52\t \t24 < AoA < 26\t\t \tRandom learning\t0.70\t\u2013\t0.17 \t\u2013\t0.19\t\u2013\t \tStruct\t0.73\t0.31\t0.20\t0.44\t0.22\t0.61\t \tMaxSim\t0.72\t0.31\t0.21\t0.38\t0.26\t0.44\t \tExtCand\t0.71\t0.42\t0.18\t0.33\t0.22\t0.50\t \tCDI-MaxSim\t0.72\t0.31\t0.23\t0.38\t0.22\t0.44\t \tAoA > 26\t\t \tRandom learning\t0.61\t\u2013\t0.24\t\u2013\t0.24\t\u2013\t \tStruct\t0.68\t0.78\t0.32\t0.72\t0.36\t0.61\t \tMaxSim\t0.70\t0.84\t0.33\t0.77\t0.35\t0.66\t \tExtCand\t0.64\t0.52\t0.28\t0.44\t0.29\t0.66\t \tCDI-MaxSim\t0.79\t0.31\t0.33\t0.77\t0.41\t0.66\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 24689, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_foot"}, "text": "Relevant CDI fraction is the ratio of statistically significant precision/recall values against a random learning model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24810, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Table\u00a01 presents a coarse-grained evaluation of the walkers (cf. \u201cDiscussion\u201d). Figure\u00a02B sums up results with respect to CDIs focusing on the very early stage of acquisition, which corresponds to  words learned before 21 months. The selected CDIs are captured differently by the models. CDIs like People and Prepositions are predicted with higher-than-random precision and recall for all Graph Walk models. CDI-MaxSim precision is slightly better than in the other models. Interestingly, the two most filled CDIs in this stage of acquisition, i.e. Body and Vehicle, are predicted with high precision but low recall (cf. \u201cMethods\u201d and SI). This means that the few words predicted are the expected ones, but the models cannot fill the CDIs. ExtCand precision is higher. Not all CDIs can be predicted in this way, e.g. Action and Household. Furthermore, model performances for Household are not distinguishable from a random learning, i.e. all bars are dotted. The high recall but the low precision of Action is poorly relevant: less of 0.1% of the CDI is covered in this stage of acquisition (however, cf. the SI, where Action category is well captured in other stages).", "sentences": [], "annotations": [], "relations": []}, {"offset": 25990, "infons": {"section_type": "DISCUSS", "type": "title_1"}, "text": "Discussion", "sentences": [], "annotations": [], "relations": []}, {"offset": 26001, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "This work introduces a cutting-edge combination of network and vector aspects of knowledge in the human mind, which historically run in parallel when modelling language and its cognitive processes.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26199, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "Using data from 1000 toddlers between 18 and 30 months from the CHILDES project, our FERMULEX network revealed a core of words facilitating word production and invisible to methods based on network structure or vector similarities only. This core was detected via conformity, a metric extending assortative mixing estimation in a multi-scale, node-centric fashion. Our numerical experiments identified this core as a set of highly frequent, short, polysemous and well-connected nouns and verbs, i.e. a language kernel containing concepts versatile enough to communicate via basic sentences (cf.) and whose access via spreading activation is facilitated by network connectivity. Revealing the presence of such a core through our analyses provides for the first time quantitative support of recent empirical findings showing that typical learners can start combining words in basic sentences after 30 months of age. The kernel persisted even when co-occurrences from child-directed speech were ignored (see SI): the conformity core emerged from an interplay between semantic/phonological associations and psycholinguistic norms in the mental lexicon of linguistic knowledge.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27372, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "It is important to underline that previous network-only models using the same data were not able to highlight such kernel. Analogously, as shown here, focusing only on vectorial similarities could not identify such kernel either. We thus consider the combination of vectorial and network aspects of associative knowledge to represent an interesting \u201cthird direction\u201d of investigation, merging aspects of relevance for investigating how the cognitive reflection of language works. FERMULEX inherits from networks the ability to map the local and global layout of associations words engage in, e.g. phonological degree explaining patterns of short-term memory retention or network distances reproducing patterns of semantic similarity judgements. From vector models of words, i.e. word embeddings, FERMULEX inherits the ability to encode features of concepts beyond mere network patterns, potentially leading the way to future investigation of distributional semantics integrating network science within a coherent, mathematical framework.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28414, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "To investigate the assembly over time of such a crucial core of linguistic knowledge, we implemented artificial models of word learning as biased random walks over FERMULEX, inspired by past approaches using walkers to investigate the mental lexicon. We found that the conformity core does not emerge suddenly over time, differently from other language kernels modelled as viable component in other studies. Instead, the conformity core is progressively built in ways that are captured only by combining the network and vector aspects of words together with CDI filling rates. This finding quantitatively stresses that the conformity core\u2014containing building blocks for producing syntactically coherent words\u2014emerges from strategies dependent on semantic categories, which are partly captured by CDIs.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29220, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "We also used the same random walkers for capturing how different CDIs filled over time through normative learning, giving unprecedented focus to learning strategies for individual aspects of children\u2019s knowledge. In our analyses, different CDIs are found to fill at different times over developmental stages, further emphasizing that language learning is not a uniformly random process. Inventories relative to food and action themes are found to be predicted well by our model, confirming recent independent studies that these salient familiar themes are crucial for predicting early language acquisition.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29829, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "Notice also that words in some CDIs might be learned according to context-specific strategies, so that a single, general word-learning strategy might not fit all cases. For instance, according to the Pervasiveness Hypothesis by Clerkin and colleagues, toddlers would tend to learn earlier words more frequently occurring across several daily contexts. This visual prevalence/occurrence would be crucially missing from CDIs like Household or Action, which were in fact poorly reproduced by our model. These negative findings indicate the presence of local strategies for learning words in physical settings that are at work in toddlers but missing from the current instance of FERMULEX.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30515, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "For inventories like Body or Vehicle, a combination of network structure and feature similarities corresponded to a significant boost in precision over predictions from random learning. This is quantitative evidence for combining network and vector aspects of the mental lexicon. A further boost in precision was found when the random walker was allowed to backtrack. This indicates that some components of the mental lexicon are not built sequentially, without appending words to the most recent lexical item, as assumed in attachment kernel models, but rather filling gaps in the whole vocabulary available to children, as shown also by other approaches with persistent homology and gap filling.", "sentences": [], "annotations": [], "relations": []}, {"offset": 31213, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "Interestingly, recency in word acquisition is found to be more a powerful strategy for reconstructing the filling of CDIs like People or Prepositions, where our most elaborate random walker based on recency beats the back-tracking one. Our quantitative results open the way for further discussion and interpretation in light of psychological studies behind early language learning.", "sentences": [], "annotations": [], "relations": []}, {"offset": 31595, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "This first conception of FERMULEX has some key limitations, which can be addressed in future research. For example, our approach considers only normative learning, i.e. how most children learn words over time. This learning dynamic might be different from how individual children with different language learning skills might learn words over time. Future research should thus test the presence of the language kernel and its time-evolution dynamics in a longitudinal cohort of children. Since the occurrence of the language kernel characterises normative learning in a large population of 1000 and more toddlers and it supports the production of early sentences observed in normative talkers, we expect for the kernel to be present in normative learners but also to be disrupted or incomplete in late talkers. If supported by data, then the language kernel revealed here could become a crucial early predictor of delayed language development in young children. Another limitation is that our predictions do not treat learning as the outcome of a statistical process, where words are learned with certain probabilities. Rather we model word learning as a binary learned/not learned process. We chose to follow this approach for model parsimony and indicate the addition of statistical learning within the FERMULEX framework as an exciting future research direction. Future enhancements of random-walk models should account also for distinctiveness in addition to similarity. The recent work by Siew indicates that global feature distinctiveness, i.e. how many different semantic features are possessed by a word, correlates with earlier acquisition. Hence, random walkers accounting for switches between distinctiveness and similarity might enhance prediction results and represent an exciting future research direction. Another important approach for future research might be casting language acquisition as a percolation problem, which has been explored in feature-rich networks only recently. An important limitation of our study is that it adopts CDIs for modelling language learning, however these inventories are not grounded in theories from cognitive psychology but were rather created ad-hoc by psycholinguists. Future instances of FERMULEX networks should rely on word learning data that is more representative across semantic and syntactic categories.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33958, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 33966, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Multiplex layers", "sentences": [], "annotations": [], "relations": []}, {"offset": 33983, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We modelled word learning as a cognitive process acting on a mental representation of linguistic knowledge. Structure in this representation is given by a multiplex lexical network, where nodes represent words that are replicated and connected across different semantic and phonological levels of the network.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34293, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Only layers of relevance for word learning acquisition were considered, namely: (i) free associations, indicating memory recall patterns between words from semantic memory, (ii) co-occurrences in child-directed speech, (iii) feature-sharing norms, indicating which concepts shared at least one semantic feature from the McRae dataset and (iv) phonological similarities, representing which words differed by the addition/substitution/deletion of one phoneme only. Hills and colleagues showed that the words with larger degrees in free association networks were also more likely to be acquired at earlier ages, a phenomenon known also as lure of the associates (cf. also). A subsequent study by Carlson and colleagues found a similar effect also in phonological networks built from child-directed speech. Investigations of co-occurrence and feature sharing networks by Beckage and Colunga reported that highly connected words were distinct trademarks of early word production in typical talkers. Importantly, these four aspects of knowledge in the human mind produced network representations that were irreducible. Layers represented different connectivity patterns among words and could thus not be aggregated or erased without decreasing structural information about the system in terms of Von Neumann graph entropy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 35610, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Normative age of acquisition", "sentences": [], "annotations": [], "relations": []}, {"offset": 35639, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Network models of language acquisition often use normative datasets that follow the development of language production in toddlers. The most prominent data source is CHILDES (Child Language Data Exchange System), a multi-language corpus of the TalkBank system established by MacWhinney and Snow, storing data about language acquisition in toddlers between age 16 and 36 months. No new experiments were conducted in the current study, and no new data were generated accordingly. Data were granted to the corresponding author by the TreeBank project after a request from the CHILDES platform (https://childes.talkbank.org/) for secondary analysis. CHILDES and TreeBank have IRB approval and guidelines (https://talkbank.org/share/irb/), so that all researchers joining such research repository, like ourselves, have to abide to these ethical standard in any secondary data analysis, like the current one.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36542, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We used CHILDES data to rank words in the order they are learned by most English toddlers. By considering the fraction of children producing a certain word in a given month, within each month, words were assigned a production probability. Month after month, a rank in descending order of production probability was constructed as a proxy for normative learning of most toddlers, as done in previous studies.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36950, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Features", "sentences": [], "annotations": [], "relations": []}, {"offset": 36959, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "This study selected word features shown in previous research to influence early language acquisition, namely frequency in child-directed speech, word length and polysemy. Polysemy scores indicated the numbers of meanings relative to a given word in WordNet, a proxy to word polysemy successfully used in quantitative studies of early word learning. Due to the highly-skewed distribution of variables (e.g., Zipf\u2019s law for word frequency), we regularised data by recasting it from numerical to categorical, as to avoid biases in computing conformity. We grouped each variable into discrete bins, fine tuning bin boundaries so as to obtain non-empty bins featuring the same order of magnitude of entries. This fine-tuning led to splitting words in quintiles for both word frequency and polysemy and in tertiles for length.", "sentences": [], "annotations": [], "relations": []}, {"offset": 37782, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Conformity", "sentences": [], "annotations": [], "relations": []}, {"offset": 37793, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We characterise the interplay between structure and features through conformity, which estimates the mixing patterns of nodes in a feature-rich network, i.e. a categorical node-attributed network. This measure can find heterogeneous behaviour among all nodes of a network. Conformity enables a multi-scale strategy by leveraging node distances for computing label-similarities between a target node and other nodes. A distance damping parameter  is needed for decreasing the impact of label-similarities over longer network distances between the target node and its connected neighbors. Based on previous investigations, we adopt a value of  giving more emphasis to closer neighbours in a given network topology. See the SI or for a formal description of the measure and the motivation behind its choice in this work.", "sentences": [], "annotations": [], "relations": []}, {"offset": 38611, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "When analysing conformity, we need to test whether the measured values are a trivial consequence of structural (or attributive) patterns or rather come from a non-trivial interplay between the two. To characterise this, we resort to two null models: (i) random re-shuffling the node attribute labels while maintaining network topology (Null Model 1, Fig.\u00a01D,), and (ii) randomly rewiring of links while preserving the node degree and attribute labels (Null Model 2, Fig.\u00a01D). In other words, let us consider this question: Are two labels at the endpoint of an edge significant for the distribution of conformity or can we observe similar patterns by randomly rewiring the attributive or structural model components? While rewiring labels or connectivity patterns, respectively, we keep the other component fixed. For building Null Model 1, a random label permutation is enough to disrupt correlations between structure and features. For building Null Model 2, we used a configuration model to obtain a degree preserving graph randomisation, that is, given N nodes and any arbitrary degree sequence , we place  stubs on each node i in the graph; then we match each stub with another one until all stubs are matched. The conformity distributions of the null models in Fig.\u00a01D refer to the average node scores from 100 randomised instances of FERMULEX network.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39972, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "All conformity distributions are analysed through kernel density estimates (KDEs) and ridgelines (Fig. 1D); in particular, these last ones get a better picture of mixing heterogeneity between the class labels on the original toddlers\u2019 lexicon.", "sentences": [], "annotations": [], "relations": []}, {"offset": 40218, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Core: definition and evaluation", "sentences": [], "annotations": [], "relations": []}, {"offset": 40250, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "For finding a potential language core, we model each word as a vector of conformity scores. This results in a vector space where classic clustering algorithms as K-Means can be run. We reveal a relevant set of words among the six optimal clusters identified by K-Means through the elbow method. The SI provides methodological details about this configuration.", "sentences": [], "annotations": [], "relations": []}, {"offset": 40610, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "A set of several quality functions are proposed to characterise the language core. We focus on modularity, conductance, cut ratio, internal edge density, hub dominance and transitivity. Modularity, conductance and cut ratio focus on the links within and outside a community: They measure how well-separated a cluster is from the rest of the network. Edge density, transitivity and hub dominance characterise the internal structure of the core. In particular, transitivity and hub dominance characterise it in terms of triadic closure and cliquishness level, i.e. the creation of subgraphs where each node is fully connected to others. See the SI for their formal description. All in all, these network metrics are used to characterise the structure of the different cores found via conformity (in FERMULEX), via core-detection on the network structure only and via K-Modes on feature embeddings only. Notice that these measures, combined, provide info about the distinctiveness and connectedness of a given component/cluster in a network.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41649, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Graph walks", "sentences": [], "annotations": [], "relations": []}, {"offset": 41661, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Struct (Graph Walk 1): Words are connected by unweighted links, hence the next word is chosen according to the underlying structure only. Similarly, the set of candidates is chosen from the adjacent neighborhood of the current word;", "sentences": [], "annotations": [], "relations": []}, {"offset": 41894, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "MaxSim (Graph Walk 2): Edges are weighted according to the pairwise similarity between nodes\u2019 features. Jaccard similarity is used (cf. SI), and frequency, length and polysemy are all considered. The same strategy of Struct is used for the set of candidates;", "sentences": [], "annotations": [], "relations": []}, {"offset": 42155, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "ExtCand (Graph Walk 3): The same strategy of MaxSim is used for weighing links; the set of candidates is chosen from the adjacent neighborhood of all the words already learned;", "sentences": [], "annotations": [], "relations": []}, {"offset": 42332, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "CDI-MaxSim (Graph Walk 4): Links are weighted according to a CDI-based pairwise similarity between the attributes of nodes as well as the availability and attractiveness (cf. SI), and it needs to be updated at each iteration. The same strategy of Struct and MaxSim is used for the set of candidates.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42632, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We aim to model early word acquisition by progressively combining the network and vector components of FERMULEX to achieve this goal, the core idea is to generate a word rank that is progressively filled according to the different graph walk strategies, each one incorporating specific assumptions. In this work we compare four alternative random walk models each one having a unique rationale on how to weigh links and/or to determine the set of candidates for the next to-be-learned word. In particular:Struct and MaxSim are biased random walks considering, respectively, topology or similarity between words (i.e., the network structure or the vector space) while ExtCand and CDI-MaxSim aim for a more holistic approach.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43356, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "ExtCand visit strategy is designed to mime non-sequential word learning in children (cf.), where the word acquired at step  could be similar to any word already learned before, thus enabling an interplay between exploration and exploitation of CDIs. When the last word determines the topology of similar candidates for the next acquisition step, resembling a Markovian process, the walker possesses a bias to remain within the same CDI. By considering as to-be-learned candidates all previously learned words, the walker has a chance of backtracking and acquiring more words within the CDI sharing tightly similar concepts.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43980, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "CDI-MaxSim, the CDI-based model relies on pairwise similarity between two words modulated by additional information on the filling of CDIs they belong to. For additional details and a formal description of the pairwise similarity function adopted refer to the SI.", "sentences": [], "annotations": [], "relations": []}, {"offset": 44244, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Graph walk evaluation", "sentences": [], "annotations": [], "relations": []}, {"offset": 44266, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Accuracy, precision and recall are used to evaluate the goodness of ranks\u2019 prediction, as commonly done in statistics and machine learning. Accuracy is defined as the number of correct predictions, i.e. true positives or TP, divided by the total number of predictions. In this domain, TPs are words belonging to a CDI that are learned by a random walker in a specific bin of age of acquisition. Precision is the fraction of relevant elements among all the retrieved ones including non-relevant elements, i.e. false positives or FP. In this domain, FPs are words that fill a CDI as expected in a particular age of acquisition bin, but they are not the exact same words considered in normative learning. For instance, dog might contribute to increase FPs because it belongs to the Animals CDI but the normative learning contemplated cat instead of dog. Finally, recall is the fraction of relevant elements that are retrieved. Missing relevant elements (false negatives or FNs) are CDI\u2019s words that are not retrieved by a random walker in a particular bin of age of acquisition. The above definitions imply that there can be predictions with high recall and low precision, because there are many words that satisfy the semantic category roughly represented by the CDI (e.g. guessing as learned names of animals) but different from the specific words learned during normative acquisition (e.g. other names of animals). This interplay spans from the specific characterisation of random-walk predictions and it is accounted for in the \u201cResults\u201d and \u201cDiscussion\u201d sections. See the SI for a complete formalization of the measures, and toy examples.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45919, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 45945, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Publisher's note", "sentences": [], "annotations": [], "relations": []}, {"offset": 45962, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46081, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "These authors contributed equally: Massimo Stella and Giulio Rossetti.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46152, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 46178, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The online version contains supplementary material available at 10.1038/s41598-022-27029-6.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46270, "infons": {"section_type": "AUTH_CONT", "type": "title"}, "text": "Author contributions", "sentences": [], "annotations": [], "relations": []}, {"offset": 46291, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "Designed the study: S.C., M.V., M.S. and G.R. Curated the data: S.C. and M.S. Performed data analysis: S.C. Interpreted the data: S.C., M.V., M.S. and G.R. Supervision: M.S. and G.R. Wrote the manuscript: All authors.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46509, "infons": {"section_type": "AUTH_CONT", "type": "title"}, "text": "Data availibility", "sentences": [], "annotations": [], "relations": []}, {"offset": 46527, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "All the network layers used for this study were obtained from. All word kernels generated during this study are included in this published article and its supplementary information files.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46715, "infons": {"section_type": "SUPPL", "type": "title"}, "text": "Code availability", "sentences": [], "annotations": [], "relations": []}, {"offset": 46733, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "Conformity code for genrating the word kernels is available on https://github.com/GiulioRossetti/conformity.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46842, "infons": {"section_type": "COMP_INT", "type": "title"}, "text": "Competing interests", "sentences": [], "annotations": [], "relations": []}, {"offset": 46862, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare no competing interests.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46906, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 46917, "infons": {"fpage": "184", "lpage": "197", "name_0": "surname:Vitevitch;given-names:MS", "pub-id_doi": "10.4324/9780367853259-10", "section_type": "REF", "source": "Netw. Scie. Cogn. Psychol.", "type": "ref", "volume": "1", "year": "2019"}, "text": "Can network science connect mind, brain, and behavior?", "sentences": [], "annotations": [], "relations": []}, {"offset": 46972, "infons": {"fpage": "355", "issue": "4", "lpage": "378", "name_0": "surname:Zock;given-names:M", "pub-id_doi": "10.17791/jcs.2015.16.4.355", "section_type": "REF", "source": "J. Cogn. Sci.", "type": "ref", "volume": "16", "year": "2015"}, "text": "Words in books, computers and the human mind", "sentences": [], "annotations": [], "relations": []}, {"offset": 47017, "infons": {"section_type": "REF", "type": "ref"}, "text": "Aitchison, J. Words in the mind: An introduction to the mental lexicon. John Wiley & Sons, (2012).", "sentences": [], "annotations": [], "relations": []}, {"offset": 47116, "infons": {"fpage": "301", "issue": "7", "lpage": "306", "name_0": "surname:Elman;given-names:JL", "pub-id_doi": "10.1016/j.tics.2004.05.003", "pub-id_pmid": "15242689", "section_type": "REF", "source": "Trends Cogn. Sci.", "type": "ref", "volume": "8", "year": "2004"}, "text": "An alternative view of the mental lexicon", "sentences": [], "annotations": [], "relations": []}, {"offset": 47158, "infons": {"section_type": "REF", "type": "ref"}, "text": "Hills, T.T., & Kenett. Y.N. Is the mind a network? Maps, vehicles, and skyhooks in cognitive network science. Top. Cogn. Sci.14(1), 189\u2013208 (2022).", "sentences": [], "annotations": [], "relations": []}, {"offset": 47308, "infons": {"fpage": "20190825", "issue": "2238", "name_0": "surname:Castro;given-names:N", "name_1": "surname:Siew;given-names:CSQ", "pub-id_doi": "10.1098/rspa.2019.0825", "pub-id_pmid": "32831584", "section_type": "REF", "source": "Proc. R. Soc. A", "type": "ref", "volume": "476", "year": "2020"}, "text": "Contributions of modern network science to the cognitive sciences: Rvisiting research spirals of representation and process", "sentences": [], "annotations": [], "relations": []}, {"offset": 47432, "infons": {"fpage": "10", "issue": "1", "name_0": "surname:Stella;given-names:M", "pub-id_doi": "10.3390/bdcc3010010", "section_type": "REF", "source": "Big Data Cogn. Comput.", "type": "ref", "volume": "3", "year": "2019"}, "text": "Modelling early word acquisition through multiplex lexical networks and machine learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 47521, "infons": {"section_type": "REF", "type": "ref"}, "text": "Beckage, N.M., & Colunga, E. Network growth modeling to capture individual lexical learning. Complexity (2019).", "sentences": [], "annotations": [], "relations": []}, {"offset": 47633, "infons": {"section_type": "REF", "type": "ref"}, "text": "Beckage, N.M., & Colunga, E. Language networks as models of cognition: Understanding cognition through language. In: Towards a theoretical framework for analyzing complex linguistic networks pp. 3\u201328 (Springer, 2016).", "sentences": [], "annotations": [], "relations": []}, {"offset": 47853, "infons": {"section_type": "REF", "type": "ref"}, "text": "Beck, F. Mind, brain, and dualism in modern physics. In: Psycho-physical dualism today: An interdisciplinary approach, pp 69\u201397 (Rowman & Littlefield, New York, 2008).", "sentences": [], "annotations": [], "relations": []}, {"offset": 48023, "infons": {"fpage": "407", "issue": "6", "name_0": "surname:Collins;given-names:AM", "name_1": "surname:Loftus;given-names:EF", "pub-id_doi": "10.1037/0033-295X.82.6.407", "section_type": "REF", "source": "Psychol. Rev.", "type": "ref", "volume": "82", "year": "1975"}, "text": "A spreading-activation theory of semantic processing", "sentences": [], "annotations": [], "relations": []}, {"offset": 48076, "infons": {"fpage": "410", "issue": "5", "lpage": "430", "name_0": "surname:Quillian;given-names:MR", "pub-id_doi": "10.1002/bs.3830120511", "pub-id_pmid": "6059773", "section_type": "REF", "source": "Behav. Sci.", "type": "ref", "volume": "12", "year": "1967"}, "text": "Word concepts: A theory and simulation of some basic semantic capabilities", "sentences": [], "annotations": [], "relations": []}, {"offset": 48151, "infons": {"fpage": "408", "issue": "2", "lpage": "423", "name_0": "surname:Vitevitch;given-names:MS", "pub-id_doi": "10.1044/1092-4388(2008/030)", "pub-id_pmid": "18367686", "section_type": "REF", "source": "J. Speech Lang. Hear. Res.", "type": "ref", "volume": "51", "year": "2008"}, "text": "What can graph theory tell us about word learning and lexical retrieval?", "sentences": [], "annotations": [], "relations": []}, {"offset": 48224, "infons": {"fpage": "213", "lpage": "234", "name_0": "surname:Boleda;given-names:Gemma", "pub-id_doi": "10.1146/annurev-linguistics-011619-030303", "section_type": "REF", "source": "Annu. Rev. Ling.", "type": "ref", "volume": "6", "year": "2020"}, "text": "Distributional semantics and linguistic theory", "sentences": [], "annotations": [], "relations": []}, {"offset": 48271, "infons": {"fpage": "1006", "issue": "6", "lpage": "1033", "name_0": "surname:G\u00fcnther;given-names:F", "name_1": "surname:Rinaldi;given-names:L", "name_2": "surname:Marelli;given-names:M", "pub-id_doi": "10.1177/1745691619861372", "pub-id_pmid": "31505121", "section_type": "REF", "source": "Perspect. Psychol. Sci.", "type": "ref", "volume": "14", "year": "2019"}, "text": "Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions", "sentences": [], "annotations": [], "relations": []}, {"offset": 48386, "infons": {"fpage": "211", "issue": "2", "name_0": "surname:Landauer;given-names:TK", "name_1": "surname:Dumais;given-names:ST", "pub-id_doi": "10.1037/0033-295X.104.2.211", "section_type": "REF", "source": "Psychol. Rev.", "type": "ref", "volume": "104", "year": "1997"}, "text": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "sentences": [], "annotations": [], "relations": []}, {"offset": 48514, "infons": {"fpage": "151", "lpage": "171", "name_0": "surname:Lenci;given-names:Alessandro", "pub-id_doi": "10.1146/annurev-linguistics-030514-125254", "section_type": "REF", "source": "Annu. Rev. Ling.", "type": "ref", "volume": "4", "year": "2018"}, "text": "Distributional models of word meaning", "sentences": [], "annotations": [], "relations": []}, {"offset": 48552, "infons": {"fpage": "41", "issue": "1", "lpage": "78", "name_0": "surname:Steyvers;given-names:M", "name_1": "surname:Tenenbaum;given-names:JB", "pub-id_doi": "10.1207/s15516709cog2901_3", "pub-id_pmid": "21702767", "section_type": "REF", "source": "Cogn. Sci.", "type": "ref", "volume": "29", "year": "2005"}, "text": "The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth", "sentences": [], "annotations": [], "relations": []}, {"offset": 48652, "infons": {"fpage": "2261", "issue": "1482", "lpage": "2265", "name_0": "surname:Cancho;given-names:RFI", "name_1": "surname:Sol\u00e9;given-names:RV", "pub-id_doi": "10.1098/rspb.2001.1800", "section_type": "REF", "source": "Proc. R. Soc. Lond. Ser. B Biol. Sci.", "type": "ref", "volume": "268", "year": "2001"}, "text": "The small world of human language", "sentences": [], "annotations": [], "relations": []}, {"offset": 48686, "infons": {"fpage": "1", "name_0": "surname:Siew;given-names:SQ", "pub-id_doi": "10.1155/2019/2108423", "section_type": "REF", "source": "Complexity", "type": "ref", "volume": "1", "year": "2019"}, "text": "Cognitive network science: A review of research on cognition through the lens of network representations, processes, and dynamics", "sentences": [], "annotations": [], "relations": []}, {"offset": 48816, "infons": {"fpage": "1", "name_0": "surname:Kumar;given-names:AA", "name_1": "surname:Steyvers;given-names:M", "name_2": "surname:Balota;given-names:DA", "section_type": "REF", "source": "Top. Cogn. Sci.", "type": "ref", "volume": "1", "year": "2021"}, "text": "A critical review of network-based and distributional approaches to semantic memory structure and processes", "sentences": [], "annotations": [], "relations": []}, {"offset": 48924, "infons": {"fpage": "11", "lpage": "16", "name_0": "surname:Kenett;given-names:YN", "pub-id_doi": "10.1016/j.cobeha.2018.08.010", "section_type": "REF", "source": "Curr. Opin. Behav. Sci.", "type": "ref", "volume": "27", "year": "2019"}, "text": "What can quantitative measures of semantic distance tell us about creativity?", "sentences": [], "annotations": [], "relations": []}, {"offset": 49002, "infons": {"section_type": "REF", "type": "ref"}, "text": "Vitevitch, M.S. et al. Using complex networks to understand the mental lexicon. In: Yearbook of the Poznan Linguistic Meeting. Vol. 1. 1. Sciendo, pp. 119\u2013138 (2014).", "sentences": [], "annotations": [], "relations": []}, {"offset": 49171, "infons": {"fpage": "46730", "name_0": "surname:Stella;given-names:M", "name_1": "surname:Beckage;given-names:NM", "name_2": "surname:Brede;given-names:M", "pub-id_doi": "10.1038/srep46730", "pub-id_pmid": "28436476", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "7", "year": "2017"}, "text": "Multiplex lexical networks reveal patterns in early word acquisition in children", "sentences": [], "annotations": [], "relations": []}, {"offset": 49252, "infons": {"fpage": "1", "issue": "1", "lpage": "11", "name_0": "surname:Stella;given-names:M", "pub-id_doi": "10.1038/s41598-018-20730-5", "pub-id_pmid": "29311619", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "8", "year": "2018"}, "text": "Multiplex model of mental lexicon reveals explosive learning in humans", "sentences": [], "annotations": [], "relations": []}, {"offset": 49323, "infons": {"fpage": "1", "issue": "1", "lpage": "14", "name_0": "surname:Levy;given-names:O", "pub-id_doi": "10.1038/s41598-021-93925-y", "pub-id_pmid": "33414495", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "11", "year": "2021"}, "text": "Unveiling the nature of interaction between semantics and phonology in lexical access based on multilayer networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 49438, "infons": {"section_type": "REF", "type": "ref"}, "text": "Pennington, J., Socher, R., & Manning, C.D. Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). (2014), pp. 1532\u20131543.", "sentences": [], "annotations": [], "relations": []}, {"offset": 49654, "infons": {"fpage": "213", "issue": "1", "lpage": "231", "name_0": "surname:De Deyne;given-names:S", "name_1": "surname:Storms;given-names:G", "pub-id_doi": "10.3758/BRM.40.1.213", "pub-id_pmid": "18411545", "section_type": "REF", "source": "Behav. Res. Methods", "type": "ref", "volume": "40", "year": "2008"}, "text": "Word associations: Network and semantic properties", "sentences": [], "annotations": [], "relations": []}, {"offset": 49705, "infons": {"fpage": "1", "name_0": "surname:Jackson;given-names:J", "section_type": "REF", "source": "Perspect. Psychol. Sci.", "type": "ref", "volume": "1", "year": "2021"}, "text": "From text to thought: How analyzing language can advance psychological science", "sentences": [], "annotations": [], "relations": []}, {"offset": 49784, "infons": {"fpage": "203", "issue": "2", "lpage": "208", "name_0": "surname:Lund;given-names:K", "name_1": "surname:Burgess;given-names:C", "pub-id_doi": "10.3758/BF03204766", "section_type": "REF", "source": "Behav. Res. Methods Instrum. Comput.", "type": "ref", "volume": "28", "year": "1996"}, "text": "Producing high-dimensional semantic spaces from lexical co-occurrence", "sentences": [], "annotations": [], "relations": []}, {"offset": 49854, "infons": {"fpage": "1", "lpage": "41", "name_0": "surname:Comin;given-names:CH", "pub-id_doi": "10.1016/j.physrep.2020.03.002", "section_type": "REF", "source": "Phys. Rep.", "type": "ref", "volume": "861", "year": "2020"}, "text": "Complex systems: Features, similarity and connectivity", "sentences": [], "annotations": [], "relations": []}, {"offset": 49909, "infons": {"fpage": "1", "issue": "1", "lpage": "26", "name_0": "surname:Veremyev;given-names:A", "pub-id_doi": "10.1007/s41109-019-0228-y", "section_type": "REF", "source": "Appl. Netw. Sci.", "type": "ref", "volume": "4", "year": "2019"}, "text": "Graph-based exploration and clustering analysis of semantic spaces", "sentences": [], "annotations": [], "relations": []}, {"offset": 49976, "infons": {"fpage": "25", "issue": "1", "lpage": "34", "name_0": "surname:Rossetti;given-names:G", "name_1": "surname:Citraro;given-names:S", "name_2": "surname:Milli;given-names:L", "pub-id_doi": "10.1109/MIS.2021.3051291", "section_type": "REF", "source": "IEEE Intell. Syst.", "type": "ref", "volume": "36", "year": "2021"}, "text": "Conformity: A path-aware homophily measure for node-attributed networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 50048, "infons": {"section_type": "REF", "type": "ref"}, "text": "MacWhinney, B. The CHILDES project: The database. Vol. 2. (Psychology Press, 2000).", "sentences": [], "annotations": [], "relations": []}, {"offset": 50132, "infons": {"fpage": "046111", "issue": "4", "name_0": "surname:Holme;given-names:P", "pub-id_doi": "10.1103/PhysRevE.72.046111", "section_type": "REF", "source": "Phys. Rev. E", "type": "ref", "volume": "72", "year": "2005"}, "text": "Core-periphery organization of complex networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 50180, "infons": {"section_type": "REF", "type": "ref"}, "text": "Whelan, C., Harrell, G., & Wang, J. Understanding the k-medians problem. In: Proceedings of the International Conference on Scientific Computing (CSC). The Steering Committee of The World Congress in Computer Science, Computer, p. 219 (2015).", "sentences": [], "annotations": [], "relations": []}, {"offset": 50423, "infons": {"fpage": "553", "issue": "2", "lpage": "565", "name_0": "surname:Hadley;given-names:PA", "name_1": "surname:McKenna;given-names:MM", "name_2": "surname:Rispoli;given-names:M", "pub-id_doi": "10.1044/2017_AJSLP-17-0098", "pub-id_pmid": "29497741", "section_type": "REF", "source": "Am. J. Speech Lang. Pathol.", "type": "ref", "volume": "27", "year": "2018"}, "text": "Sentence diversity in early language development: Recommendations for target selection and progress monitoring", "sentences": [], "annotations": [], "relations": []}, {"offset": 50534, "infons": {"fpage": "1069", "issue": "12", "lpage": "1076", "name_0": "surname:Griffiths;given-names:TL", "name_1": "surname:Steyvers;given-names:M", "name_2": "surname:Firl;given-names:A", "pub-id_doi": "10.1111/j.1467-9280.2007.02027.x", "pub-id_pmid": "18031414", "section_type": "REF", "source": "Psychol. Sci.", "type": "ref", "volume": "18", "year": "2007"}, "text": "Google and the mind: Predicting fluency with PageRank", "sentences": [], "annotations": [], "relations": []}, {"offset": 50588, "infons": {"name_0": "surname:Fenson;given-names:Larry", "section_type": "REF", "source": "MacArthur-Bates communicative development inventories", "type": "ref", "year": "2007"}, "text": "", "sentences": [], "annotations": [], "relations": []}, {"offset": 50589, "infons": {"fpage": "1717", "issue": "6", "lpage": "1724", "name_0": "surname:Van Rensbergen;given-names:B", "name_1": "surname:Storms;given-names:G", "name_2": "surname:De Deyne;given-names:S", "pub-id_doi": "10.3758/s13423-015-0832-5", "pub-id_pmid": "25893712", "section_type": "REF", "source": "Psychon. Bull. Rev.", "type": "ref", "volume": "22", "year": "2015"}, "text": "Examining assortativity in the mental lexicon: Evidence from word associations", "sentences": [], "annotations": [], "relations": []}, {"offset": 50668, "infons": {"fpage": "389", "issue": "3", "lpage": "426", "name_0": "surname:Casas;given-names:B", "pub-id_doi": "10.1075/is.16036.cas", "section_type": "REF", "source": "Interact. Stud.", "type": "ref", "volume": "19", "year": "2018"}, "text": "The polysemy of the words that children learn over time", "sentences": [], "annotations": [], "relations": []}, {"offset": 50724, "infons": {"fpage": "1369", "issue": "11", "lpage": "1379", "name_0": "surname:Ferrer-i-Cancho;given-names:R", "name_1": "surname:Vitevitch;given-names:MS", "section_type": "REF", "source": "J. Am. Soc. Inf. Sci.", "type": "ref", "volume": "69", "year": "2018"}, "text": "The origins of Zipf\u2019s meaning-frequency law", "sentences": [], "annotations": [], "relations": []}, {"offset": 50770, "infons": {"section_type": "REF", "type": "ref"}, "text": "Huang, Z. Clustering large data sets with mixed numeric and categorical values. In: Proceedings of the 1st Pacific-Asia conference on knowledge discovery and data mining (PAKDD), pp. 21\u201334 (Citeseer, 1997)", "sentences": [], "annotations": [], "relations": []}, {"offset": 50978, "infons": {"section_type": "REF", "type": "ref"}, "text": "Pepper, J., & Weitzman, E. It takes two to talk: A practical guide for parents of children with language delays. The Hanen Centre, (2004).", "sentences": [], "annotations": [], "relations": []}, {"offset": 51117, "infons": {"section_type": "REF", "type": "ref"}, "text": "Yamaguchi, H. et al. Controlling internal structure of communities on graph generator. In: 2020 IEEE/ACM international conference on advances in social networks analysis and mining (ASONAM). IEEE., pp. 937\u2013940 (2020).", "sentences": [], "annotations": [], "relations": []}, {"offset": 51337, "infons": {"fpage": "1", "issue": "1", "lpage": "9", "name_0": "surname:Valba;given-names:O", "name_1": "surname:Gorsky;given-names:A", "pub-id_doi": "10.1038/s41598-022-09499-w", "pub-id_pmid": "34992227", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "12", "year": "2022"}, "text": "K-clique percolation in free association networks and the possible mechanism behind the 7 pm 2 law", "sentences": [], "annotations": [], "relations": []}, {"offset": 51436, "infons": {"fpage": "1", "issue": "1", "lpage": "34", "name_0": "surname:\u0160krlj;given-names:B", "name_1": "surname:Renoust;given-names:B", "pub-id_doi": "10.1007/s41109-020-00331-w", "section_type": "REF", "source": "Appl. Netw. Sci.", "type": "ref", "volume": "5", "year": "2020"}, "text": "Layer entanglement in multiplex, temporal multiplex, and coupled multilayer networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 51521, "infons": {"fpage": "1", "issue": "1", "lpage": "20", "name_0": "surname:Pio-Lopez;given-names:L", "pub-id_doi": "10.1038/s41598-021-87987-1", "pub-id_pmid": "33414495", "section_type": "REF", "source": "Sci. Rep.", "type": "ref", "volume": "11", "year": "2021"}, "text": "MultiVERSE: A multiplex and multiplex-heterogeneous network embedding approach", "sentences": [], "annotations": [], "relations": []}, {"offset": 51600, "infons": {"fpage": "e12899", "issue": "11", "name_0": "surname:Chang;given-names:LM", "name_1": "surname:De\u00e1k;given-names:GO", "pub-id_doi": "10.1111/cogs.12899", "pub-id_pmid": "33164262", "section_type": "REF", "source": "Cogn. Sci.", "type": "ref", "volume": "44", "year": "2020"}, "text": "Adjacent and Non-Adjacent Word Contexts Both Predict Age of Acquisition of English Words: A Distributional Corpus Analysis of Child-Directed Speech", "sentences": [], "annotations": [], "relations": []}, {"offset": 51748, "infons": {"fpage": "1", "lpage": "20", "name_0": "surname:Cox;given-names:C", "section_type": "REF", "source": "Nat. Hum. Behav.", "type": "ref", "volume": "1", "year": "2022"}, "text": "A systematic review and Bayesian meta-analysis of the acoustic features of infant-directed speech", "sentences": [], "annotations": [], "relations": []}, {"offset": 51846, "infons": {"section_type": "REF", "type": "ref"}, "text": "MacWhinney, B. The CHILDES project: Tools for analyzing talk, Volume II: The database (Psychology Press, 2014).", "sentences": [], "annotations": [], "relations": []}, {"offset": 51958, "infons": {"fpage": "e0136277", "issue": "8", "name_0": "surname:Utsumi;given-names:A", "pub-id_doi": "10.1371/journal.pone.0136277", "pub-id_pmid": "26295940", "section_type": "REF", "source": "PLoS ONE", "type": "ref", "volume": "10", "year": "2015"}, "text": "A complex network approach to distributional semantic models", "sentences": [], "annotations": [], "relations": []}, {"offset": 52019, "infons": {"fpage": "e246", "issue": "2", "lpage": "e262", "name_0": "surname:Pomper;given-names:R", "name_1": "surname:Saffran;given-names:JR", "pub-id_doi": "10.1111/cdev.13053", "pub-id_pmid": "29512135", "section_type": "REF", "source": "Child Dev.", "type": "ref", "volume": "90", "year": "2019"}, "text": "Familiar object salience affects novel word learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 52072, "infons": {"section_type": "REF", "type": "ref"}, "text": "Siow, S., & Plunkett, K. Exploring the variable effects of frequency and semantic diversity as predictors for a word\u2019s ease of acquisition in different word classes. In: Proceedings of the Annual Meeting of the Cognitive Science Society. Vol. 43 (2021).", "sentences": [], "annotations": [], "relations": []}, {"offset": 52328, "infons": {"fpage": "20160055", "issue": "1711", "name_0": "surname:Clerkin;given-names:EM", "pub-id_doi": "10.1098/rstb.2016.0055", "section_type": "REF", "source": "Philos. Trans. R. Soc. B Biol. Sci.", "type": "ref", "volume": "372", "year": "2017"}, "text": "Real-world visual statistics and infants\u2019 first-learned object names", "sentences": [], "annotations": [], "relations": []}, {"offset": 52399, "infons": {"fpage": "P05006", "issue": "5", "name_0": "surname:Stella;given-names:M", "name_1": "surname:Brede;given-names:M", "pub-id_doi": "10.1088/1742-5468/2015/05/P05006", "section_type": "REF", "source": "J. Stat. Mech: Theory Exp.", "type": "ref", "volume": "2015", "year": "2015"}, "text": "Patterns in the English language: Phonological networks, percolation and assembly models", "sentences": [], "annotations": [], "relations": []}, {"offset": 52488, "infons": {"fpage": "682", "issue": "9", "lpage": "692", "name_0": "surname:Sizemore;given-names:AE", "pub-id_doi": "10.1038/s41562-018-0422-4", "pub-id_pmid": "30333998", "section_type": "REF", "source": "Nat. Hum. Behav.", "type": "ref", "volume": "2", "year": "2018"}, "text": "Knowledge gaps in the early growth of semantic feature networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 52552, "infons": {"fpage": "e19348", "issue": "5", "name_0": "surname:Beckage;given-names:N", "name_1": "surname:Smith;given-names:L", "name_2": "surname:Hills;given-names:T", "pub-id_doi": "10.1371/journal.pone.0019348", "pub-id_pmid": "21589924", "section_type": "REF", "source": "PLoS ONE", "type": "ref", "volume": "6", "year": "2011"}, "text": "Small worlds and semantic network growth in typical and late talkers", "sentences": [], "annotations": [], "relations": []}, {"offset": 52621, "infons": {"fpage": "906", "issue": "6", "lpage": "914", "name_0": "surname:Romberg;given-names:AR", "name_1": "surname:Saffran;given-names:JR", "section_type": "REF", "source": "Cogn. Sci.", "type": "ref", "volume": "1", "year": "2010"}, "text": "Statistical learning and language acquisition", "sentences": [], "annotations": [], "relations": []}, {"offset": 52667, "infons": {"fpage": "e13008", "issue": "7", "name_0": "surname:Siew;given-names:SQ", "pub-id_doi": "10.1111/cogs.13008", "pub-id_pmid": "34213787", "section_type": "REF", "source": "Cogn. Sci.", "type": "ref", "volume": "45", "year": "2021"}, "text": "Global and local feature distinctiveness effects in language acquisition", "sentences": [], "annotations": [], "relations": []}, {"offset": 52740, "infons": {"fpage": "1", "issue": "1", "lpage": "12", "name_0": "surname:Artime;given-names:O", "name_1": "surname:De Domenico;given-names:M", "pub-id_doi": "10.1038/s41467-021-22721-z", "pub-id_pmid": "33397941", "section_type": "REF", "source": "Nat. Commun.", "type": "ref", "volume": "12", "year": "2021"}, "text": "Percolation on feature-enriched interconnected systems", "sentences": [], "annotations": [], "relations": []}, {"offset": 52795, "infons": {"fpage": "987", "issue": "3", "lpage": "1006", "name_0": "surname:De Deyne;given-names:S", "pub-id_doi": "10.3758/s13428-018-1115-7", "pub-id_pmid": "30298265", "section_type": "REF", "source": "Behav. Res. Methods", "type": "ref", "volume": "51", "year": "2019"}, "text": "The Small World of Words English word association norms for over 12,000 cue words", "sentences": [], "annotations": [], "relations": []}, {"offset": 52877, "infons": {"fpage": "547", "issue": "4", "lpage": "559", "name_0": "surname:McRae;given-names:K", "pub-id_doi": "10.3758/BF03192726", "pub-id_pmid": "16629288", "section_type": "REF", "source": "Behav. Res. Methods", "type": "ref", "volume": "37", "year": "2005"}, "text": "Semantic feature production norms for a large set of living and nonliving things", "sentences": [], "annotations": [], "relations": []}, {"offset": 52958, "infons": {"fpage": "622", "issue": "9", "lpage": "623", "name_0": "surname:Hills;given-names:TT", "name_1": "surname:Siew;given-names:CSQ", "pub-id_doi": "10.1038/s41562-018-0428-y", "pub-id_pmid": "31346280", "section_type": "REF", "source": "Nat. Hum. Behav.", "type": "ref", "volume": "2", "year": "2018"}, "text": "Filling gaps in early word learning", "sentences": [], "annotations": [], "relations": []}, {"offset": 52994, "infons": {"fpage": "159", "lpage": "180", "name_0": "surname:Carlson;given-names:MT", "name_1": "surname:Sonderegger;given-names:M", "name_2": "surname:Bane;given-names:M", "pub-id_doi": "10.1016/j.jml.2014.05.005", "pub-id_pmid": "25089073", "section_type": "REF", "source": "J. Mem. Lang.", "type": "ref", "volume": "75", "year": "2014"}, "text": "How children explore the phonological network in child-directed speech: A survival analysis of children\u2019s first word productions", "sentences": [], "annotations": [], "relations": []}, {"offset": 53125, "infons": {"fpage": "729", "issue": "6", "lpage": "739", "name_0": "surname:Hills;given-names:TT", "pub-id_doi": "10.1111/j.1467-9280.2009.02365.x", "pub-id_pmid": "19470123", "section_type": "REF", "source": "Psychol. Sci.", "type": "ref", "volume": "20", "year": "2009"}, "text": "Longitudinal analysis of early semantic networks: Preferential attachment or preferential acquisition?", "sentences": [], "annotations": [], "relations": []}, {"offset": 53228, "infons": {"section_type": "REF", "type": "ref"}, "text": "Miller, G.A. WordNet: An electronic lexical database (MIT press, 1998).", "sentences": [], "annotations": [], "relations": []}, {"offset": 53300, "infons": {"section_type": "REF", "type": "ref"}, "text": "Zipf, G.K. Human behavior and the principle of least effort: An introduction to human ecology. Ravenio Books, (2016).", "sentences": [], "annotations": [], "relations": []}, {"offset": 53418, "infons": {"section_type": "REF", "type": "ref"}, "text": "Molloy, M. et al. A critical point for random graphs with a given degree sequence. In: The Structure and Dynamics of Networks, pp. 240\u2013258 (Princeton University Press, 2011).", "sentences": [], "annotations": [], "relations": []}, {"offset": 53595, "infons": {"section_type": "REF", "type": "ref"}, "text": "MacQueen, J. et al. Some methods for classification and analysis of multivariate observations. In: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. 14. Oakland, CA, USA. (1967), pp. 281\u2013297.", "sentences": [], "annotations": [], "relations": []}, {"offset": 53835, "infons": {"section_type": "REF", "type": "ref"}, "text": "Newman, M. Networks (Oxford University Press, 2018).", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]