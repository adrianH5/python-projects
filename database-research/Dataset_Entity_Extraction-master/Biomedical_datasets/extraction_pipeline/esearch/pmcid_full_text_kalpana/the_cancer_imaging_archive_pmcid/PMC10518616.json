[{"source": "PMC", "date": "20231002", "key": "pmc.key", "infons": {}, "documents": [{"id": "10518616", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.3389/fmedt.2023.1157919", "article-id_pmc": "10518616", "article-id_pmid": "37752910", "elocation-id": "1157919", "kwd": "cuckoo search algorithm deep learning encoder jaccard similarity score lung nodule", "license": "This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.", "name_0": "surname:M;given-names:Grace John", "name_1": "surname:S;given-names:Baskar", "section_type": "TITLE", "type": "front", "volume": "5", "year": "2023"}, "text": "DeepNet model empowered cuckoo search algorithm for the effective identification of lung cancer nodules", "sentences": [], "annotations": [], "relations": []}, {"offset": 104, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 117, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "Globally, lung cancer is a highly harmful type of cancer. An efficient diagnosis system can enable pathologists to recognize the type and nature of lung nodules and the mode of therapy to increase the patient's chance of survival. Hence, implementing an automatic and reliable system to segment lung nodules from a computed tomography (CT) image is useful in the medical industry.", "sentences": [], "annotations": [], "relations": []}, {"offset": 498, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 506, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "This study develops a novel fully convolutional deep neural network (hereafter called DeepNet) model for segmenting lung nodules from CT scans. This model includes an encoder/decoder network that achieves pixel-wise image segmentation. The encoder network exploits a Visual Geometry Group (VGG-19) model as a base architecture, while the decoder network exploits 16 upsampling and deconvolution modules. The encoder used in this model has a very flexible structural design that can be modified and trained for any resolution based on the size of input scans. The decoder network upsamples and maps the low-resolution attributes of the encoder. Thus, there is a considerable drop in the number of variables used for the learning process as the network recycles the pooling indices of the encoder for segmentation. The Thresholding method and the cuckoo search algorithm determines the most useful features when categorizing cancer nodules.", "sentences": [], "annotations": [], "relations": []}, {"offset": 1445, "infons": {"section_type": "ABSTRACT", "type": "abstract_title_1"}, "text": "Results and discussion", "sentences": [], "annotations": [], "relations": []}, {"offset": 1468, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The effectiveness of the intended DeepNet model is cautiously assessed on the real-world database known as The Cancer Imaging Archive (TCIA) dataset and its effectiveness is demonstrated by comparing its representation with some other modern segmentation models in terms of selected performance measures. The empirical analysis reveals that DeepNet significantly outperforms other prevalent segmentation algorithms with 0.962\u2009\u00b1\u20090.023% of volume error, 0.968\u2009\u00b1\u20090.011 of dice similarity coefficient, 0.856\u2009\u00b1\u20090.011 of Jaccard similarity index, and 0.045\u2009\u00b1\u20090.005s average processing time.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2073, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 2086, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Lung cancer is the most lethal cancer affecting men and women, leading to 18.4% of cancer mortality globally in 2018. Current oncology breakthroughs including tyrosine kinase inhibitors and immune checkpoint inhibitors provide considerably larger survival improvements for cancer patients. But, much work is still to be carried out in computer-aided diagnosis systems, particularly in medical screening and timely lung cancer diagnosis. Automatic segmentation and classification would directly impact the workflow of medical practice in radiation oncology, one of the most widely used treatment methods for lung cancer.", "sentences": [], "annotations": [], "relations": []}, {"offset": 2706, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Lung cancer radio therapeutics exploit medical imaging to find the exact location of nodules and electron densities to estimate the dosage at any point in the patient for disease management. Effective isolation of the lung nodules is indispensable as inaccuracies might cause under- or over-radiation of the malignant and benign cells. It is projected that a 1\u2005mm variation of the nodule isolation could result in the radiation therapy dosage estimations by up to 15%. Hence, automatic and precise isolation can considerably decrease the time required for oncologists to plan effective therapy and re-plan adaptive therapy based on the variations in the nodules.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3371, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Manual segmentation of the lung nodules from CT scans is an error-prone and challenging task. As the correctness of the cancer diagnostic system depends on the severity of cancer and the oncologist's knowledge based on experience and decision, it leads to an inaccurate assessment. Due to lung nodules' very complex biological, molecular, and structural features, the analysis of these scans is hard in clinical pathology. Simultaneously, these confusing aspects encouraged numerous investigators to design new screening methods and analytic data to support the early detection of liver cancer, which can classify cancer cells with better accuracy and enhance clinical results. Besides, lung nodules' automatic screening will be faster than manual segmentation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 4133, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In conjunction with advances in storing and the quality of clinical scans, the current innovation of machine learning approaches has powered rigorous exploration in the domain of artificial intelligence (AI) for analyzing clinical images. Deep learning (DL) approaches are a section of AI-based neural networks that are effectively employed to resolve problems of image segmentation or classification at remarkable speeds without degrading accuracy. Many effective automatic diagnostic models have been established to solve these clinical imaging problems. However, the main obstacle in designing a fully automatic tool is the inhomogeneity of the databases that can be used for any CT, particularly when collected from various medical organizations. CT images with various findings or reconstruction elements make the structure of the lung region different. The approaches found in the present literature frequently need powerful preprocessing methods. The data synchronization problem remains to be resolved by a data-oriented method, demanding large databases signifying all features of this heterogeneity.", "sentences": [], "annotations": [], "relations": []}, {"offset": 5243, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The convolutional neural network (CNN) mimics the human visual system and is recognized to be the best image segmentation approach. Deep convolutional neural (DCNN) networks have recently been used to design an automated system for identifying and classifying lung cancer through medical imaging. This approach enables promising results, particularly in lung cancer segmentation. The usefulness of this approach on lung nodule segmentation has been evaluated against other machine learning methods recently. Also, some researchers explored the benefits of exploiting deep convolutional networks for nodule segmentation against pathologists. The decoder network makes use of 16 up sampling and deconvolution modules, the encoder network is predicated on a VGG model. This model employs an encoder with a highly adaptable architectural construction that can be learned to produce images of arbitrary resolutions, independent of the size of their input scans. In order to improve the encoder's low-resolution features, the decoder network up samples and maps them. The thresholding method removes redundant groups by isolating the particular lung regions and nodules of interest that the cuckoo search algorithm may conclusively identify. At the feature extraction stage, the surface highlighting for the specific nodule is unaffected by the instances from similar neighborhoods. DCNN outperforms oncologists\u2019 decisions in nodule detection.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6683, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "This work proposes a fully convolutional deep neural network (DeepNet) model for segmenting lung nodules from CT scans.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6803, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "DeepNet model includes an encoder-decoder network to achieve pixel-wise image segmentation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6895, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The cuckoo search algorithm separates the lung nodule in the dataset images in the form of segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 6999, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The encoder module exploits a VGG-19 model as a base architecture, whereas the decoder module exploits 16 upsampling and deconvolution modules.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7143, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The encoder has a flexible structural design that can be modified and trained for any resolution and size of input scans.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7265, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The decoder module upsamples and maps the low-resolution attributes of the encoder. Thus, there is a considerable drop in the number of variables used for the learning process as the network reuses the pooling parameters of the encoder for segmentation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7519, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The DeepNet has been evaluated in terms of error rate, dice similarity coefficient, Jaccard similarity index, and average processing time.", "sentences": [], "annotations": [], "relations": []}, {"offset": 7658, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The main contribution of this paper: The remaining parts of this manuscript are arranged as follows: We explore the related works about DCNN-based nodule isolation methods in section 2. Section 3 discusses the segmentation network and explores how every phase operates. Section 4 discusses the experimental procedure used in this work. Finally, we conclude Section 5.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8026, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Related work", "sentences": [], "annotations": [], "relations": []}, {"offset": 8039, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Recently, numerous DL methods have been developed to solve the segmentation problem. In this section, we have studied some DL methods for handling lung cancer isolation. Mukherjee et al. developed an isolation approach using DL to localize the lung cancer and retain the nodules' structural characteristics through the graph cut technique. The outcome of isolation approach is77.67% of DSC and ASD of 0.24. The proposed model is not trained for comprehensive dataset and need to investigate how increasing the model's network complexity can impact its effectiveness.", "sentences": [], "annotations": [], "relations": []}, {"offset": 8606, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Wang et al. developed a multiple-view convolutional network to segment lung cancer by collecting axial, sagittal, and coronal observations about the cancer voxel. The outcome of multiple-view convolutional network is77.67% of DSC and ASD of 0.24. The proposed model is not trained for comprehensive dataset and need to investigate how increasing the model's network complexity can impact its effectiveness.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9013, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Roy et al. proposed a technique to integrate a level set and DL for isolating the affected regions in the lung. Since the lung is a three-dimensional (3D) structure, we need to review 3D DL models for segmenting lung nodules. More accuracy is produced for the proposed method. The segmentation of lung nodule is the major drawback.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9345, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Hossain et al. proposed a novel dilated hybrid-3D CNN structure for nodule segmentation (3D LungNet). It can exploit the 3D data existing within image volumes. First, a binary classification algorithm selects images that may include slices of a nodule. To isolate the nodules, the designated images are fed to the DL algorithm for segmentation which selects attribute vectors from every 2D slice through dilated CNN and then combines the pooled vectors using 3D convolutional operations by integrating the morphological features in the CT image. The automated pipeline for lung tumor detection outperforms all segmentation network. Plans for the future include simultaneously developing a binary classifier and segmentation system, as well as training the pipeline with thicker stacks of segments.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10143, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Badrinarayanan et al. proposed a new deep CNN structure for semantic element-wise isolation called SegNet. This primary isolation model contains an encoding module, an equivalent decoding module, and an element-wise classifier. The role of the decoder module is to relate the lower-resolution attribute vector of the encoding module to the high-resolution input attribute vector for element-wise analysis. The innovation of this model lies in how the decoding module unspools its distorted input attribute vector. Especially the decoding module exploits pooling parameters calculated in the corresponding upsampling phase to achieve non-linear unspooling. This removes the training predictability of unspooling operations. SegNet has competitive performance on big and datasets, including strong scores for roadway scene interpretation. The end-to-end learning of deep division structures deserves more research effort because it is a more difficult topic.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11100, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Chen et al. propose a new 3D DL model for lung nodule isolation from CT scans, called multiple-attention U-Net (MAU-Net). This model first uses a dual attention unit at the restriction of the U-Net that defines the definite relationship between channel and spatial attributes. The multiple-attention unit is then used to dynamically compute and combine multiresolution attributes from the dual attention unit from the encoding module. ResNet exploits different residual convolutional modules to extract the CT scans' significant attributes effectively. The attributes from all levels of the ResNet were combined into a single output. This simple architecture realized a combination of shallow appearance attributes and deep semantic attributes to produce dense pixel outputs. The lung cancer segmentation by existing dual attention methods is well explained. The proposed methods fail to investigate its performance in various medical imaging tasks.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12050, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Zhao et al. developed a contextual CNN using 3D U-Net to isolate and categorize nodules automatically and help oncologists interpret CT scans. The skip connections in conventional U-Net cause distortion in selected input attributes. On the other hand, the higher-resolution attributes selected by this model generally do not comprise sufficient higher-level boundary statistics of the image, causing distress in the decisions of the model significantly. Finally, in order to decrease the number of false positive candidate nodules, a contextual CNN is utilized to categorize nodules as malignant or benign. The main advantage is that contextual information of nodule is considered for the prediction process. The proposed method need to be considered for small dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12820, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Seo et al. developed a Modified U-Net (mU-Net) for segmenting lung cancer from CT scans. This model exploits a residual unit with de-convolutional and activation functions using dropout connections to resolve the issues due to low-resolution attributes in conventional U-Net architecture. U-Net is a widely recognized CNN model for isolating lung nodules. The U-Net structure contains two parts; a shrinking phase to collect background information and a symmetric growing phase to achieve precise localization. The shrinking phase comprises successive convolution and max-pooling layers. It is employed to select features while limiting the size of the feature vector. The growing phase contains convolution layers and achieves up-conversion to obtain the attribute's dimension related to the loss of morphological features. Besides, the localization data is exchanged between the shrinking and the growing module using dropout units. These connections are operated autonomously and permit information to be communicated from one module to another within the network without adding any processing overhead. Finally, this research introduces a more powerful deep learning network for segmentation, which, depending on the specifics of the situation, may provide better outcomes than competing networks in the segmentation of liver and tumor areas, where the border is not evident and the target item is tiny.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14228, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Jalali et al. proposed an adapted U-Net where the encoding module is substituted by a learned ResNet-34 model. This network uses a bidirectional convolutional long short-term memory to integrate the selected attribute vector of the equivalent shrinking phase into the earlier growth of the up-convolution module. Then, a densely connected convolution module is used for the shrinking phase. Several abovementioned models have accomplished their goals effectively. However, their segmentation performance in terms of volume error, dice similarity coefficient, Jaccard similarity score, and average segmentation time is often not the best. Therefore, a novel model called DeepNet is developed for segmenting lung nodules with improved performance. The problem of a large number of false positives has also been addressed by the proposed strategy. The proposed strategy has also overcome the problem of removing nodules that have become connected to the lung wall. A further possibility for future efforts is the use of several deep learning-based systems to classify medical pictures.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15311, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Li et al. used liquid biopsy to provide a novel method for early screening, diagnosis, and management of lung cancer, particularly when tissue samples are unavailable. The use of circulation biomarkers and liquid biopsies in lung cancer allows for assessing the immediate molecular, genetic, and epigenetic profile of cancerous cells identified as drug-resistant clones from earlier therapy. Liquid biopsies are helpful because they are non-invasive, simple to collect, represent the overall condition of cancer, and provide real-time surveillance. Before liquid biopsy can be broadly employed in clinical practise, it needs to be improved through the employment of more cutting-edge molecular biological detection techniques in order to increase its validity and applicability.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16090, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Jiang et al. described the multimodality MRI-based radiomics approach for predicting lung cancer. Radiomics characters are derived using random forest techniques using the multimodal MRI scan data. After collecting data from the initial cohort, the randomized forest radiomics score varies depending on a separate group of individuals. The accuracy of the predictions was determined using a combination of the ROC curve, the calibration curve, and the decision curve.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16558, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Ji et al. designed a encoder-decoder design approach to normalize 3D point clouds by taking properties into account, before building voxels to feed into the procedure for learning. The suggested technique performs exceptionally well overall, making a significant contribution to the multi-class object identification from 3D tunnel clouds of points.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16908, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Amr Abdelraouf et al. modeled attention-based multi-encoder-decoders (Att-MED) for estimating travel times. Short-term, daily, and weekly traffic trends are only some of the input sequences that are used by the model's convolutional- Long short-term memory (LSTM) to record the spatial-temporal link between them. The model also uses an LSTM to progressively predict outputs. In addition, an attention method is employed to quantify the value of each traffic sequence's input into the final forecasts. When the suggested network architecture is trained from beginning to end, it outperforms baseline models in terms of forecasting accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 17549, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Wei et al. examined the Lane Changing (LC) procedure and recommends LC segmentation and sampling approach that divides the procedure into four distinct phases. Using route data from all four LC phases, we validate the optimum attention-aided encoder-decoder model and subsequently use it to inform the creation of a heuristic network model. While doing so, the suggested heuristic network is linked to the Deep Neural Network (DNN) to forecast vehicle kinematics data. Finally, a combined cascade prediction model is formed by testing the heuristic network and DNN in succession; this model may execute a fine-grained LC specification on the basis of the prediction outcomes. The experimental findings demonstrate that the suggested cascade forecasting model can accurately anticipate the trajectory, velocity, acceleration, and steering angle of a vehicle over a long period of time and can provide a fine-grained LC characterization. Further theoretical investigation intelligent connected vehicles (ICVs) and connected autonomous vehicles (CAVs).may benefit from the presented prediction model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18647, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Wang et al. isolated distinct cell populations from single-cell transcriptome profiles, a single-cell deep clustering model using a dual denoising autoencoder with bipartite graph ensemble clustering (scBGEDA) is presented. Data is first offered to be projected into a low-dimensional place using a single-cell dual denoising autoencoder network, which can then learn feature illustration through explicit simulation for denoising reconstruction loss.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19099, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Yu et al. discussed regarding a single-cell model-based deep graph embedding clustering (scTAG) technique uses a deep graph convolutional structure to acquire cell-cell topological descriptions and to identify cell clusters in real time. To train the low-dimensional latent participation, scTAG incorporates the zero-inflated negative binomial (ZINB) model into a topological adaptive graph convolutional autoencoder.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19517, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Overall, several methods showed promise in lung nodule identification and tracking. Detecting infrequently formalized cancer from large and diverse volumes of lung CT scan images with varying contour, size, and location, the existing method accurately distinguishes between vascular, solitary, pleural, and juxta-pleural adenomas, demonstrating rigorous methods and techniques applicable across distinct datasets.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19931, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The suggested work utilizes a DeepNet model to automatically extract the self-learned characteristics for lung cancer detection based on their malignant untrustworthiness. Classification accuracy, sensitivity, specificity, and fewer false positive rate are improved. The methods, including the specific tools and data sets used, are emphasized, and the findings are compared to previous research in the field.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20341, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "DeepNet model for segmenting lung nodule", "sentences": [], "annotations": [], "relations": []}, {"offset": 20382, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Segmentation of lung nodules on CT scans is imperative for cancer disease management like analysis, radiation therapy, and reaction calculation. This work presents a fully automatic DL-based lung nodule segmentation model that can manage different CT scans. The DeepNet includes an encoder/decoder network that achieves pixel-wise image segmentation. The encoder module contains a VGG-19 model as a base architecture, while the decoding module exploits 16 upsampling and deconvolution units. The encoder used in this model is trained for any resolution and size of input scans. The decoding network unspools and relates the lower-level attributes of the encoder. Thus, there is a considerable drop in the number of variables used for the learning process as the model reuses the pooling parameters of the encoder for isolating the nodules in the lung region. Before processing the CT scans we need to apply preprocessing techniques to improve the quality of the image. DeepNet model is evaluated based on a real-time database by comparing its presentation with similar advanced models concerning volume error, dice coefficient, Jaccard index, and speed.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21536, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "It takes a significant amount of time and it is not always accurate for doctors to confirm lung cancer using CT scans. With the suggested technique, clinicians can detect lung nodules early and analyze their interior structure. As part of the contribution to various problems with diagnosing lung cancer, the Cuckoo search algorithm extracts the area of interest with a unique segmentation method that employs thresholding. Using the Cuckoo search algorithm, nodules of varying sizes and shapes may be precisely separated with a small number of parameters.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22093, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "As shown in Figure\u00a01, the steps involved in the future diagnosis of lung malignancy are as follows: (1) pre-processing to improve contrast and reduce noise; (2) using a Cuckoo search algorithm to separate the cancer cell from its surroundings; (3) feature extraction based on regions of concern; (4) retrieval of descriptive words from segmented lung lesions; and (5) using support vector machines to determine whether the injury is abnormal or not. Detailed explanations of these stages are provided below.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22602, "infons": {"file": "fmedt-05-1157919-g001.jpg", "id": "F1", "section_type": "FIG", "type": "fig_caption"}, "text": "The architecture of the DeepNet model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22641, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Image acquisition", "sentences": [], "annotations": [], "relations": []}, {"offset": 22659, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "It's the first step towards doing anything important. The technique involves retrieving and analyzing digital images from a source. A wide variety of scanners, including x-ray, MRI, and CT machines, are employed to get the final pictures. In this case, a CT scanner is used to get the picture. This scanning technique generates cross-sectional scans of each pixel. The obtained image is given for the next stage of pre-processing.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23090, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Data preprocessing", "sentences": [], "annotations": [], "relations": []}, {"offset": 23109, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "As the attributes are directly extracted from CT scans, it displays various gray scales and intensities essential to using a preprocessing strategy before such scans are fed to the segmentation algorithm. Data preprocessing approaches include normalization and standardization. This study employs a fast and simple method called a min-max scalar to achieve normalization as defined by Equation 1.As shown in Equation 1 normalization has been expressed for the segmentation process. Where  is the normalized attribute value retrieved from attribute space ,  is the minimum attribute value, and  is the maximum attribute value. In this paper, the input scans are preprocessed by eliminating noise and artifacts in the CT scans. Preprocessing includes the following phases: (i) the input scans are of various sizes and intensities. The input image with different sizes and intensities are used in the initial stage. The next stage is edge detection stages and this is obtained by grey scale images. The grey scale images are converted into RGB format luma and chroma. Luma and chroma color space is most significant for segmentation. Hence, all the scans have been transformed into a regular size of 128\u2009\u00d7\u2009128 before applying the segmentation algorithm; (ii) A filter with values ([\u22121, 0, \u22121], [0, 5, 0], [\u22121, 0, \u22121]) is used for finding edges of the nodules (iii) the values of each picture element are computed by transforming the red-green-blue (RGB) color to the luma and chroma (YUV) color space. Luminance is more imperative than color for segmentation. Hence, the resolution of V (red projection) and U (blue projection) are decreased however Y is preserved at high-resolution (iv) the intensity values of each picture element are balanced by transforming the YUV color back to RGB color space by flattening boundaries and intensity equalization. The pre-processed image is given to the DeepNet Nodule segmentation stage. In pre-processing stage the noise in image is removed using weighted histogram equalization and the intensity of equalization. From the pre-processed image edges are detected and the color image of RGB format is obtained. RGB image is transferred into YUV format. The above step is followed by segmentation and filtration. The final output image is obtained as RGB transformation as shown in Figure\u00a02.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25451, "infons": {"file": "fmedt-05-1157919-g002.jpg", "id": "F2", "section_type": "FIG", "type": "fig_caption"}, "text": "Pre-processed image and the output image as RGB transformation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25515, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Nodule segmentation using DeepNet", "sentences": [], "annotations": [], "relations": []}, {"offset": 25549, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "This method seeks certain classes of pixels inside an image to create a concentrated image object and checks their proximity to one another. In medical imaging, segmenting the foreground data from the backdrop is challenging. The segmentation method excels at recognizing or smearing front objects' context information. Following the threshold collection standard, this method takes advantage of the largest class variation between the context and the target. The context represents the complete left and right section of the lung, whereas the target represent the affected region in the lung. It divides the picture into the foreground and background parts according to the tones of the grayscale. The separation between the two zones will be greatest if the smallest possible threshold is reached. Variance is a helpful measure, with a bigger number indicating a greater gap between the two regions. Separating the two is inadequate if the areas or contexts are incorrectly divided. Therefore, if there is a lot of variation across groups, the risk of making a mistake in classifying the cancer cell is reduced to get more continuous segmentation, as illustrated in Figure\u00a03.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26728, "infons": {"file": "fmedt-05-1157919-g003.jpg", "id": "F3", "section_type": "FIG", "type": "fig_caption"}, "text": "Path diagram of segmentation process.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26766, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Figure\u00a04 illustrates the architecture of DeepNet. It includes a pixel-wise semantic segmentation model with an encoder/decoder structure. The results obtained from the decoding module are given to a vital segmentation layer, which provides final nodule isolation. DeepNet uses the VGG-19 model as a base network. The fully-connected layers are dropped to preserve the maximum resolution attribute vector for the unspooling procedure in the decoding module. Thus, the size of the resultant pixel-wise encoding module is reduced related to other structures. In DeepNet, each encoding unit has convolution and batch normalization layers with a filter bank. These modules generate a set of attribute vectors. Then, the exponential linear unit (ELU) accelerates the training procedure.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27548, "infons": {"file": "fmedt-05-1157919-g004.jpg", "id": "F4", "section_type": "FIG", "type": "fig_caption"}, "text": "Architecture of DeepNet for lung nodule segmentation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 27602, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The DeepNet model initiates with an input scan  and activation map . The input feeds data to the subsequent convolutional and pooling modules. From the attribute value calculation, the channel calculation is obtained from softmax segmentation. Finally, the decoding module is provided with an isolation vector , afterward applying the softmax (where  is the segmentation parameter). , has the front and background channels. These channels are defined as , and , correspondingly.As deliberated in Equation 2, front and background channels have been described. Then, ELUs are used for activations to increase the speed of the training procedure. ELU converges to the maximum point at the end of the learning and validation procedures. The operation of ELU can be described as given in Equations 3, 4.As found in Equations 3, 4 max-pooling function has been explored from ELU operation. Where  the hyper-parameter is handled for the saturation of deconvolution layer inputs. The max-pooling modules are used with a stride rate of two and a 2\u2009\u00d7\u20092 window. These max-pooling parameters are generated in the encoder and then unpooled in the decoding module. This approach keeps edge information in the isolated nodules and reduces training variables. The Batch Normalization (BN) module tests each convolutional module. Batch normalization data are computed from the learning procedure used for testing and skip sampling.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29022, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "In DeepNet, a dropout method is used as an approximate inference to do probabilistic implications over the isolation process. The likelihood of dropout  is optimized here. In our intended structure, the normal likelihood of skipping a connection is fixed at 50% (i.e., ). The first encoding module is provided with the equivalent (nearby) decoding unit with red-green-blue color space. This is different from the other encoder/decoder networks, which generate an equal number for the channel and size attribute vector based on the inputs given to the encoding module. As a final point, dense filter vectors are transmitted for pixel-wise isolation. The segmentation procedure is achieved using the softmax layer.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29735, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The fitness function of the DeepNet model is defined by Equation 5, and i values are correlated with the max-pooling functionAs obtained in Equation 5 fitness function of the DeepNet model has been founded. Where N is the number of data samples and M is the number of the labeled class used in this study;  denotes the anticipated outcome vector;  is the measured output vector of the M -th class. The classification (softmax) operation is given in Equation 6.As demonstrated in Equation 6 softmax operation has been evaluated. Where  is cross-entropy loss. Now, the function  can be adapted by the weight penalty to add a  value to preserve the weights from getting larger in the fitness function of DSC, as given in Equation 7.As described in Equation 7 weights have been discussed.  denotes the anticipated outcome. Where  denotes the link weight;  are the numbers of layers and connections in each layer, respectively.  represent the number of data samples and M is the number of the labeled class,  denote the cross-entropy loss,  denote the number of outcomes  represent the cross-entropy loss. N, M represent the number of samples and labeled classes. CSA follows the segmentation method.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30931, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Cuckoo search algorithm (CSA)", "sentences": [], "annotations": [], "relations": []}, {"offset": 30961, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "This method spreads the cuckoo generation function, reducing the situation's complications. During the search, the DeepNet method can reach many nests. A new solution has been found, which consists of discovering the position of the cuckoo egg. The following is an outline of the stages involved in the search operation. A cuckoo bird will lay one egg at a time in a nest that has been selected at random. The parasite nests did not change in any way, and the number of eggs inside them continued to rise until they reached their maximum capacity. When the cuckoo's egg is discovered, the bird hosting seems to have the option of either discarding the egg or destroying the nest and starting over with a fresh one. Figure\u00a05 expresses the cuckoo search algorithm (CSA).", "sentences": [], "annotations": [], "relations": []}, {"offset": 31731, "infons": {"file": "fmedt-05-1157919-g005.jpg", "id": "F5", "section_type": "FIG", "type": "fig_caption"}, "text": "Path diagram of cuckoo search algorithm (CSA).", "sentences": [], "annotations": [], "relations": []}, {"offset": 31778, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The CSA has been enhanced as a result of the Levy flight theory. CSA determines a suitable threshold for removing the lung nodule.The method that has been suggested for making the optimal selection (Algorithm 1) makes use of the analogy that is presented below:", "sentences": [], "annotations": [], "relations": []}, {"offset": 32040, "infons": {"file": "A1.xml", "id": "A1", "section_type": "TABLE", "type": "table_caption"}, "text": "CSA to divide up cancer", "sentences": [], "annotations": [], "relations": []}, {"offset": 32064, "infons": {"file": "A1.xml", "id": "A1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/></colgroup><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Input: <italic>m</italic>, <inline-formula><mml:math id=\"IM27\" overflow=\"scroll\"><mml:mi>Q</mml:mi><mml:mi mathvariant=\"normal\">&amp;</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Output: <inline-formula><mml:math id=\"IM28\" overflow=\"scroll\"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">If <inline-formula><mml:math id=\"IM29\" overflow=\"scroll\"><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>;</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Return E</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Else</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"IM30\" overflow=\"scroll\"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>\n</inline-formula>\n</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">End</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">For (<italic>j</italic>\u2009=\u20090; <italic>j</italic>\u2009&gt;\u20090; <italic>j</italic>++)</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"IM31\" overflow=\"scroll\"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi mathvariant=\"normal\">_</mml:mi><mml:msup><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>\n</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">End</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">If <inline-formula><mml:math id=\"IM32\" overflow=\"scroll\"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>\u03c3</mml:mi><mml:mo>\u2295</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">le</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant=\"normal\">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">\u2032</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>\u03b4</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Return <inline-formula><mml:math id=\"IM33\" overflow=\"scroll\"><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>\u03b4</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo>\u2212</mml:mo><mml:mi>\u03b4</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Else</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"IM34\" overflow=\"scroll\"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>\n</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Choose <inline-formula><mml:math id=\"IM35\" overflow=\"scroll\"><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">End</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">If <inline-formula><mml:math id=\"IM36\" overflow=\"scroll\"><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Then</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">replace <inline-formula><mml:math id=\"IM37\" overflow=\"scroll\"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula></td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">else</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"IM38\" overflow=\"scroll\"><mml:msubsup><mml:mi>m</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math>\n</inline-formula>\n</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">End</td></tr></tbody></table>\n"}, "text": "Input: m, \t \tOutput: \t \tIf ;\t \tReturn E\t \tElse\t \t\t \tEnd\t \tFor (j\u2009=\u20090; j\u2009>\u20090; j++)\t \t\t \tEnd\t \tIf \t \tReturn \t \tElse\t \t\t \tChoose \t \tEnd\t \tIf \t \tThen\t \treplace \t \telse\t \t\t \tEnd\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 32248, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The new answer, which illustrates the subcategory of thresholds, is represented by an egg of a cuckoo. Additionally, CSA is exploited in the process of segmenting the lung nodule. The quality of the eggs each hosting nest produces is either 0 or 1, representing the threshold partition used in the segmentation technique. Pa is the likelihood that a cuckoo egg will be found by the bird serving as the host.  has a threshold that has been established. This shows eliminating the boundary groupings that are least important to the overall study and deleting these threshold levels from further consideration.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32856, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The initial stage of the CSA algorithm is initiated with the input values m, Q&N. m is the number of hosting nests, Q is the likelihood of discovery of an alien, and N represents the highest amount of iterations. The generation of the initial host is given as . Then the for loop is executed for the j\u2009=\u20090 conditions. The evaluation of E is obtained from . A new resolution is created for the resolution  Where the symbol  is entry-wise multiplication,  indicates the step size, Lev . Evaluate E . Select a nest  Randomly. If  then replace  with . Confiscate a worse nest with q. Create a new nest using Levy flights. Retain the best resolutions. From the segmented section of the cancer cell, the features are extracted in the following section.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33607, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Feature extraction", "sentences": [], "annotations": [], "relations": []}, {"offset": 33626, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The original purpose of the LBP operator intended pattern detection. The operators label each pixel by thresholding the picture using the average pixel value and obtaining the resulting binary number. Attributes extraction consists of the following steps in the paragraphs below.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33906, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "At initial stage the value of \u201c1\u201d is assigned if the central pixel's value exceeds the adjacent pixel's and \u201c0\u201d otherwise.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34037, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Then the binary integer may be generated by comparing each pixel with other pixels.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34121, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Final stage is the histogram calculation for the whole cell", "sentences": [], "annotations": [], "relations": []}, {"offset": 34181, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "The phrase allows one to determine an LBP value. Where v stands for uniform pattern and y and x are two numbers that have .", "sentences": [], "annotations": [], "relations": []}, {"offset": 34305, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Identify the surrounding area.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34336, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Make a grid on the window viewing through and combine all the pixels in a cell with their neighbors. As shown in Equation 8, the texture image has been deliberated. Where  is the central pixel's grey value,  is the modulation index of the position pixels , and n is the pixel in the picture where O is greater than zero forming a locally-oriented neighborhood set.  are the numbers that covers an LBP value. The textured image is defined by creating a  histogram after distinguishing each pixel in a photograph. The extracted characters are given to the classification stage for accurate classification.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34940, "infons": {"section_type": "INTRO", "type": "title_2"}, "text": "Classification of lung images", "sentences": [], "annotations": [], "relations": []}, {"offset": 34970, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "DeepNet model recognition of lung cancer is the capstone of this study. A deep learning neural network does not manually extract the features for training; it may be used to prepare images for the classification process. Instead, deep learning uses the collected lung CT image or separated image to recognize the edges present in the picture and significant characteristics employed in the neural network training using a huge quantity of data. Due to the large dimensionality of the information, the network typically employs 150 hidden layers.", "sentences": [], "annotations": [], "relations": []}, {"offset": 35516, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "To forecast the manually extracted characteristics associated with lung cancer, the learned features are maintained in a database as a template. The procedure of classification involves self-training and categorization of the spectral characteristics that were manually collected. The network examines the lung characteristics and uses the extracted features in a manner consistent with the unstructured classification stage to the hidden node it has included. Particular network and weighting values are used to generalize the network and reduce the number of unnecessary characteristics required for classification. Since the network bias value is set to 1 during training, input nodes with moreover one of the total number of training features in the feature space are used. The relative importance of the connections in a network and the error reduction is represented hereIn Equation 9, O stands for the radius generalization, and  represents the hammering strength. The weight value from the hidden to output nodes is then specified as either , indicating that the feature is extracted from the lungs and is utilized to predict whether or not the features are a cancerous or noncancerous stage. At last, the value is disguised as output. The results of the training procedure are then characterized as follows,The obtained value has been compared to characteristics gathered and taught using deep learning for cancer classification. The classification of cancer cells is based on  Characters as 0 and 1. The double-time comparison procedure increases identification accuracy and effectively decreases the false classification rate. The complete flow chart of the proposed system is shown in Figure\u00a06. The input image is pre-processed, segmented and filtered. The filtered sample is given to Cuckoo search algorithm which is implemented in the testing phase. The training phase includes the DeepNet which is used for classifying the data sample as normal and abnormal. The treatment is implemented according to classification stages.", "sentences": [], "annotations": [], "relations": []}, {"offset": 37556, "infons": {"file": "fmedt-05-1157919-g006.jpg", "id": "F6", "section_type": "FIG", "type": "fig_caption"}, "text": "The flow chart of DeepNet model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 37589, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Empirical analysis", "sentences": [], "annotations": [], "relations": []}, {"offset": 37608, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To demonstrate the effectiveness of DeepNet, a comprehensive experimental study is carried out on a 3.6\u2005GHz Intel Core i7-4790 processor with 16GB memory and a Windows 10 operating system. The effectiveness of the DeepNet model is assessed by relating the results with six related classification models, viz. 3D LungNet, 3D U-Net, mU-Net, 3D-Segnet, MAU-Net, ResNet. All these networks including our DeepNet exploit deep learning algorithms for segmentation and are trained using the same training configuration. The comparison is illustrated in Root Mean Square error, F1-score Ratio, Efficiency ratio.", "sentences": [], "annotations": [], "relations": []}, {"offset": 38214, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Dataset acquisition", "sentences": [], "annotations": [], "relations": []}, {"offset": 38234, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To evaluate the effectiveness of any DL algorithm, we require a huge database that produces a better solution. This work uses a corpus of labeled lung CT scans from the TCIA database collected from the National Cancer Institute of Cancer Analysis Consortium Lung Cohort. The collected samples are related to proteomic, genomic, and medical statistics. The gathered scans are kept in the form of DICOM (digital imaging and communications in medicine) with definite labels including imaging, gender, date of birth, study dates, etc. 5,043 scans are used for segmenting lung cancer from various cases with 48 sequences. In this study, we purposefully divide the available database into 70% of instances (i.e., 3,530 images) for training and 30% (i.e., 1,513 images) of instances for testing.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39023, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Moreover, since we employed 10-fold cross-validation (10-FCV), the total data samples are divided into 10 parts (each of 10%). Now, one fold (10%) is applied for testing, while the residual samples (90%) are split for validation and training. Using 10-FCV guarantees that each sample in the data gets to be in a test. 10-FCV is considered for five samples, and the confusion matrix for each sample is illustrated in Figure\u00a07.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39450, "infons": {"file": "fmedt-05-1157919-g007.jpg", "id": "F7", "section_type": "FIG", "type": "fig_caption"}, "text": "(A) the training phase of DeepNet. (B) The Testing Phase of DeepNet.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39519, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Dataset Description: DeepNet model aimed to develop a machine learning and deep learning (CNN) system to identify lung cancer. The main aim is to gather all information for quick and straightforward picture classification.", "sentences": [], "annotations": [], "relations": []}, {"offset": 39742, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Performance measure", "sentences": [], "annotations": [], "relations": []}, {"offset": 39762, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The performance measurement of DeepNet in the form of a confusion matrix is shown in Figures\u00a07A,B. The calculation for both the testing and training phases of the confusion matrix is calculated for each samples. The sample data description is divided into 10 parts as five samples.", "sentences": [], "annotations": [], "relations": []}, {"offset": 40045, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The performance of the DeepNet model is quantitatively assessed using some selected evaluation metrics like the volume error (VE), dice similarity coefficient (DSC), Jaccard similarity index, and average processing time. These metrics are computed by calculating the variation between the segmentation outcomes and a manually marked ground truth. The volume error is calculated using Equation 11.where G is the ground truth (i.e., standard gold image) and S is the segmentation result obtained by DeepNet. For a medical practice,  is more likely acceptable. The Dice similarity score is generally employed to define the representation of the segmentation process on the input CT scan. DSC is a similarity index between two pixels. It refers to the fitness degree between the original and segmented images. The value of DSC is always in [0, 1] and is measured using Equation 12.The Jaccard similarity score (JSS) is an assessment measure employed to assess the effectiveness of any isolation method. Given a dataset, the JSS measure provides the similarity between the target scan and the ground truth. It is defined by Equation 10.As explored in Equation 10 JSS has been demonstrated. This study considers the average processing time as the performance measure.", "sentences": [], "annotations": [], "relations": []}, {"offset": 41307, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Empirical results", "sentences": [], "annotations": [], "relations": []}, {"offset": 41325, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The DeepNet segmentation model is implemented using the deep learning toolbox in MATLAB R2018b software. The comprehensive outcomes obtained from the DeepNet segmentation model are given in Table\u00a01. To realize more precise solutions, the 10-FCV technique is used. Consequently, the whole database is divided into 10 parts. For each trial, one part is applied for testing, and the other parts are used for training the classifier. Now the mean value of all ten tests is considered for evaluation. A comprehensive analysis of our results discloses the potential and weakness of our DeepNet approach. Generally, irrespective of the dimension of the standard gold image, the DeepNet model segments the nodules effectively. Figure\u00a08 shows sample input images used for experimentation, segmented images obtained by DeepNet, and their corresponding ground truth images. Though the nodules are in different random localities within the lung and appear in various sizes, the segmented nodules coincide almost perfectly.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42338, "infons": {"file": "T1.xml", "id": "T1", "section_type": "TABLE", "type": "table_caption"}, "text": "The results obtained by DeepNet from the TCIA dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 42393, "infons": {"file": "T1.xml", "id": "T1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Criteria</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">VE (%)</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">DSC</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">JSS</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Average processing time (s)</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3D Lung Net</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.871\u2009\u00b1\u20090.047</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.656\u2009\u00b1\u20090.154</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.687\u2009\u00b1\u20090.014</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.457\u2009\u00b1\u20090.002</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3D Seg Net</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.671\u2009\u00b1\u20090.041</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.786\u2009\u00b1\u20090.014</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.660\u2009\u00b1\u20090.012</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.083\u2009\u00b1\u20090.001</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3D U-Net</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.325\u2009\u00b1\u20090.059</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.843\u2009\u00b1\u20090.179</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.711\u2009\u00b1\u20090.011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.160\u2009\u00b1\u20090.001</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">mU-Net</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.347\u2009\u00b1\u20090.049</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.854\u2009\u00b1\u20090.183</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.758\u2009\u00b1\u20090.008</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.178\u2009\u00b1\u20090.002</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">MAU-Net</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.145\u2009\u00b1\u20090.015</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.866\u2009\u00b1\u20090.133</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.804\u2009\u00b1\u20090.010</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.199\u2009\u00b1\u20090.003</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">ResNet</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.345\u2009\u00b1\u20090.016</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.953\u2009\u00b1\u20090.014</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.814\u2009\u00b1\u20090.009</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.168\u2009\u00b1\u20090.004</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">DeepNet</td><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mean\u2009\u00b1\u2009std</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.962\u2009\u00b1\u20090.023</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.968\u2009\u00b1\u20090.011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.856\u2009\u00b1\u20090.011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.045\u2009\u00b1\u20090.005</td></tr></tbody></table>\n"}, "text": "Model\tCriteria\tVE (%)\tDSC\tJSS\tAverage processing time (s)\t \t3D Lung Net\tMean\u2009\u00b1\u2009std\t4.871\u2009\u00b1\u20090.047\t0.656\u2009\u00b1\u20090.154\t0.687\u2009\u00b1\u20090.014\t1.457\u2009\u00b1\u20090.002\t \t3D Seg Net\tMean\u2009\u00b1\u2009std\t3.671\u2009\u00b1\u20090.041\t0.786\u2009\u00b1\u20090.014\t0.660\u2009\u00b1\u20090.012\t0.083\u2009\u00b1\u20090.001\t \t3D U-Net\tMean\u2009\u00b1\u2009std\t2.325\u2009\u00b1\u20090.059\t0.843\u2009\u00b1\u20090.179\t0.711\u2009\u00b1\u20090.011\t0.160\u2009\u00b1\u20090.001\t \tmU-Net\tMean\u2009\u00b1\u2009std\t1.347\u2009\u00b1\u20090.049\t0.854\u2009\u00b1\u20090.183\t0.758\u2009\u00b1\u20090.008\t0.178\u2009\u00b1\u20090.002\t \tMAU-Net\tMean\u2009\u00b1\u2009std\t1.145\u2009\u00b1\u20090.015\t0.866\u2009\u00b1\u20090.133\t0.804\u2009\u00b1\u20090.010\t0.199\u2009\u00b1\u20090.003\t \tResNet\tMean\u2009\u00b1\u2009std\t0.345\u2009\u00b1\u20090.016\t0.953\u2009\u00b1\u20090.014\t0.814\u2009\u00b1\u20090.009\t1.168\u2009\u00b1\u20090.004\t \tDeepNet\tMean\u2009\u00b1\u2009std\t0.962\u2009\u00b1\u20090.023\t0.968\u2009\u00b1\u20090.011\t0.856\u2009\u00b1\u20090.011\t0.045\u2009\u00b1\u20090.005\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 43174, "infons": {"file": "fmedt-05-1157919-g008.jpg", "id": "F8", "section_type": "FIG", "type": "fig_caption"}, "text": "Segmentation results.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43196, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In classification process  denotes the true positive,  indicates the true negative,  express the false positive,  is examines the false negative.", "sentences": [], "annotations": [], "relations": []}, {"offset": 43342, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Table\u00a01 shows the isolation results obtained by the DeepNet model. The conventional 3D LungNet architecture provides 4.871\u2009\u00b1\u20090.047% volume error and 0.656\u2009\u00b1\u20090.154 DSC. 3D LungNet can exploit the 3D statistics present within CT scan volumes effectively. Therefore, this model can achieve a 0.687\u2009\u00b1\u20090.014 dice similarity coefficient and 1.457\u2009\u00b1\u20090.002 Jaccard similarity index. The average computational time per case of the 3D LungNet model is 1.457\u2009\u00b1\u20090.002\u2005s. SegNet is more effective in terms of performance measures since it keeps the upsampling parameters of the attribute vector and employs them in its decoding module to realize better results than the 3D LungNet model. It provides a 3.671\u2009\u00b1\u20090.041 mean volume error, 0.786\u2009\u00b1\u20090.014 dice similarity index, and 0.660\u2009\u00b1\u20090.012 Jaccard similarity score. This model takes 0.083\u2009\u00b1\u20090.001\u2005s for segmenting nodules.", "sentences": [], "annotations": [], "relations": []}, {"offset": 44252, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The 3D U-Net approach achieves improved results compared to 3D LungNet and 3D SegNet models by concurrently applying the concept of context data and global position. Also, it guarantees the preservation of the entire quality of the input scans. Hence it achieves a relatively lower volume error (2.325\u2009\u00b1\u20090.059) and a higher dice coefficient (0.843\u2009\u00b1\u20090.179) as well as a higher Jaccard similarity index (0.711\u2009\u00b1\u20090.011). Moreover, it takes reduced mean processing time per case (0.160\u2009\u00b1\u20090.001\u2005s). Conversely, higher-level attributes selected by this model often do not comprise sufficient higher-resolution boundary statistics of the input, leading to increased indecision where higher-resolution boundaries mainly impact the final decisions including the lung nodule segmentation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45054, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The modified U-Net contains a residual block with de-convolutional and activation functions to the dropout unit to avoid distorted attributes' repetition. For small object inputs, attributes in the dropout units are not integrated with attributes in the residual block. Also, the recommended model has supplementary convolutional modules in the dropping out of a unit to select higher-resolution attributes of minor object inputs and higher-level attributes of higher-level boundary data of big object inputs. Hence, this network provides better performance for lung nodule segmentation in terms of dice similarity index (0.854\u2009\u00b1\u20090.183), the volume of error (1.347\u2009\u00b1\u20090.049%), and the Jaccard coefficient (0.758\u2009\u00b1\u20090.008). For effective nodule segmentation it consumes 0.178\u2009\u00b1\u20090.002\u2005s for each sample.", "sentences": [], "annotations": [], "relations": []}, {"offset": 45876, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "By applying a 3D encoder/decoder-based CNN structure, MAU-Net realizes good performance for precisely isolating lung nodules from volumetric CT images. This model achieves performances of 1.145\u2009\u00b1\u20090.015%, 0.866\u2009\u00b1\u20090.133, 0.804\u2009\u00b1\u20090.010, and 0.199\u2009\u00b1\u20090.003 in volume error, dice coefficient, Jaccard similarity index, and average processing time, respectively. By applying the concept of dropout connections ResNet delivers 0.345\u2009\u00b1\u20090.016% of volume error, 0.953\u2009\u00b1\u20090.014 of dice similarity index, 0.814\u2009\u00b1\u20090.009 of Jaccard score, and 1.168\u2009\u00b1\u20090.004\u2005s of average processing.", "sentences": [], "annotations": [], "relations": []}, {"offset": 46484, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The DeepNet model outperforms all other network models in terms of metrics selected for performance evaluation. DeepNet emphasizes that the attribute pooling function of DeepNet guarantees a less computationally expensive structure related to other segmentation models. It achieves performance measures of 0.962\u2009\u00b1\u20090.023, 0.968\u2009\u00b1\u20090.011, 0.856\u2009\u00b1\u20090.011, and 0.045\u2009\u00b1\u20090.005 in volume error, dice coefficient, Jaccard similarity index, and average processing time, respectively shown in Figures\u00a09, 10.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47001, "infons": {"file": "fmedt-05-1157919-g009.jpg", "id": "F9", "section_type": "FIG", "type": "fig_caption"}, "text": "Performance of various segmentation models in terms of mean values.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47069, "infons": {"file": "fmedt-05-1157919-g010.jpg", "id": "F10", "section_type": "FIG", "type": "fig_caption"}, "text": "Performance of various segmentation models in terms of mean values.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47137, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Accuracy, loss, and computation time are all useful effective assessment characteristics that may be used to assess a medical image's efficacy. Regarding measuring the model's effectiveness, accuracy is a crucial metric. It returns the fraction of an image's pixels that have been properly labeled.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47436, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The neural network's error, or Loss, may be anticipated using the Loss function's computations. It's a key network performance metric. Computational Time is the amount of time it takes for a process to compute or carry out its actions. Time spent processing is reduced if the process is straightforward, whereas more time is needed for complicated procedures.", "sentences": [], "annotations": [], "relations": []}, {"offset": 47796, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Accuracy ratio (%) and loss curve", "sentences": [], "annotations": [], "relations": []}, {"offset": 47830, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "CT scans in DICOM format represent a total of 1,018 instances. Since it is challenging to train high-size pictures in DeepNet, the images were preprocessed to reduce the size to 512\u2009\u00d7\u2009512. Table\u00a02 and Figures\u00a011A,B show the epoch, loss, and accuracy results from the CT image dataset experiments.", "sentences": [], "annotations": [], "relations": []}, {"offset": 48134, "infons": {"file": "T2.xml", "id": "T2", "section_type": "TABLE", "type": "table_caption"}, "text": "Epoch, verses loss, and accuracy results.", "sentences": [], "annotations": [], "relations": []}, {"offset": 48176, "infons": {"file": "T2.xml", "id": "T2", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Epoch</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Loss</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Accuracy (%)</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">13</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.032</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.3</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">25</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.012</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.9</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">38</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0045</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">50</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0052</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">63</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0033</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">75</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0041</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">88</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0025</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">100</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0019</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">113</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0026</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">125</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0023</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">138</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0015</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">150</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">163</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0016</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.6</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">175</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0008</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">96</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">188</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0056</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">97</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">200</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0031</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">213</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">225</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.7</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">238</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">250</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0008</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.3</td></tr></tbody></table>\n"}, "text": "Epoch\tLoss\tAccuracy (%)\t \t1\t0.71\t76.5\t \t13\t0.032\t85.3\t \t25\t0.012\t88.9\t \t38\t0.0045\t90.1\t \t50\t0.0052\t90.5\t \t63\t0.0033\t91.5\t \t75\t0.0041\t92\t \t88\t0.0025\t92.5\t \t100\t0.0019\t93\t \t113\t0.0026\t93.5\t \t125\t0.0023\t94\t \t138\t0.0015\t94.5\t \t150\t0.0013\t95\t \t163\t0.0016\t95.6\t \t175\t0.0008\t96\t \t188\t0.0056\t97\t \t200\t0.0031\t98\t \t213\t0.0013\t98.5\t \t225\t0.0013\t98.7\t \t238\t0.0013\t99.1\t \t250\t0.0008\t99.3\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 48554, "infons": {"file": "fmedt-05-1157919-g011.jpg", "id": "F11", "section_type": "FIG", "type": "fig_caption"}, "text": "(A) Epoch vs. accuracy and (B) epoch vs. loss. (C) Iteration vs. Loss and (D) iteration vs. accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 48656, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The loss and accuracy of the DeepNet for each epoch is shown in Figures\u00a011A,B with the loss of 0.0008 and accuracy of 99.3. As shown in Equation 14, the classification accuracy rate is calculatedThe total number of observations (m) is based on the particular number of accurate classifications (s). Training and testing images are separated into distinct categories in evaluating the network's efficiency in distinguishing cancer and non-cancerous pictures. The photos from the provided dataset are fed into the DeepNet model during training, with 90% of the images serving as examples. Table\u00a03 and Figures\u00a011C,D provide iteration, loss, and accuracy statistics from experiments performed on the CT image dataset.", "sentences": [], "annotations": [], "relations": []}, {"offset": 49373, "infons": {"file": "T3.xml", "id": "T3", "section_type": "TABLE", "type": "table_caption"}, "text": "Iteration vs. Loss and Accuracy Outcomes.", "sentences": [], "annotations": [], "relations": []}, {"offset": 49415, "infons": {"file": "T3.xml", "id": "T3", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Iteration</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Loss</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Accuracy (%)</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">50</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.022</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.3</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">100</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.021</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.9</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">150</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0032</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.2</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">200</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0044</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.6</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">250</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.3</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">300</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0021</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">350</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0055</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">400</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0029</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">450</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0024</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">500</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0026</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.8</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">550</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.9</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">600</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0012</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">650</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">700</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0007</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.8</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">750</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0036</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">96</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">800</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0031</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">96.4</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">850</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0043</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">900</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0033</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.7</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">950</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0023</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">1,000</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0009</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.5</td></tr></tbody></table>\n"}, "text": "Iteration\tLoss\tAccuracy (%)\t \t1\t0.71\t73.5\t \t50\t0.022\t82.3\t \t100\t0.021\t87.9\t \t150\t0.0032\t90.2\t \t200\t0.0044\t91.6\t \t250\t0.0011\t92.3\t \t300\t0.0021\t92\t \t350\t0.0055\t92.5\t \t400\t0.0029\t94\t \t450\t0.0024\t94.5\t \t500\t0.0026\t94.8\t \t550\t0.0011\t94.9\t \t600\t0.0012\t95\t \t650\t0.0013\t95.1\t \t700\t0.0007\t95.8\t \t750\t0.0036\t96\t \t800\t0.0031\t96.4\t \t850\t0.0043\t97.5\t \t900\t0.0033\t97.7\t \t950\t0.0023\t98.1\t \t1,000\t0.0009\t99.5\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 49811, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The loss and accuracy of the DeepNet for each Iteration is shown in Figures\u00a011C,D with the loss of 0.0007 and accuracy of 99.5. After the training phase is complete, 10% of the same dataset's testing images are used to assess the model's performance. Here, the samples of photos are fed into a network model for cancer/non-cancer image classification.", "sentences": [], "annotations": [], "relations": []}, {"offset": 50164, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Computation time (%)", "sentences": [], "annotations": [], "relations": []}, {"offset": 50185, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The experimental study on the CT image collection includes measurements of computation time, losses, and accuracy. In addition to explaining lung cancer, the DeepNet model is evaluated using the Kaggle dataset for training and testing reasons for cancer segmentation and classification in MATLAB. In this case, samples of photographs are fed into a network model trained to detect cancer in pictures (both malignant and benign). Table\u00a04 and Figures\u00a012A,B show the outcomes of experimental research on a CT scan image collection for computation time, loss, and accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 50757, "infons": {"file": "T4.xml", "id": "T4", "section_type": "TABLE", "type": "table_caption"}, "text": "Computation time verses loss and accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 50800, "infons": {"file": "T4.xml", "id": "T4", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Computation Time (Sec)</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Loss</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">Accuracy (%)</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">30</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">1,248</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.022</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.3</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">2,626</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.021</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.9</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3,908</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0032</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.2</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">4,930</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0044</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.6</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">6,075</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.3</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">7,142</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0021</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">8,343</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0055</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">9,689</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0029</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">10,214</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0024</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">11,692</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0026</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.8</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">12,242</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.9</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">13,743</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0012</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">14,357</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0013</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">15,820</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0007</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.8</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">16,818</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0036</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">96</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">17,965</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0031</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">96.4</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">18,745</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0043</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">19,810</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0033</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.7</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">20,835</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0023</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.1</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">21,870</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0009</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.5</td></tr></tbody></table>\n"}, "text": "Computation Time (Sec)\tLoss\tAccuracy (%)\t \t30\t0.71\t73.5\t \t1,248\t0.022\t82.3\t \t2,626\t0.021\t87.9\t \t3,908\t0.0032\t90.2\t \t4,930\t0.0044\t91.6\t \t6,075\t0.0011\t92.3\t \t7,142\t0.0021\t92\t \t8,343\t0.0055\t92.5\t \t9,689\t0.0029\t94\t \t10,214\t0.0024\t94.5\t \t11,692\t0.0026\t94.8\t \t12,242\t0.0011\t94.9\t \t13,743\t0.0012\t95\t \t14,357\t0.0013\t95.1\t \t15,820\t0.0007\t95.8\t \t16,818\t0.0036\t96\t \t17,965\t0.0031\t96.4\t \t18,745\t0.0043\t97.5\t \t19,810\t0.0033\t97.7\t \t20,835\t0.0023\t98.1\t \t21,870\t0.0009\t99.5\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 51261, "infons": {"file": "fmedt-05-1157919-g012.jpg", "id": "F12", "section_type": "FIG", "type": "fig_caption"}, "text": "(A) computation time vs. loss and 1(B) computation time vs. accuracy.", "sentences": [], "annotations": [], "relations": []}, {"offset": 51331, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The loss and accuracy of the DeepNet for Computation time is shown in Figures\u00a012A,B with the loss of 0.0007 and accuracy of 99.5. Compared to previous research articles, DeepNet's final result is the highest degree of accuracy obtained: 99.6% with a calculation duration of 45,141\u2005s on a single-CPU workstation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 51646, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Figure\u00a011 is a visual depiction of the accuracy percentages from the several studies included in the literature review. Figure\u00a012 compares the computation time of the existing method with DeepNet. This results in improved classification accuracy, less loss and computation time that exceeds that of 3D LungNet SegNet MAU-Net, ResNet-34 model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 51991, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Root mean square error (%)", "sentences": [], "annotations": [], "relations": []}, {"offset": 52018, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The author presented a technique to categorize lung nodules using CT scans. The lung segmentation would take place via background subtraction and a cuckoo search algorithm, and the image characteristics would then be retrieved. The collected characteristics are given for different classifiers, such as DeepNet. Afterward, the classifier determines and categorizes the photos as either benign or malignant. To locate lung nodules, the author suggests using the DeepNet classifier, which has an accuracy rate of around 99.6%. Figure\u00a013 deliberates the RMSE Rate with low error based on the cuckoo search algorithm (CSA).", "sentences": [], "annotations": [], "relations": []}, {"offset": 52639, "infons": {"file": "fmedt-05-1157919-g013.jpg", "id": "F13", "section_type": "FIG", "type": "fig_caption"}, "text": "RMSE rate.", "sentences": [], "annotations": [], "relations": []}, {"offset": 52650, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "This value is scaled such that it would fall somewhere between +1 (where the left lung is ventilated; ) and 1 (where the right lung is ventilated; ). Acute respiratory injury and baseline values for the imbalance coefficient  were determined for each pixel within every location. This paper presents a model for classifying lung cancer nodules from CT scans. The model uses many classifiers to identify cancer, which increases the model's efficiency and, as a result, lowers the error rate.", "sentences": [], "annotations": [], "relations": []}, {"offset": 53141, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "F1-score ratio (%)", "sentences": [], "annotations": [], "relations": []}, {"offset": 53160, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Fusing deep learning concepts for lung nodule categorization is proposed as a solution to the issues of the field, such as the lengthy and difficult classification detection phase, low accuracy rate, and high false positive rate. The model's architecture is a convolutional network model with 50 layers, and it reconstructs the average global pooling layer, the FC layer, and the classification algorithm. As the abovementioned equation shows the F1-score value and Figure\u00a014 when compared to other existing methods, DeepNet achieves a high value with the help of CSA.  represent the total number of positive values,  denote the false positive values,  represent the false negative values. The experimental findings on the Kaggle dataset show that the DeepNet model outperforms the published results of all the other methods, including the neural network models and a typical ML algorithm, in terms of accuracy and F1 score. Comparing the suggested technique to those of existing classifiers, it emerges with the highest scores in terms of both accuracy and F1-score.", "sentences": [], "annotations": [], "relations": []}, {"offset": 54229, "infons": {"file": "fmedt-05-1157919-g014.jpg", "id": "F14", "section_type": "FIG", "type": "fig_caption"}, "text": "F1-score ratio (%).", "sentences": [], "annotations": [], "relations": []}, {"offset": 54249, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Efficiency ratio (%)", "sentences": [], "annotations": [], "relations": []}, {"offset": 54270, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The patient's diagnosis is confirmed alongside the formulation of a suitable treatment strategy when the nodule information (density, shape, and texture traits) is analyzed for the potential presence of malignancy. The process of recognizing the nodules is a difficult one. Inadequate professional expertise, distractions, or exhaustion when recording scans, among other factors, may weaken nodule detection, contributing to mischaracterizations of false positives using the available data. The DeepNet model needs to be highly sensitive while having a low number of false positives, a low installation cost, a low cost of maintenance tasks, vulnerability management assurance, and high levels of automation to improve efficiency. The lung CT images are obtained from the dataset. The image noise is removed using an approach called weighted histogram equalization.", "sentences": [], "annotations": [], "relations": []}, {"offset": 55136, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "As illustrated in Figure\u00a015, efficiency ratios are compared with the existing methods. This successfully removed the noise from the image, which improved the image quality. The cuckoo search algorithm is utilized to segregate the affected region. Several spectral characteristics may be extracted from the compressed area. These are investigated using a DeepNet to diagnose lung cancer.", "sentences": [], "annotations": [], "relations": []}, {"offset": 55524, "infons": {"file": "fmedt-05-1157919-g015.jpg", "id": "F15", "section_type": "FIG", "type": "fig_caption"}, "text": "Efficiency ratio (%).", "sentences": [], "annotations": [], "relations": []}, {"offset": 55546, "infons": {"section_type": "CONCL", "type": "title_1"}, "text": "Conclusion", "sentences": [], "annotations": [], "relations": []}, {"offset": 55557, "infons": {"section_type": "CONCL", "type": "paragraph"}, "text": "Lung cancer is the deadliest type of cancer. Developing an automatic and reliable system to segment lung nodules from a CT image is a very practical tool in the healthcare industry. The DeepNet model detects and separates lung nodules in CT scans. The DeepNet model employs an encoder/decoder network to accomplish pixel-wise picture segmentation. With the help of 16 upsampling and deconvolution modules, the decoder network improves upon the output of an encoder network that uses a Visual Geometry Group (VGG-19) model as its foundational design. The structural design of the decoder can be trained to produce outputs of arbitrary resolution, depending on the size of the input scans. The encoder's fuzzy data is mapped and upsampled by the decoder's network. As a result, the network reuses the pooling indices of the encoder for segmentation, drastically reducing the total number of variables required for training. The cuckoo search algorithm is used to find the most relevant features. The DeepNet model is evaluated based on the real-world database known as The Cancer Imaging Archive (TCIA) dataset. It shows its efficacy by comparing its representation to other contemporary segmentation methods along a few key performance metrics. The empirical analysis shows that DeepNet significantly outperforms other prevalent segmentation algorithms with 0.962\u2009\u00b1\u20090.023% of volume error, 0.968\u2009\u00b1\u20090.011 of dice similarity coefficient, 0.856\u2009\u00b1\u20090.011 of Jaccard similarity index, and 0.045\u2009\u00b1\u20090.005\u2005s average processing time. The accuracy ratio of our prosed method is 99.5%, 99.3%, and the loss is 0.008, 0.009 verses of epoch, iteration, and computation time. The overall efficiency ratio of our method is 98.7%, the F1-score ratio of 96.2% and the RMSE value is 0.0016 compared to other existing methods. Because of GPU memory space limitations, the pictures of this data were reduced to  pixels before our training step. In the future, this method would like to figure out what causes lung cancer in people and use that information to predict cancer. The DeepNet has highest efficiency ratio, F1-score with average processing time, less error rate. F1-Score of DeepNet is high with less RMSE value and loss ratio. The DeepNet method is compared with the loss and accuracy in terms of epoch, iteration and computation time. The segmentation algorithm gives better result in the form of volume error, dice similarity coefficient.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58008, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Data availability statement", "sentences": [], "annotations": [], "relations": []}, {"offset": 58036, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The original contributions presented in the study are included in the article/Supplementary Material, further inquiries can be directed to the corresponding author.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58201, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Ethics Statement", "sentences": [], "annotations": [], "relations": []}, {"offset": 58218, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "Ethical review and approval was not required for this study in accordance with the national legislation and the institutional requirements.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58358, "infons": {"section_type": "AUTH_CONT", "type": "title_1"}, "text": "Author contributions", "sentences": [], "annotations": [], "relations": []}, {"offset": 58379, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "Conception and design of study: GM. Acquisition of data: BS. Analysis and/or interpretation of data: GM. All authors contributed to the article and approved the submitted version.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58559, "infons": {"section_type": "COMP_INT", "type": "title_1"}, "text": "Conflict of interest", "sentences": [], "annotations": [], "relations": []}, {"offset": 58580, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.", "sentences": [], "annotations": [], "relations": []}, {"offset": 58753, "infons": {"section_type": "COMP_INT", "type": "title_1"}, "text": "Publisher's note", "sentences": [], "annotations": [], "relations": []}, {"offset": 58770, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.", "sentences": [], "annotations": [], "relations": []}, {"offset": 59117, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 59128, "infons": {"comment": "2020", "fpage": "e191", "issue": "2", "lpage": "203", "name_0": "surname:Arbyn;given-names:M", "name_1": "surname:Weiderpass;given-names:E", "name_2": "surname:Bruni;given-names:L", "name_3": "surname:de Sanjos\u00e9;given-names:S", "name_4": "surname:Saraiya;given-names:M", "name_5": "surname:Ferlay;given-names:J", "pub-id_doi": "10.1016/S2214-109X(19)30482-6", "section_type": "REF", "source": "Lancet Glob Health", "type": "ref", "volume": "8", "year": "2018"}, "text": "Estimates of incidence and mortality of cervical cancer in 2018: a worldwide analysis", "sentences": [], "annotations": [], "relations": []}, {"offset": 59214, "infons": {"fpage": "iv1", "issue": "suppl. 4", "lpage": "21", "name_0": "surname:Postmus;given-names:PE", "name_1": "surname:Kerr;given-names:KM", "name_2": "surname:Oudkerk;given-names:M", "name_3": "surname:Senan;given-names:S", "name_4": "surname:Waller;given-names:DA", "name_5": "surname:Vansteenkiste;given-names:J", "pub-id_doi": "10.1093/annonc/mdx222", "pub-id_pmid": "28881918", "section_type": "REF", "source": "Ann Oncol", "type": "ref", "volume": "28", "year": "2017"}, "text": "Early and locally advanced non-small-cell lung cancer (NSCLC): ESMO clinical practice guidelines for diagnosis, treatment, and follow-up", "sentences": [], "annotations": [], "relations": []}, {"offset": 59351, "infons": {"fpage": "688", "issue": "12", "lpage": "99", "name_0": "surname:Jaffray;given-names:DA", "pub-id_doi": "10.1038/nrclinonc.2012.194", "pub-id_pmid": "23165124", "section_type": "REF", "source": "Nat Rev Clin Oncol", "type": "ref", "volume": "9", "year": "2012"}, "text": "Image-guided radiotherapy: from current concept to future perspectives", "sentences": [], "annotations": [], "relations": []}, {"offset": 59422, "infons": {"fpage": "3423", "issue": "1", "name_0": "surname:Primakov;given-names:SP", "name_1": "surname:Ibrahim;given-names:A", "name_2": "surname:van Timmeren;given-names:JE", "name_3": "surname:Wu;given-names:G", "name_4": "surname:Keek;given-names:SA", "name_5": "surname:Beuque;given-names:M", "pub-id_doi": "10.1038/s41467-022-30841-3", "pub-id_pmid": "35701415", "section_type": "REF", "source": "Nat Commun", "type": "ref", "volume": "13", "year": "2022"}, "text": "Automated detection and segmentation of non-small cell lung cancer computed tomography images", "sentences": [], "annotations": [], "relations": []}, {"offset": 59516, "infons": {"fpage": "234", "lpage": "41", "name_0": "surname:Ronneberger;given-names:O", "name_1": "surname:Fischer;given-names:P", "name_2": "surname:Brox;given-names:T", "section_type": "REF", "source": "Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, October 5\u20139, 2015, Proceedings, Part III 18", "type": "ref", "year": "2015"}, "text": "U-Net: convolutional networks for biomedical image segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 59580, "infons": {"fpage": "954", "issue": "6", "lpage": "61", "name_0": "surname:Ardila;given-names:D", "name_1": "surname:Kiraly;given-names:AP", "name_2": "surname:Bharadwaj;given-names:S", "name_3": "surname:Choi;given-names:B", "name_4": "surname:Reicher;given-names:JJ", "name_5": "surname:Peng;given-names:L", "pub-id_doi": "10.1038/s41591-019-0447-x", "pub-id_pmid": "31110349", "section_type": "REF", "source": "Nat Med", "type": "ref", "volume": "25", "year": "2019"}, "text": "End-to-end lung cancer screening with deep three-dimensional learning on low-dose chest computed tomography", "sentences": [], "annotations": [], "relations": []}, {"offset": 59688, "infons": {"fpage": "134", "issue": "1", "lpage": "44", "name_0": "surname:Jiang;given-names:J", "name_1": "surname:Hu;given-names:YC", "name_2": "surname:Liu;given-names:CJ", "name_3": "surname:Halpenny;given-names:D", "name_4": "surname:Hellmann;given-names:MD", "name_5": "surname:Deasy;given-names:JO", "pub-id_doi": "10.1109/TMI.2018.2857800", "pub-id_pmid": "30040632", "section_type": "REF", "source": "IEEE Trans Med Imaging", "type": "ref", "volume": "38", "year": "2018"}, "text": "Multiple resolution residually connected feature streams for automatic lung tumor segmentation from CT images", "sentences": [], "annotations": [], "relations": []}, {"offset": 59798, "infons": {"fpage": "757", "issue": "11", "name_0": "surname:Mackin;given-names:D", "name_1": "surname:Fave;given-names:X", "name_2": "surname:Zhang;given-names:L", "name_3": "surname:Fried;given-names:D", "name_4": "surname:Yang;given-names:J", "name_5": "surname:Taylor;given-names:B", "pub-id_doi": "10.1097/RLI.0000000000000180", "pub-id_pmid": "26115366", "section_type": "REF", "source": "Invest Radiol", "type": "ref", "volume": "50", "year": "2015"}, "text": "Measuring computed tomography scanner variability of radiomics features", "sentences": [], "annotations": [], "relations": []}, {"offset": 59870, "infons": {"fpage": "137", "issue": "1", "lpage": "78", "name_0": "surname:Asgari Taghanaki;given-names:S", "name_1": "surname:Abhishek;given-names:K", "name_2": "surname:Cohen;given-names:JP", "name_3": "surname:Cohen-Adad;given-names:J", "name_4": "surname:Hamarneh;given-names:G", "pub-id_doi": "10.1007/s10462-020-09854-1", "section_type": "REF", "source": "Artif Intell Rev", "type": "ref", "volume": "54", "year": "2021"}, "text": "Deep semantic segmentation of natural and medical images: a review", "sentences": [], "annotations": [], "relations": []}, {"offset": 59937, "infons": {"fpage": "177", "issue": "1", "lpage": "85", "name_0": "surname:Norman;given-names:B", "name_1": "surname:Pedoia;given-names:V", "name_2": "surname:Majumdar;given-names:S", "pub-id_doi": "10.1148/radiol.2018172322", "pub-id_pmid": "29584598", "section_type": "REF", "source": "Radiology", "type": "ref", "volume": "288", "year": "2018"}, "text": "Use of 2D U-Net convolutional neural networks for automated cartilage and meniscus segmentation of knee MR imaging data to determine relaxometry and morphometry", "sentences": [], "annotations": [], "relations": []}, {"offset": 60098, "infons": {"fpage": "203", "issue": "2", "lpage": "11", "name_0": "surname:Isensee;given-names:F", "name_1": "surname:Jaeger;given-names:PF", "name_2": "surname:Kohl;given-names:SAA", "name_3": "surname:Petersen;given-names:J", "name_4": "surname:Maier-Hein;given-names:KH", "pub-id_doi": "10.1038/s41592-020-01008-z", "pub-id_pmid": "33288961", "section_type": "REF", "source": "Nat Methods", "type": "ref", "volume": "18", "year": "2021"}, "text": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 60187, "infons": {"fpage": "699", "issue": "2", "lpage": "711", "name_0": "surname:Gu;given-names:R", "name_1": "surname:Wang;given-names:G", "name_2": "surname:Song;given-names:T", "name_3": "surname:Huang;given-names:R", "name_4": "surname:Aertsen;given-names:M", "name_5": "surname:Deprest;given-names:J", "pub-id_doi": "10.1109/TMI.2020.3035253", "section_type": "REF", "source": "IEEE Trans Med Imaging", "type": "ref", "volume": "40", "year": "2020"}, "text": "Ca-net: comprehensive attention convolutional neural networks for explainable medical image segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 60292, "infons": {"fpage": "1205", "lpage": "08", "name_0": "surname:Mukherjee;given-names:S", "name_1": "surname:Huang;given-names:X", "name_2": "surname:Bhagalia;given-names:RR", "section_type": "REF", "type": "ref", "year": "2017"}, "text": "Lung nodule segmentation using deep learned prior based graph cut", "sentences": [], "annotations": [], "relations": []}, {"offset": 60358, "infons": {"fpage": "1752", "lpage": "55", "name_0": "surname:Wang;given-names:S", "name_1": "surname:Zhou;given-names:M", "name_2": "surname:Gevaert;given-names:O", "name_3": "surname:Tang;given-names:Z", "name_4": "surname:Dong;given-names:D", "name_5": "surname:Liu;given-names:Z", "section_type": "REF", "type": "ref", "year": "2017"}, "text": "A multi-view deep convolutional neural networks for lung nodule segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 60435, "infons": {"fpage": "31", "lpage": "8", "name_0": "surname:Roy;given-names:R", "name_1": "surname:Chakraborti;given-names:T", "name_2": "surname:Chowdhury;given-names:AS", "pub-id_doi": "10.1016/j.patrec.2019.03.004", "section_type": "REF", "source": "Pattern Recognit Lett", "type": "ref", "volume": "123", "year": "2019"}, "text": "A deep learning-shape-driven level set synergism for pulmonary nodule segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 60518, "infons": {"fpage": "1348", "lpage": "52", "name_0": "surname:Hossain;given-names:S", "name_1": "surname:Najeeb;given-names:S", "name_2": "surname:Shahriyar;given-names:A", "name_3": "surname:Abdullah;given-names:ZR", "name_4": "surname:Haque;given-names:MA", "section_type": "REF", "type": "ref", "year": "2019"}, "text": "A pipeline for lung tumor detection and segmentation from CT scans using dilated convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 60629, "infons": {"fpage": "2481", "issue": "12", "lpage": "95", "name_0": "surname:Badrinarayanan;given-names:V", "name_1": "surname:Kendall;given-names:A", "name_2": "surname:Cipolla;given-names:R", "pub-id_doi": "10.1109/TPAMI.2016.2644615", "pub-id_pmid": "28060704", "section_type": "REF", "source": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "type": "ref", "volume": "39", "year": "2017"}, "text": "Segnet: a deep convolutional encoder-decoder architecture for image segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 60710, "infons": {"fpage": "543", "lpage": "52", "name_0": "surname:Chen;given-names:W", "name_1": "surname:Yang;given-names:F", "name_2": "surname:Zhang;given-names:X", "name_3": "surname:Xu;given-names:X", "name_4": "surname:Qiao;given-names:X", "pub-id_doi": "10.1016/j.procs.2021.08.056", "section_type": "REF", "source": "Procedia Comput Sci", "type": "ref", "volume": "192", "year": "2021"}, "text": "MAU-net: multiple attention 3D U-net for lung cancer segmentation on CT images", "sentences": [], "annotations": [], "relations": []}, {"offset": 60789, "infons": {"fpage": "356", "lpage": "61", "name_0": "surname:Zhao;given-names:C", "name_1": "surname:Han;given-names:J", "name_2": "surname:Jia;given-names:Y", "name_3": "surname:Gou;given-names:F", "section_type": "REF", "type": "ref", "year": "2018"}, "text": "Lung nodule detection via 3D U-Net and contextual convolutional neural network", "sentences": [], "annotations": [], "relations": []}, {"offset": 60868, "infons": {"fpage": "1316", "issue": "5", "lpage": "25", "name_0": "surname:Seo;given-names:H", "name_1": "surname:Huang;given-names:C", "name_2": "surname:Bassenne;given-names:M", "name_3": "surname:Xiao;given-names:R", "name_4": "surname:Xing;given-names:L", "pub-id_doi": "10.1109/TMI.2019.2948320", "pub-id_pmid": "31634827", "section_type": "REF", "source": "IEEE Trans Med Imaging", "type": "ref", "volume": "39", "year": "2019"}, "text": "Modified U-Net (mU-Net) with the incorporation of object-dependent high-level features for improved liver and liver-tumor segmentation in CT images", "sentences": [], "annotations": [], "relations": []}, {"offset": 61016, "infons": {"fpage": "268", "issue": "1", "name_0": "surname:Jalali;given-names:Y", "name_1": "surname:Fateh;given-names:M", "name_2": "surname:Rezvani;given-names:M", "name_3": "surname:Abolghasemi;given-names:V", "name_4": "surname:Anisi;given-names:MH", "pub-id_doi": "10.3390/s21010268", "pub-id_pmid": "33401581", "section_type": "REF", "source": "Sensors", "type": "ref", "volume": "21", "year": "2021"}, "text": "ResBCDU-Net: a deep learning framework for lung CT image segmentation", "sentences": [], "annotations": [], "relations": []}, {"offset": 61086, "infons": {"fpage": "25", "issue": "1", "name_0": "surname:Li;given-names:W", "name_1": "surname:Liu;given-names:JB", "name_2": "surname:Hou;given-names:LK", "name_3": "surname:Yu;given-names:F", "name_4": "surname:Zhang;given-names:J", "name_5": "surname:Wu;given-names:W", "pub-id_doi": "10.1186/s12943-022-01505-z", "pub-id_pmid": "35057806", "section_type": "REF", "source": "Mol Cancer", "type": "ref", "volume": "21", "year": "2022"}, "text": "Liquid biopsy in lung cancer: significance in diagnostics, prediction, and treatment monitoring", "sentences": [], "annotations": [], "relations": []}, {"offset": 61182, "infons": {"fpage": "2266", "issue": "4", "lpage": "76", "name_0": "surname:Jiang;given-names:Z", "name_1": "surname:Wang;given-names:B", "name_2": "surname:Han;given-names:X", "name_3": "surname:Zhao;given-names:P", "name_4": "surname:Gao;given-names:M", "name_5": "surname:Zhang;given-names:Y", "pub-id_doi": "10.1007/s00330-021-08368-w", "pub-id_pmid": "34978579", "section_type": "REF", "source": "European Radiology", "type": "ref", "volume": "32", "year": "2022"}, "text": "Multimodality MRI-based radiomics approach to predict the posttreatment response of lung cancer brain metastases to gamma knife radiosurgery", "sentences": [], "annotations": [], "relations": []}, {"offset": 61323, "infons": {"fpage": "104456", "name_0": "surname:Zhang;given-names:Z", "name_1": "surname:Ji;given-names:A", "name_2": "surname:Wang;given-names:K", "name_3": "surname:Zhang;given-names:L", "pub-id_doi": "10.1016/j.autcon.2022.104456", "section_type": "REF", "source": "Autom Constr", "type": "ref", "volume": "142", "year": "2022"}, "text": "Unrollingnet: an attention-based deep learning approach for the segmentation of large-scale point clouds of tunnels", "sentences": [], "annotations": [], "relations": []}, {"offset": 61439, "infons": {"fpage": "11960", "issue": "8", "lpage": "9", "name_0": "surname:Abdelraouf;given-names:A", "name_1": "surname:Abdel-Aty;given-names:M", "name_2": "surname:Yuan;given-names:J", "pub-id_doi": "10.1109/TITS.2021.3108939", "section_type": "REF", "source": "IEEE Trans Intell Transp Syst", "type": "ref", "volume": "23", "year": "2021"}, "text": "Utilizing attention-based multi-encoder-decoder neural networks for freeway traffic speed prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 61540, "infons": {"fpage": "103706", "name_0": "surname:Wei;given-names:C", "name_1": "surname:Hui;given-names:F", "name_2": "surname:Yang;given-names:Z", "name_3": "surname:Jia;given-names:S", "name_4": "surname:Khattak;given-names:AJ", "pub-id_doi": "10.1016/j.trc.2022.103706", "section_type": "REF", "source": "Transp Res Part C Emerg", "type": "ref", "volume": "140", "year": "2022"}, "text": "Fine-grained highway autonomous vehicle lane-changing trajectory prediction based on a heuristic attention-aided encoder-decoder model", "sentences": [], "annotations": [], "relations": []}, {"offset": 61675, "infons": {"fpage": "btad075", "issue": "2", "name_0": "surname:Wang;given-names:Y", "name_1": "surname:Yu;given-names:Z", "name_2": "surname:Li;given-names:S", "name_3": "surname:Bian;given-names:C", "name_4": "surname:Liang;given-names:Y", "name_5": "surname:Wong;given-names:KC", "pub-id_doi": "10.1093/bioinformatics/btad075", "pub-id_pmid": "36734596", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "39", "year": "2023"}, "text": "scBGEDA: deep single-cell clustering analysis via a dual denoising autoencoder with bipartite graph ensemble clustering", "sentences": [], "annotations": [], "relations": []}, {"offset": 61795, "infons": {"fpage": "4671", "issue": "4", "lpage": "79", "name_0": "surname:Yu;given-names:Z", "name_1": "surname:Lu;given-names:Y", "name_2": "surname:Wang;given-names:Y", "name_3": "surname:Tang;given-names:F", "name_4": "surname:Wong;given-names:KC", "name_5": "surname:Li;given-names:X", "pub-id_doi": "10.1609/aaai.v36i4.20392", "section_type": "REF", "source": "Proceedings of the AAAI conference on artificial intelligence", "type": "ref", "volume": "36", "year": "2022"}, "text": "Zinb-based graph embedding autoencoder for single-cell rna-seq interpretations", "sentences": [], "annotations": [], "relations": []}, {"offset": 61874, "infons": {"name_0": "surname:Li;given-names:P", "name_1": "surname:Wang;given-names:S", "name_2": "surname:Li;given-names:T", "name_3": "surname:Lu;given-names:J", "name_4": "surname:HuangFu;given-names:Y", "name_5": "surname:Wang;given-names:D", "pub-id_doi": "10.7937/TCIA.2020.NNC2-0461", "section_type": "REF", "type": "ref", "year": "2020"}, "text": "A Large-Scale CT and PET/CT Dataset for Lung Cancer Diagnosis", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]