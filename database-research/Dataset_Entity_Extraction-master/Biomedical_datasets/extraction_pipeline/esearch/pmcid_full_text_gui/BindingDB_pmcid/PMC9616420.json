[{"source": "PMC", "date": "20221031", "key": "pmc.key", "infons": {}, "documents": [{"id": "9616420", "infons": {"license": "CC BY"}, "passages": [{"offset": 0, "infons": {"article-id_doi": "10.1038/s41598-022-23014-1", "article-id_pmc": "9616420", "article-id_pmid": "36307509", "article-id_publisher-id": "23014", "elocation-id": "18200", "kwd": "Computational biology and bioinformatics Drug discovery", "license": "Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.", "name_0": "surname:Wei;given-names:Bomin", "name_1": "surname:Zhang;given-names:Yue", "name_2": "surname:Gong;given-names:Xiang", "section_type": "TITLE", "title": "Subject terms", "type": "front", "volume": "12", "year": "2022"}, "text": "DeepLPI: a novel deep learning-based model for protein\u2013ligand interaction prediction for drug repurposing", "sentences": [], "annotations": [], "relations": []}, {"offset": 108, "infons": {"section_type": "ABSTRACT", "type": "abstract"}, "text": "The substantial cost of new drug research and development has consistently posed a huge burden for both pharmaceutical companies and patients. In order to lower the expenditure and development failure rate, repurposing existing and approved drugs by identifying interactions between drug molecules and target proteins based on computational methods have gained growing attention. Here, we propose the DeepLPI, a novel deep learning-based model that mainly consists of ResNet-based 1-dimensional convolutional neural network (1D CNN) and bi-directional long short term memory network (biLSTM), to establish an end-to-end framework for protein\u2013ligand interaction prediction. We first encode the raw drug molecular sequences and target protein sequences into dense vector representations, which go through two ResNet-based 1D CNN modules to derive features, respectively. The extracted feature vectors are concatenated and further fed into the biLSTM network, followed by the MLP module to finally predict protein\u2013ligand interaction. We downloaded the well-known BindingDB and Davis dataset for training and testing our DeepLPI model. We also applied DeepLPI on a COVID-19 dataset for externally evaluating the prediction ability of DeepLPI. To benchmark our model, we compared our DeepLPI with the baseline methods of DeepCDA and DeepDTA, and observed that our DeepLPI outperformed these methods, suggesting the high accuracy of the DeepLPI towards protein\u2013ligand interaction prediction. The high prediction performance of DeepLPI on the different datasets displayed its high capability of protein\u2013ligand interaction in generalization, demonstrating that the DeepLPI has the potential to pinpoint new drug-target interactions and to find better destinations for proven drugs.", "sentences": [], "annotations": [], "relations": []}, {"offset": 1890, "infons": {"section_type": "INTRO", "type": "title_1"}, "text": "Introduction", "sentences": [], "annotations": [], "relations": []}, {"offset": 1903, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Introducing a new drug to the market has been characterized to be risky, time-consuming, and costly. Drug discovery is the first phase of drug research and development (R&D) that starts with identifying targets of an unmet disease such as proteins, followed by creating and optimizing a promising compound that can interact with the targets efficiently and safely. This step usually involves hundreds and thousands of compounds, yet only about 8% of which as drug leads can enter the phase of the in vitro and in vivo preclinical research. To shorten the duration and improve the success rate in the phase of drug discovery, drug repurposing has become a hotspot of new drug research and development over the past few years, which intends to find an effective cure for a disease from a large amount of existing and approved drugs that were developed for other purposes. For example, prednisone was originally developed for the treatment of inflammatory diseases, but it is likely to be effective against Parkinson's disease as well. In midst of all the drug repurposing methods, in silico computational-based methods to screen pharmaceutical compound libraries and identify drug-target interactions (DTIs) or protein\u2013ligand interactions (PLIs) have gained increasing attention and made significant breakthroughs due to the development in high performance computational architectures and advances in machine learning methods.", "sentences": [], "annotations": [], "relations": []}, {"offset": 3330, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Over the last decade, a variety of machine learning-based models have been developed to identify PLIs from millions of ligands and proteins. One type of models utilized 3D structures of proteins and drug molecules aiming at capturing interaction details in predictions of the drug-target binding affinity, such as Atomnet and SE-OnionNet, but insufficient 3D protein structure data limited the practicability, generalizability, and accuracy. To exploit the vastly available protein sequencing data, a new type of model calculates human-selected features and predicts drug-target interactions with conventional machine learning. The disadvantages of these methods are that they not only require much domain knowledge but also possibly lead to a loss of the information about raw protein\u2013ligand interactions due to limited features. Deep learning-based models can automatically learn highly complex and abstract level of features from large-scale datasets without extensive manual creation. Yet the recent development considers only simple encoding of input letter information. Without the contextual information, this type of model may not capture the complex protein features and thus have limited accuracy and generalizability.", "sentences": [], "annotations": [], "relations": []}, {"offset": 4561, "infons": {"section_type": "INTRO", "type": "paragraph"}, "text": "Here, we propose DeepLPI, an innovative deep learning-based model to predict protein\u2013ligand interaction using the simple formats of raw protein 1D sequences and 1D ligands (i.e., drug molecular) SMILES (Simplified Molecular Input Line Entry System) strings as inputs, rather than manual-generated features or complex 3D protein structures. To capture contextual information in the sequence data, we first respectively employ Natural Language Processing-inspired, pre-trained models of Mol2Vec and ProSE to embed drug SMILES strings and protein FASTA sequences as numeric vectors. These embedded numeric vectors are then fed into two blocks, each of them consisting of two modules termed head convolutional module and ResNet-based convolutional neural network (CNN) module, to encode proteins and drug sequences, respectively. The encoded representations are concatenated into a vector and further fed into a bi-directional long short-term memory (biLSTM) layer, followed by three fully connected layers. We download the BindingDB dataset and Davis dataset to train the DeepLPI model and adjusted the hyperparameters and internally independently evaluate its performance towards PLI prediction. We further transformed the model on a COVID-19 3CL Protease dataset for externally assessing the prediction ability of DeepLPI. To benchmark our model, we compared DeepLPI with the baseline methods of DeepDTA and DeepCDA towards protein\u2013ligand interaction. The prediction performance is quantitatively represented in terms of area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive (PPV), predictive value, and negative predictive value (NPV). The high performance of our DeepLPI towards protein\u2013ligand interaction prediction suggests that our model has the potential to accurately identify protein\u2013ligand interaction and hence, promote the new drug development.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6469, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Methods", "sentences": [], "annotations": [], "relations": []}, {"offset": 6477, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Dataset and data preprocessing", "sentences": [], "annotations": [], "relations": []}, {"offset": 6508, "infons": {"file": "41598_2022_23014_Fig1_HTML.jpg", "id": "Fig1", "section_type": "FIG", "type": "fig_caption"}, "text": "Distribution of BindingDB data used to develop the DeepLPI model. (a) Distribution of the  values and the threshold for determining active/inactive. (b) Distribution of\u00a0interaction in binary classes. (c) Distribution of lengths of protein sequences. (d) Distribution of lengths of drug molecular SMILES strings.", "sentences": [], "annotations": [], "relations": []}, {"offset": 6821, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We use the BindingDB and Davis datasets to train and evaluate our DeepLPI model. We also use the COVID-19 3C-like Protease dataset from Diamond Light Source for further assessment. All datasets are publicly accessible. The BindingDB is a continually updating database that contains 2,278,226 experimentally identified binding affinities between 8005 target proteins and 986,143 small drug molecules up to July 29, 2021. We first apply the following criteria to compile the dataset for the development of our model (Fig. S1): (1) excluding binding interactions with multichain protein complexes because it is not capable of identifying which chain of the protein interacts with the molecule; (2) retaining binding interactions only represented by  value and it means that other measurements in the form of  or  values are removed; (3) keeping common drug molecules and target proteins occurring in at least three and six interactions in the entire dataset, respectively; (4) removing data with invalid  values and removing duplicated data entries. For example, we notice that some data used \">\" and \"<\" in the labeled values to indicate ranges, and directly exclude them for the subsequent analysis. Additionally, there are some zeros in the values which should not appear based on the definition of binding affinity measurement of . Thus, we treat them as invalid values and simply removed them; As a result, a total of 36,111 interactions with 17,773 drug molecules and 1,915 protein targets are finally used in developing our model. (5) As a binary classification problem in this study, we use label 1 to represent a pair of protein and ligand being active if their corresponding  value is less than 100\u00a0nM and use label 0 to represent a pair being inactive if their  value is greater or equal to 100\u00a0nM since a greater dissociation constant means weaker binding. In this case, 59.9% of data are labeled active, and 40.1% of data are labeled inactive (Fig.\u00a01a, b). The median (standard deviation, [minimum, maximum]) lengths of drug molecular SMILES strings and protein sequences are 52 (45.81, [1, 760]) and 445 (456.1, [9, 7096]) (Fig.\u00a01c, d).", "sentences": [], "annotations": [], "relations": []}, {"offset": 8973, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We then randomly select 85% of the pre-processed BindingDB dataset as the training set and the remaining 15% as the internal independent testing set to train and evaluate our DeepLPI model. To optimize hyperparameters, we further allocate 10% of all data from the training set for validation during the training phase, and the rest are used as a training subset (i.e., 75% of all data). The AUROC and four classification metrics (sensitivity, specificity, PPV and NPV) are computed.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9456, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To characterize the model generalizability, different drugs and proteins were selected and reserved for the testing set (Fig. S2). \u201cDrug unseen\u201d testing set consists of drugs not seen in the training set, \u201cProtein unseen\u201d testing set consists of proteins not seen in the training set, and \u201cNone seen\u201d testing set consists of drugs and proteins neither seen in the training set.", "sentences": [], "annotations": [], "relations": []}, {"offset": 9846, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The reason for choosing  rather than other binding measurements is for enabling our model performance to be reasonably compared on Davis dataset, which contains interactions of 442 unique proteins and 68 unique compounds. The Davis dataset only reports  values of the kinase protein family and the relevant inhibitors. We used the same protocol to obtain the class label as we did above. The Davis dataset was referenced from the Davis work and downloaded from the URL therein. The dataset contained duplicated data entries where the drug-protein pairs are the same, but the binding affinity values are different, potentially due to the experiment conditions. We keep only one entry in each group of duplicates. This doesn't affect the balance of the dataset because according to our binary threshold, all data entries in the same duplicate group in fact have the same binary label. After the treatment, there are 24,548 interaction data entries. We split them into training, validation, and testing sets according to the same method described above.", "sentences": [], "annotations": [], "relations": []}, {"offset": 10897, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To find effective drugs for SARS-CoV-2, we applied our model on a COVID-19 dataset where 879 small molecule drugs were tested on the SARS-COV-2 3C-like protease. The experiment measured EC50 results. For classification, we label 1 to indicate drug-protease active if EC50 is less than 30\u00a0nM or 0 representing inactivity. The data is retrieved from a large XChem crystallographic fragment screen against SARS-CoV-2 main protease at high resolution from MIT AiCures. Among those data, 78 are active according to the threshold.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11423, "infons": {"section_type": "METHODS", "type": "title_1"}, "text": "Model design", "sentences": [], "annotations": [], "relations": []}, {"offset": 11436, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Overview of DeepLPI model", "sentences": [], "annotations": [], "relations": []}, {"offset": 11462, "infons": {"file": "41598_2022_23014_Fig2_HTML.jpg", "id": "Fig2", "section_type": "FIG", "type": "fig_caption"}, "text": "The overview of the DeepLPI model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 11497, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "The proposed DeepLPI consists of eight modules (Fig.\u00a02), including two embedding modules, two head modules, two ResNet-based CNN modules, one bi-directional LSTM (biLSTM) module, and one multilayer perceptron module (MLP). DeepLPI employs raw molecular SMILES strings and protein sequences as inputs, which are first represented as numeric vectors using the pretrained models of Mol2Vec and ProSE, respectively. The embedded vectors for the drug SMILES and the protein sequences are then fed into the respective head module and ResNet-based CNN module to extract features. The feature vectors for the inputs of drug molecules and protein targets are concatenated, pooled (max-pooling operation), and then encoded by a biLSTM layer. Subsequently, the encoded vectors are finally fed into an MLP module, and the final output is activated through a sigmoid function for binary classification to predict active/inactive labels.", "sentences": [], "annotations": [], "relations": []}, {"offset": 12422, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Embedding module", "sentences": [], "annotations": [], "relations": []}, {"offset": 12439, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "To utilize the raw drug molecular SMILES string and protein sequence as inputs to the DeepLPI model, we first encode them into numeric vector representations using the pre-trained embedding models Mol2Vec and ProSE, respectively. Mol2Vec is an unsupervised deep learning-based approach to convert a molecule into a numeric vector representation. Inspired by natural language processing (NLP) techniques, Mol2Vec regards the molecular substructures obtained by the Morgan identifier as \"words\" and the compound as \"sentences\", and then encodes them into dense vector representations based on a so-called corpus of compounds. On the other hand, the ProSE is a masked language-based model, using biLSTM networks to capture contextual features in protein FASTA sequence, which are represented into numeric vectors that encode protein structural information and functional properties. It first translates a protein sequence into a list of specific alphabets (as a \"sentence\") which map similar amino acids (as \"words\") into close numbers. Then, the ProSE model encodes the words into numeric vectors.", "sentences": [], "annotations": [], "relations": []}, {"offset": 13535, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We utilize the pre-trained Mol2Vec (download link: https://github.com/samoturk/mol2vec) and ProSE (download link, https://github.com/tbepler/prose) to obtain vector representations with a fixed length for the drug molecular compound and protein, respectively.", "sentences": [], "annotations": [], "relations": []}, {"offset": 13795, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "Head module and ResNet-based CNN module", "sentences": [], "annotations": [], "relations": []}, {"offset": 13835, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "After the embedding, we separately feed the drug molecular SMILES string vector and protein sequence vector each into the head modules with the same network architecture. The head module contained the following layers: 1D convolutional, batch normalization, nonlinear transformation (with the rectified linear unit, i.e., ReLU activation), dropout, and max-pooling.", "sentences": [], "annotations": [], "relations": []}, {"offset": 14201, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Subsequently, two ResNet-based CNN modules are connected to the corresponding head module to further encode the information of input. Suppose  is the input into a ResNet-based block, the output of stacked layers is called residual, denoted as , we then calculated ResNet-based block output with equation . Similar to the head module, the two ResNet-based CNN modules had the same network architecture. Specifically, each ResNet-based CNN module consists of three consecutive ResNet-based blocks, and each block comprises two branches, where the right branch is known as \"shortcut connection\"; and the left branch is known as a residual network that contains several stacked layers, including a 1D convolutional layer, a batch normalization layer, a ReLU layer, a dropout layer, another 1D convolutional layer, and one more batch normalization layer in sequence.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15063, "infons": {"section_type": "METHODS", "type": "title_3"}, "text": "biLSTM module and MLP module", "sentences": [], "annotations": [], "relations": []}, {"offset": 15092, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In the biLSTM module, we first concatenate the outputs of features extracted by the two ResNet-based CNN modules, followed by an average-pooling layer. The biLSTM, which stands for bidirectional long short-term memory, can learn long-term dependency from inputs. This network processes the input twice, once from starting to the end and once the reverse way and thus can balance the molecular information and the protein information. Finally, the outputs on both sides of biLSTM will be combined as the output vector.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15610, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "In the MLP module, we flatten the output vector of biLSTM and fed it into three stacks of consecutive FC layers. Finally, the output is passed through a sigmoid function for binary classification to predict 1/0 labels.", "sentences": [], "annotations": [], "relations": []}, {"offset": 15829, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Loss function", "sentences": [], "annotations": [], "relations": []}, {"offset": 15843, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We treat the prediction as a classification task, predicting whether the drug and protein have a strong binding or a weak binding. For the  pairs of molecular SMILES strings and protein sequences the loss function of the DeepLPI model was given by:where  is class label representing whether or not binding interaction of an input pair of protein and ligand sequences .  is the probability of interaction prediction for the input pair  by our model, ,  is the output of the MLP module of our model.  is the trainable weight matrix in our model.  is decay rate and we set it as 0.8 in this study.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16438, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Evaluation metrics", "sentences": [], "annotations": [], "relations": []}, {"offset": 16457, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Sensitivity:", "sentences": [], "annotations": [], "relations": []}, {"offset": 16470, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "where  represents true positives,  represents false positives,  represents true negatives,  represents false negatives.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16590, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Specificity:", "sentences": [], "annotations": [], "relations": []}, {"offset": 16603, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Positive predictive value (PPV):", "sentences": [], "annotations": [], "relations": []}, {"offset": 16636, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Negative predictive value (NPV):", "sentences": [], "annotations": [], "relations": []}, {"offset": 16669, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We calculate five metrics including area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) to evaluate the performance of our model.", "sentences": [], "annotations": [], "relations": []}, {"offset": 16906, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Experiment setup", "sentences": [], "annotations": [], "relations": []}, {"offset": 16923, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "Model training was done in Aliyun Cloud Computing. The node CPU used Intel(R) Xeon(R) Platinum 8163 (2.50\u00a0GHz). An Nvidia Tesla T4 GPU is supplied. The source code is available on GitHub. The model is implemented using the PyTorch library (version 1.8.1). The source code of training and evaluating DeepLPI and the requirements are available on GitHub (https://github.com/David-BominWei/DeepLPI).", "sentences": [], "annotations": [], "relations": []}, {"offset": 17321, "infons": {"section_type": "METHODS", "type": "title_2"}, "text": "Parameters setting for training DeepLPI", "sentences": [], "annotations": [], "relations": []}, {"offset": 17361, "infons": {"section_type": "METHODS", "type": "paragraph"}, "text": "We use Kaiming Initialization to initialize DeepLPI network weights. The Adam optimizer is also employed with default parameters of  and  as an optimization algorithm to train our model. Furthermore, we use a batch size of 256 and initialize the learning rate at 0.001 with a decay rate of 0.8 for every 10 epochs. The maximum number of epochs is 100 epochs for BindingDB training and 50 for Davis training. All settings for the parameters implemented in our DeepLPI model are demonstrated in Table S1. It should be noted that we use the default parameter values for the pre-trained Mol2Vec and ProSE, and we yield vector representations with a fixed length of 300 for the drug molecules, and two lengths of 100 and 6165 for the target proteins. The 6,165-element representation for protein was tested to outperform the 100-element representation, and thus in the article we report only the 6,165-element results. Generally, we manually tune and optimize the hyperparameters of the DeepLPI network and choose the number of blocks empirically in the ResNet-based module.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18431, "infons": {"section_type": "RESULTS", "type": "title_1"}, "text": "Results", "sentences": [], "annotations": [], "relations": []}, {"offset": 18439, "infons": {"section_type": "RESULTS", "type": "title_2"}, "text": "Training and evaluation results", "sentences": [], "annotations": [], "relations": []}, {"offset": 18471, "infons": {"section_type": "RESULTS", "type": "title_3"}, "text": "Evaluation on BindingDB dataset", "sentences": [], "annotations": [], "relations": []}, {"offset": 18503, "infons": {"file": "41598_2022_23014_Fig3_HTML.jpg", "id": "Fig3", "section_type": "FIG", "type": "fig_caption"}, "text": "The loss and AUROC score during the DeepLPI training on the BindingDB Kd dataset. (a) Loss scores for training and validation. (b) AUROC scores for training and validation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 18676, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We first report the\u00a0loss and metric progression during training on the training dataset from BindingDB database in Fig.\u00a03. The model training is stopped after 40 epochs once the validation loss stopped decreasing. Training beyond this point would lead to apparent overfitting marked by an\u00a0increase of the validation loss. We calculated the AUROC metric values during the model training, which achieved 0.95 and 0.89 for the training and validation, respectively.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19142, "infons": {"file": "41598_2022_23014_Fig4_HTML.jpg", "id": "Fig4", "section_type": "FIG", "type": "fig_caption"}, "text": "The prediction performance of the final DeepLPI model on BindingDB dataset. (a) The ROC curve and the determined optimal threshold. (b) Confusion matrix based on the optimal threshold. (c\u2013e) Confusion matrix and performance metrics on the three \u201cunseen\u201d drug/protein testsets: (c) Molecule unseen, (d) Protein unseen and (e) None seen.", "sentences": [], "annotations": [], "relations": []}, {"offset": 19484, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We applied the trained model on the internal independent testing set, achieving AUROC measured of 0.893. We used Youden's J statistic to determine the optimal classification threshold instead of using the\u00a0default value of 0.5 (Fig.\u00a04a). The optimal threshold was used for later calculations of confusion matrix metrics (Fig.\u00a04b-e) including the three testing sets \u201cMolecule unseen\u201d (Fig.\u00a04c), \u201cProtein unseen\u201d (Fig.\u00a04d) and \u201cNone seen\u201d (Fig.\u00a04e) where the drugs and/or the proteins are of different types (\u201cunseen\u201d) from those in the training set.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20054, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We observed that the DeepLPI model obtained the highest AUROC of 0.857 when the training set had partial knowledge of the\u00a0proteins in the testing set. In addition, AUROC can reach 0.718 when the testing set contains interaction pairs of molecules (Fig.\u00a04c) or protein (Fig.\u00a04d) that were \"unseen\" in the training set. Furthermore, AUROC can achieve 0.655 (Fig.\u00a04e) when the training set has no knowledge of the interaction pairs of molecule and protein that exist in the testing set.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20542, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_caption"}, "text": "Comparing performance of DeepLPI, DeepCDA and DeepDTA on the internal independent testing set from the BindingDB data.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20661, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\">BindingDB</th><th align=\"left\">AUROC</th><th align=\"left\">Sensitivity</th><th align=\"left\">Specificity</th><th align=\"left\">PPV</th><th align=\"left\">NPV</th></tr></thead><tbody><tr><td align=\"left\">This work</td><td char=\".\" align=\"char\"><bold>0.893</bold></td><td char=\".\" align=\"char\"><bold>0.831</bold></td><td char=\".\" align=\"char\">0.792</td><td align=\"left\">0.728</td><td char=\".\" align=\"char\"><bold>0.875</bold></td></tr><tr><td align=\"left\">DeepCDA</td><td char=\".\" align=\"char\">0.882</td><td char=\".\" align=\"char\">0.792</td><td char=\".\" align=\"char\">0.804</td><td align=\"left\">0.730</td><td char=\".\" align=\"char\">0.852</td></tr><tr><td align=\"left\">DeepDTA</td><td char=\".\" align=\"char\">0.889</td><td char=\".\" align=\"char\">0.772</td><td char=\".\" align=\"char\"><bold>0.862</bold></td><td align=\"left\"><bold>0.790</bold></td><td char=\".\" align=\"char\">0.849</td></tr><tr><td align=\"left\" colspan=\"6\"><bold>Unseen testsets combined</bold></td></tr><tr><td align=\"left\">This work</td><td char=\".\" align=\"char\"><bold>0.790</bold></td><td char=\".\" align=\"char\"><bold>0.684</bold></td><td char=\".\" align=\"char\">0.773</td><td align=\"left\"><bold>0.671</bold></td><td char=\".\" align=\"char\"><bold>0.783</bold></td></tr><tr><td align=\"left\">DeepCDA</td><td char=\".\" align=\"char\">0.448</td><td char=\".\" align=\"char\">0.000</td><td char=\".\" align=\"char\"><bold>1.0</bold></td><td align=\"left\">Nan</td><td char=\".\" align=\"char\">0.596</td></tr></tbody></table>\n"}, "text": "BindingDB\tAUROC\tSensitivity\tSpecificity\tPPV\tNPV\t \tThis work\t0.893\t0.831\t0.792\t0.728\t0.875\t \tDeepCDA\t0.882\t0.792\t0.804\t0.730\t0.852\t \tDeepDTA\t0.889\t0.772\t0.862\t0.790\t0.849\t \tUnseen testsets combined\t \tThis work\t0.790\t0.684\t0.773\t0.671\t0.783\t \tDeepCDA\t0.448\t0.000\t1.0\tNan\t0.596\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 20939, "infons": {"file": "Tab1.xml", "id": "Tab1", "section_type": "TABLE", "type": "table_foot"}, "text": "Significant values are in bold.", "sentences": [], "annotations": [], "relations": []}, {"offset": 20971, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In Table 1, we compare the performance metrics of our model with the recently published DeepCDA model and the popular baseline model DeepDTA on the independent testing set from the BindingDB data. The results demonstrated that DeepLPI scored higher AUROC by 0.011 and 0.004 than DeepCDA and DeepDTA, respectively. Given that all models\u2019 AUROC are close to 0.9, it can be concluded that DeepLPI is able to predict on BindingDB dataset with very high accuracy. DeepLPI reached a\u00a0sensitivity of 0.832 and NPV of 0.875, which were higher than both DeepCDA and DeepDTA. The specificity and PPV are close to DeepCDA but lower than DeepDTA, due to higher false positive rates indicating the DeepLPI and DeepCDA tend to \u201cover-bind\u201d drug-target pairs. This might result from the more complex network structure with LSTM methods, which invites further investigations.", "sentences": [], "annotations": [], "relations": []}, {"offset": 21836, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Overall prediction results on the combined \u201cunseen\u201d testing sets showed DeepLPI with AUROC of 0.790 is 76% better than the DeepCDA model. The DeepCDA with AUROC of 0.448 is non-predictive given that random prediction would lead to AUROC of 0.5, indicating DeepLPI has greater generalizability than the two models when applied on different kinds of drugs and proteins outside of the training domain.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22239, "infons": {"section_type": "RESULTS", "type": "title_3"}, "text": "Evaluation on Davis dataset", "sentences": [], "annotations": [], "relations": []}, {"offset": 22267, "infons": {"file": "41598_2022_23014_Fig5_HTML.jpg", "id": "Fig5", "section_type": "FIG", "type": "fig_caption"}, "text": "The loss and AUROC score during the DeepLPI training on Davis dataset (a) loss scores for training and validation. (b) AUROC scores for training and validation.", "sentences": [], "annotations": [], "relations": []}, {"offset": 22428, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Model training on Davis dataset is stopped after 40 epochs to avoid overfitting when the validation loss stopped decreasing. AUROC values during the model training achieved 0.99 and 0.91 on the training and validation sets, respectively (Fig.\u00a05).", "sentences": [], "annotations": [], "relations": []}, {"offset": 22676, "infons": {"file": "41598_2022_23014_Fig6_HTML.jpg", "id": "Fig6", "section_type": "FIG", "type": "fig_caption"}, "text": "The prediction performance of the final DeepLPI model Davis dataset. (a) The ROC curve and the determined optimal threshold. (b) Confusion matrix based on the optimal threshold\u00a0on the combined \"unseen\" testing sets. (c\u2013e) Confusion matrix and performance metrics on the three unseen drug/protein testsets: (c) Molecule unseen, (d) Protein unseen and (e) None seen.", "sentences": [], "annotations": [], "relations": []}, {"offset": 23044, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "Evaluating the trained model on the Davis independent testing set, we obtained AUROC of 0.925. We also used Youden's J statistic to determine the optimal classification threshold (Fig.\u00a06a), which was used for later calculation of confusion matrix metrics (Fig.\u00a06b\u2013e) including \u201cMolecule unseen\u201d (Fig.\u00a06c), \u201cProtein unseen\u201d (Fig.\u00a06d) and \u201cNone seen\u201d (Fig.\u00a06e) testing sets. In these \u201cunseen\u201d testing sets, DeepLPI model achieved higher accuracy in \"Protein unseen\" (Fig.\u00a06d) testing set with AUROC of 0.812, compared to in the \u201cNone seen\u201d (Fig.\u00a06e) testing set with AUROC 0.692, and the lowest accuracy was in \u201cDrug unseen\u201d (Fig.\u00a06c) testing set with AUROC 0.618. \u201cDrug unseen\u201d was expected to result in a\u00a0higher AUROC because of the partial protein knowledge it includes, and the lower AUROC result might arise from the specific drug molecules collection because Davis dataset did not contain sufficient drug molecules. A\u00a0larger dataset with more drug molecules could lead to better performance as in the BindingDB case.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24105, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table_caption"}, "text": "Comparing Performance of DeepLPI, DeepCDA and DeepDTA on the internal independent testing set from the BindingDB data.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24224, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\"/><th align=\"left\">AUROC</th><th align=\"left\">Sensitivity</th><th align=\"left\">Specificity</th><th align=\"left\">PPV</th><th align=\"left\">NPV</th></tr></thead><tbody><tr><td align=\"left\">DeepLPI</td><td char=\".\" align=\"char\"><bold>0.925</bold></td><td char=\".\" align=\"char\">0.855</td><td char=\".\" align=\"char\">0.862</td><td char=\".\" align=\"char\">0.306</td><td char=\".\" align=\"char\"><bold>0.988</bold></td></tr><tr><td align=\"left\">DeepCDA</td><td char=\".\" align=\"char\">0.912</td><td char=\".\" align=\"char\">0.766</td><td char=\".\" align=\"char\"><bold>0.896</bold></td><td char=\".\" align=\"char\"><bold>0.342</bold></td><td char=\".\" align=\"char\">0.982</td></tr><tr><td align=\"left\">DeepDTA</td><td char=\".\" align=\"char\">0.909</td><td char=\".\" align=\"char\"><bold>0.865</bold></td><td char=\".\" align=\"char\">0.795</td><td char=\".\" align=\"char\">0.221</td><td char=\".\" align=\"char\"><bold>0.989</bold></td></tr><tr><td align=\"left\" colspan=\"6\"><bold>Unseen testsets combined</bold></td></tr><tr><td align=\"left\">This work</td><td char=\".\" align=\"char\"><bold>0.791</bold></td><td char=\".\" align=\"char\"><bold>0.661</bold></td><td char=\".\" align=\"char\">0.789</td><td char=\".\" align=\"char\">0.132</td><td char=\".\" align=\"char\"><bold>0.980</bold></td></tr><tr><td align=\"left\">DeepCDA</td><td char=\".\" align=\"char\">0.741</td><td char=\".\" align=\"char\">0.511</td><td char=\".\" align=\"char\"><bold>0.813</bold></td><td char=\".\" align=\"char\"><bold>0.495</bold></td><td char=\".\" align=\"char\">0.823</td></tr></tbody></table>\n"}, "text": "\tAUROC\tSensitivity\tSpecificity\tPPV\tNPV\t \tDeepLPI\t0.925\t0.855\t0.862\t0.306\t0.988\t \tDeepCDA\t0.912\t0.766\t0.896\t0.342\t0.982\t \tDeepDTA\t0.909\t0.865\t0.795\t0.221\t0.989\t \tUnseen testsets combined\t \tThis work\t0.791\t0.661\t0.789\t0.132\t0.980\t \tDeepCDA\t0.741\t0.511\t0.813\t0.495\t0.823\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 24495, "infons": {"file": "Tab2.xml", "id": "Tab2", "section_type": "TABLE", "type": "table_foot"}, "text": "Significant values are in bold.", "sentences": [], "annotations": [], "relations": []}, {"offset": 24527, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "In Table 2, we compared the performance metrics of our model with DeepCDA and DeepDTA on the independent testing set from the Davis dataset. DeepLPI with AUROC of 0.925 scored higher values by 0.013 and 0.006 than DeepCDA and DeepDTA, respectively. Given that all models\u2019 AUROC are above 0.9, it can be concluded that DeepLPI is able to predict on Davis dataset with very high accuracy. DeepLPI reached a\u00a0sensitivity of 0.855, specificity of 0.862, NPV of 0.988 and PPV of 0.306, which were higher than or comparable to both DeepCDA and DeepDTA. The low value of PPV may be due to the unbalanced distribution of Davis dataset with less than 10% strong binding entries, and therefore leading to a much smaller true positive rate compared with the false positive. We argue that the result does not invalidate the three models but put into question the suitability of Davis dataset as a benchmark. Overall prediction results on the combined \u201cunseen\u201d testing sets show DeepLPI with AUROC of 0.791 is higher than the DeepCDA model, indicating DeepLPI has better generalizability.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25609, "infons": {"section_type": "RESULTS", "type": "title_3"}, "text": "Evaluation on COVID-19", "sentences": [], "annotations": [], "relations": []}, {"offset": 25632, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_caption"}, "text": "Comparison of DeepLPI and DeepCDA on transferring BindingDB trained model to COVID-19.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25719, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table", "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\"/><th align=\"left\">AUROC</th><th align=\"left\">Sensitivity</th><th align=\"left\">Specificity</th><th align=\"left\">PPV</th><th align=\"left\">NPV</th></tr></thead><tbody><tr><td align=\"left\">DeepLPI</td><td char=\".\" align=\"char\"><bold>0.610</bold></td><td char=\".\" align=\"char\">0.538</td><td char=\".\" align=\"char\">0.576</td><td align=\"left\"><bold>0.110</bold></td><td char=\".\" align=\"char\"><bold>0.928</bold></td></tr><tr><td align=\"left\">DeepCDA</td><td char=\".\" align=\"char\">0.400</td><td char=\".\" align=\"char\">0.000</td><td char=\".\" align=\"char\"><bold>1.000</bold></td><td align=\"left\">nan</td><td char=\".\" align=\"char\">0.911</td></tr></tbody></table>\n"}, "text": "\tAUROC\tSensitivity\tSpecificity\tPPV\tNPV\t \tDeepLPI\t0.610\t0.538\t0.576\t0.110\t0.928\t \tDeepCDA\t0.400\t0.000\t1.000\tnan\t0.911\t \t", "sentences": [], "annotations": [], "relations": []}, {"offset": 25839, "infons": {"file": "Tab3.xml", "id": "Tab3", "section_type": "TABLE", "type": "table_foot"}, "text": "Significant values are in bold.", "sentences": [], "annotations": [], "relations": []}, {"offset": 25871, "infons": {"section_type": "RESULTS", "type": "paragraph"}, "text": "We further applied the trained models using BindingDB dataset directly on the COVID-19 dataset without any fine-tuning. DeepLPI outperforms DeepCDA with an AUROC of 0.610 (Table 3). The high prediction performance on COVID-19 dataset suggests that our DeepLPI may be the potential method to find effective drugs for SARS-CoV-2. The low PPV and specificity of DeepLPI arise from large false positive rates and are indicating the potential upgrades in our future works.", "sentences": [], "annotations": [], "relations": []}, {"offset": 26339, "infons": {"section_type": "DISCUSS", "type": "title_1"}, "text": "Discussion", "sentences": [], "annotations": [], "relations": []}, {"offset": 26350, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "In our work, we successfully build DeepLPI model to predict DTI in classification tasks using 1D sequence data from protein and drug molecules. We first utilize the pre-trained embedding methods called Mol2Vec and ProSE to encode the raw drug molecular SMILES strings and target protein sequences respectively into dense vector representations. Then, we feed the encoded dense vector representations separately into head modules and ResNet-based modules to extract features, where these modules are based on 1D CNN. The extracted feature vectors are concatenated and fed into the biLSTM network, further followed by the MLP module to finally predict binary active or inactive based on Kd affinity labeled data. We used three datasets of BindingDB, Davis and COVID-19 to evaluate our DeepLPI model, and the results demonstrate that our model has a high performance on the prediction. There is a recently published novel model named DTITR that employed transformer and cross attentions for ligand\u2013protein binding affinity prediction. We compared the DTITR and our DeepLPI model and found that they used very different data preprocessing procedure. Specifically speaking, the DTITR dealt with drug information encoding using a dictionary that didn\u2019t include \u201cP\u201d for phosphorus element and treated the letters in SMILES representations case-sensitively. As a temporary solution, we compared the DTITR and DeepLPI methods based on different data preprocessing procedures and found DTITR and DeepLPI are comparable in terms of AUROC values (DTITR AUROC 0.932, DeepLPI AUROC 0.925). Future study is guaranteed to develop a unified data preprocessing procedure applicable to both DTITR and DeepLPI.", "sentences": [], "annotations": [], "relations": []}, {"offset": 28049, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "Unlike the methods to pre-define features that are heavily relied on domain knowledge or to represent sequences simply using sparse encoding approach, our DeepLPI applied pre-trained embedding models of Mol2Vec and ProSE to encode the raw drug SMILES string and target protein sequences, respectively. These semantic context embedding models are trained using a huge dataset to represent sequence data in the form of dense vectors, with consideration of structure information of molecule and target proteins to ensure that they are highly informative and efficient for feature embeddings. The language model-based contextual embedding is assumed to be the primary reason that DeepLPI outperforms DeepCDA and DeepDTA. It is admired that there exists a variety of embedding methods to encode drug compounds and protein sequences, we picked Mol2Vec and ProSE in our DeepLPI. Our method used a protein language model (ProSE) for embedding, which outperformed a couple of baseline language models. In our future study, we will further compare the performance of ProSE and other novel protein language models. We use 1D CNN in our DeepLPI model to retain the sequential correlation. We adopt a ResNet-based module in the DeepLPI. Traditional feed-forward CNN may lose useful information as the design grows deeper. Nevertheless, ResNet-based CNN can mitigate this drawback by developing a \"shortcut connection\" for the network. Consequently, data inputted into the ResNet-based CNN module can be added with the residual of the network to alleviate the loss of information. The biLSTM is employed in the DeepLPI model, which can capture long-term dependencies of the sequence, equally encode input sequence once from beginning to end and once from end to beginning. Compared to the classical LSTM, the biLSTM enables the use of the two hidden states in each LSTM memory block to preserve information from both past and future.", "sentences": [], "annotations": [], "relations": []}, {"offset": 29969, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "In our experiments, we notice that the performance of DeepLPI is not uniform on different proteins: there might exist some common biological features of those proteins such as the sequences or the spatial structures. Detailed analysis of the shared features of the proteins requires a deeper understanding of the protein-drug interaction and can potentially explain the reason that the model behaves well on some of the proteins. Such analysis would be useful to improve the model when we generalize the results later.", "sentences": [], "annotations": [], "relations": []}, {"offset": 30488, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "The DeepLPI model may help in speeding up the COVID-19 drug research. As of today, the pandemic is not showing any sign of slowing down and people are still searching for an effective and safe cure for COVID-19 patients. The current widely-used combination treatment with hydroxychloroquine and azithromycin has not been proven to be satisfactory, and there are some research efforts in using computational, especially deep neural network, techniques for searching the effective repurposed drugs. Our model can be useful in speeding up the drug search and potentially increase the success rate because the training data fed into the model is not limited to the protein structural information.", "sentences": [], "annotations": [], "relations": []}, {"offset": 31181, "infons": {"section_type": "DISCUSS", "type": "paragraph"}, "text": "Even though we have successfully built a model that can predict active/inactive interaction with high accuracy, the model still suffers from some limitations. There is still room for improvement regarding the prediction accuracy, especially when the model is applied on external datasets. To rigorously evaluate model prediction power, a control test set had better be significantly different from the training set in distributions. The \u201cunseen\u201d test sets in this work are different only in molecular and protein length distributions. Meanwhile, there exist other splitting methods such as characterized by molecular scaffold or more quantitatively, protein similarity. Comparing with alternative differential data splitting approaches is out of the scope of this study, but we will investigate the impact of different data splitting approaches in the future work. From a broader perspective, the study of repurposing drugs should not be limited only to the binding affinities. Researchers should also pay attention to the possibility of potential adverse effects of using the repurposed drug. This can be a result of new interactions between the drug and the proposed disease target, or because the drug is administered to a new group of population. Sometimes the repurposed drug could have interactions with traditional drugs on the new disease, and adverse effects might also arise from such unexpected interactions. Deep learning methods could also be used in studies on these aspects for better safety.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32693, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 32719, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Publisher's note", "sentences": [], "annotations": [], "relations": []}, {"offset": 32736, "infons": {"section_type": "SUPPL", "type": "footnote"}, "text": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32855, "infons": {"section_type": "SUPPL", "type": "title_1"}, "text": "Supplementary Information", "sentences": [], "annotations": [], "relations": []}, {"offset": 32881, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The online version contains supplementary material available at 10.1038/s41598-022-23014-1.", "sentences": [], "annotations": [], "relations": []}, {"offset": 32973, "infons": {"section_type": "AUTH_CONT", "type": "title"}, "text": "Author contributions", "sentences": [], "annotations": [], "relations": []}, {"offset": 32994, "infons": {"section_type": "AUTH_CONT", "type": "paragraph"}, "text": "B.W. conceived the idea. All authors contributed to the proposal of the algorithm and selection of database and baseline methods. B.W. and X.G. contributed to the choice of metrics. B.W. and Y.Z. contributed to the construction of testing methods. B.W. wrote the manuscript and all authors contributed to the manuscript revisions.", "sentences": [], "annotations": [], "relations": []}, {"offset": 33325, "infons": {"section_type": "SUPPL", "type": "title"}, "text": "Data availability", "sentences": [], "annotations": [], "relations": []}, {"offset": 33343, "infons": {"section_type": "SUPPL", "type": "paragraph"}, "text": "The datasets including drug information and protein information analyzed during the current study are publicly available online through the following links: BindingDB data (http://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes), Davis data (http://staff.cs.utu.fi/~aatapa/data/DrugTarget/), and COVID-19 data (https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html). The code of the LPI model and the training and testing script notebooks are available open source through the following Github link (https://github.com/David-BominWei/DeepLPI).", "sentences": [], "annotations": [], "relations": []}, {"offset": 33948, "infons": {"section_type": "COMP_INT", "type": "title"}, "text": "Competing interests", "sentences": [], "annotations": [], "relations": []}, {"offset": 33968, "infons": {"section_type": "COMP_INT", "type": "paragraph"}, "text": "The authors declare no competing interests.", "sentences": [], "annotations": [], "relations": []}, {"offset": 34012, "infons": {"section_type": "REF", "type": "title"}, "text": "References", "sentences": [], "annotations": [], "relations": []}, {"offset": 34023, "infons": {"name_0": "surname:Pushpakom;given-names:S", "pub-id_doi": "10.1038/nrd.2018.168", "pub-id_pmid": "30310233", "section_type": "REF", "source": "Nat. Rev. Drug Discov.", "type": "ref", "year": "2019"}, "text": "Drug repurposing: Progress, challenges and recommendations", "sentences": [], "annotations": [], "relations": []}, {"offset": 34082, "infons": {"name_0": "surname:Wouters;given-names:OJ", "name_1": "surname:McKee;given-names:M", "name_2": "surname:Luyten;given-names:J", "pub-id_doi": "10.1001/jama.2020.1166", "pub-id_pmid": "32749488", "section_type": "REF", "source": "JAMA", "type": "ref", "year": "2020"}, "text": "Estimated research and development investment needed to bring a new medicine to market, 2009\u20132018", "sentences": [], "annotations": [], "relations": []}, {"offset": 34182, "infons": {"fpage": "15", "issue": "4", "name_0": "surname:Mahajan;given-names:R", "name_1": "surname:Gupta;given-names:K", "pub-id_doi": "10.4103/0975-7406.72130", "section_type": "REF", "source": "J. Pharm. Bioallied Sci.", "type": "ref", "volume": "2", "year": "2010"}, "text": "Food and drug administration\u2032s critical path initiative and innovations in drug development paradigm: Challenges, progress, and controversies", "sentences": [], "annotations": [], "relations": []}, {"offset": 34326, "infons": {"name_0": "surname:Huang;given-names:F", "pub-id_doi": "10.1371/journal.ppat.1008341", "pub-id_pmid": "33275640", "section_type": "REF", "source": "PLOS Pathog.", "type": "ref", "year": "2020"}, "text": "Identification of amitriptyline HCl, flavin adenine dinucleotide, azacitidine and calcitriol as repurposing drugs for influenza A H5N1 virus-induced lung injury", "sentences": [], "annotations": [], "relations": []}, {"offset": 34487, "infons": {"name_0": "surname:Sun;given-names:P", "name_1": "surname:Guo;given-names:J", "name_2": "surname:Winnenburg;given-names:R", "name_3": "surname:Baumbach;given-names:J", "pub-id_doi": "10.1016/j.drudis.2016.10.008", "pub-id_pmid": "29101000", "section_type": "REF", "source": "Drug Discov. Today", "type": "ref", "year": "2017"}, "text": "Drug repurposing by integrated literature mining and drug\u2013gene\u2013disease triangulation", "sentences": [], "annotations": [], "relations": []}, {"offset": 34576, "infons": {"name_0": "surname:Kimber;given-names:TB", "name_1": "surname:Chen;given-names:Y", "name_2": "surname:Volkamer;given-names:A", "pub-id_doi": "10.3390/ijms22094435", "pub-id_pmid": "33922714", "section_type": "REF", "source": "Int. J. Mol. Sci.", "type": "ref", "year": "2021"}, "text": "deep learning in virtual screening: Recent applications and developments", "sentences": [], "annotations": [], "relations": []}, {"offset": 34649, "infons": {"section_type": "REF", "type": "ref"}, "text": "Wallach, I., Dzamba, M., & Heifets, A. AtomNet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery. arXiv preprint arXiv:1510.02855 (2015).", "sentences": [], "annotations": [], "relations": []}, {"offset": 34835, "infons": {"name_0": "surname:Wang;given-names:S", "pub-id_doi": "10.3389/fgene.2020.607824", "pub-id_pmid": "35350819", "section_type": "REF", "source": "Front. Genet.", "type": "ref", "year": "2021"}, "text": "SE-OnionNet: A convolution neural network for protein-ligand binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 34924, "infons": {"name_0": "surname:Liu;given-names:Z", "pub-id_doi": "10.1093/bioinformatics/btu626", "pub-id_pmid": "26681335", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "year": "2015"}, "text": "PDB-wide collection of binding data: Current status of the PDBbind database", "sentences": [], "annotations": [], "relations": []}, {"offset": 35000, "infons": {"fpage": "583", "lpage": "589", "name_0": "surname:Jumper;given-names:J", "name_1": "surname:Evans;given-names:R", "name_2": "surname:Pritzel;given-names:A", "pub-id_doi": "10.1038/s41586-021-03819-2", "pub-id_pmid": "34265844", "section_type": "REF", "source": "Nature", "type": "ref", "volume": "596", "year": "2021"}, "text": "Highly accurate protein structure prediction with AlphaFold", "sentences": [], "annotations": [], "relations": []}, {"offset": 35060, "infons": {"name_0": "surname:Bayat;given-names:A", "pub-id_doi": "10.1136/bmj.324.7344.1018", "pub-id_pmid": "11976246", "section_type": "REF", "source": "BMJ", "type": "ref", "year": "2002"}, "text": "Science, medicine, and the future: Bioinformatics", "sentences": [], "annotations": [], "relations": []}, {"offset": 35110, "infons": {"name_0": "surname:Ballester;given-names:PJ", "name_1": "surname:Mitchell;given-names:JBO", "pub-id_doi": "10.1093/bioinformatics/btq112", "pub-id_pmid": "20236947", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "year": "2010"}, "text": "A machine learning approach to predicting protein\u2013ligand binding affinity with applications to molecular docking", "sentences": [], "annotations": [], "relations": []}, {"offset": 35225, "infons": {"name_0": "surname:Li;given-names:H", "name_1": "surname:Leung;given-names:K-S", "name_2": "surname:Wong;given-names:M-H", "name_3": "surname:Ballester;given-names:P", "pub-id_doi": "10.3390/molecules200610947", "pub-id_pmid": "26729089", "section_type": "REF", "source": "Molecules", "type": "ref", "year": "2015"}, "text": "Low-quality structural and interaction data improves binding affinity prediction via random forest", "sentences": [], "annotations": [], "relations": []}, {"offset": 35324, "infons": {"section_type": "REF", "type": "ref"}, "text": "Hu, P.-W., Chan, K.C.C. & You, Z.-H. Large-scale prediction of drug-target interactions from deep representations, in 2016 International Joint Conference on Neural Networks (IJCNN), 2016, 1236\u20131243. 10.1109/IJCNN.2016.7727339.", "sentences": [], "annotations": [], "relations": []}, {"offset": 35553, "infons": {"fpage": "i821", "issue": "17", "lpage": "i829", "name_0": "surname:\u00d6zt\u00fcrk;given-names:H", "name_1": "surname:\u00d6zg\u00fcr;given-names:A", "name_2": "surname:Ozkirimli;given-names:E", "pub-id_doi": "10.1093/bioinformatics/bty593", "pub-id_pmid": "30423097", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "volume": "34", "year": "2018"}, "text": "DeepDTA: Deep drug-target binding affinity prediction", "sentences": [], "annotations": [], "relations": []}, {"offset": 35607, "infons": {"name_0": "surname:Abbasi;given-names:K", "name_1": "surname:Razzaghi;given-names:P", "name_2": "surname:Poso;given-names:A", "name_3": "surname:Amanlou;given-names:M", "name_4": "surname:Ghasemi;given-names:JB", "name_5": "surname:Masoudi-Nejad;given-names:A", "pub-id_doi": "10.1093/bioinformatics/btaa544", "pub-id_pmid": "32462178", "section_type": "REF", "source": "Bioinformatics", "type": "ref", "year": "2020"}, "text": "DeepCDA: Deep cross-domain compound\u2013protein affinity prediction through LSTM and convolutional neural networks", "sentences": [], "annotations": [], "relations": []}, {"offset": 35720, "infons": {"fpage": "27", "issue": "1", "lpage": "35", "name_0": "surname:Jaeger;given-names:S", "name_1": "surname:Fulle;given-names:S", "name_2": "surname:Turk;given-names:S", "pub-id_doi": "10.1021/acs.jcim.7b00616", "pub-id_pmid": "29268609", "section_type": "REF", "source": "J. Chem. Inf. Model.", "type": "ref", "volume": "58", "year": "2018"}, "text": "Mol2vec: Unsupervised machine learning approach with chemical intuition", "sentences": [], "annotations": [], "relations": []}, {"offset": 35792, "infons": {"name_0": "surname:Bepler;given-names:T", "name_1": "surname:Berger;given-names:B", "pub-id_doi": "10.1016/j.cels.2021.05.017", "pub-id_pmid": "34139171", "section_type": "REF", "source": "Cell Syst.", "type": "ref", "year": "2021"}, "text": "Learning the protein language: Evolution, structure, and function", "sentences": [], "annotations": [], "relations": []}, {"offset": 35858, "infons": {"fpage": "D198", "issue": "Issue suppl_1", "lpage": "D201", "name_0": "surname:Liu;given-names:T", "name_1": "surname:Lin;given-names:Y", "name_2": "surname:Wen;given-names:X", "name_3": "surname:Jorissen;given-names:RN", "name_4": "surname:Gilson;given-names:MK", "pub-id_doi": "10.1093/nar/gkl999", "pub-id_pmid": "17145705", "section_type": "REF", "source": "Nucleic Acids Res.", "type": "ref", "volume": "35", "year": "2007"}, "text": "BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities", "sentences": [], "annotations": [], "relations": []}, {"offset": 35958, "infons": {"fpage": "5", "issue": "11", "name_0": "surname:Davis;given-names:MI", "pub-id_doi": "10.1038/nbt.1990", "pub-id_pmid": "21221079", "section_type": "REF", "source": "Nat. Biotechnol.", "type": "ref", "volume": "29", "year": "2011"}, "text": "Comprehensive analysis of kinase inhibitor selectivity", "sentences": [], "annotations": [], "relations": []}, {"offset": 36013, "infons": {"section_type": "REF", "type": "ref"}, "text": "Diamond Light Source. Main protease structure and XChem fragment screen. https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36176, "infons": {"section_type": "REF", "type": "ref"}, "text": "Huang, K. et al. Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development; 2021. arXiv preprints arXiv:2102.09548.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36332, "infons": {"name_0": "surname:Rogers;given-names:D", "name_1": "surname:Hahn;given-names:M", "pub-id_doi": "10.1021/ci100050t", "pub-id_pmid": "20426451", "section_type": "REF", "source": "J. Chem. Inf. Model.", "type": "ref", "year": "2010"}, "text": "Extended-connectivity fingerprints", "sentences": [], "annotations": [], "relations": []}, {"offset": 36367, "infons": {"section_type": "REF", "type": "ref"}, "text": "He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770\u2013778 (2016). 10.1109/CVPR.2016.90.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36565, "infons": {"section_type": "REF", "type": "ref"}, "text": "He, K., Zhang, X., Ren, S. & Sun, J. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification, in 2015 IEEE International Conference on Computer Vision (ICCV) 1026\u20131034 (2015). 10.1109/ICCV.2015.123.", "sentences": [], "annotations": [], "relations": []}, {"offset": 36802, "infons": {"section_type": "REF", "type": "ref"}, "text": "Kingma, D.P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).", "sentences": [], "annotations": [], "relations": []}, {"offset": 36907, "infons": {"fpage": "105772", "name_0": "surname:Monteiro;given-names:NRC", "name_1": "surname:Oliveira;given-names:JL", "name_2": "surname:Arrais;given-names:JP", "pub-id_doi": "10.1016/j.compbiomed.2022.105772", "pub-id_pmid": "35777085", "section_type": "REF", "source": "Comput. Biol. Med.", "type": "ref", "volume": "147", "year": "2022"}, "text": "DTITR: End-to-end drug\u2013target binding affinity prediction with transformers", "sentences": [], "annotations": [], "relations": []}, {"offset": 36985, "infons": {"section_type": "REF", "type": "ref"}, "text": "Gardner, M., Grus, J., Neumann, M. et al. AllenNLP: A deep semantic natural language processing platform. arXiv preprint arXiv:1803.07640 (2018).", "sentences": [], "annotations": [], "relations": []}, {"offset": 37131, "infons": {"section_type": "REF", "type": "ref"}, "text": "Asgari, E. & Mofrad, M.R.K. ProtVec: A continuous distributed representation of biological sequences. arXiv preprint arXiv:1503.05140 (2015).", "sentences": [], "annotations": [], "relations": []}], "annotations": [], "relations": []}]}]