{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "445bc088-d0e6-4f39-89f5-646df6c77248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading 4‑bit quantized model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b8857949da443b9049ceb602f3a109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA adapters...\n",
      "trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6543\n",
      "Loading dataset from dependency_dataset.jsonl...\n",
      "Applying prompt formatting...\n",
      "Tokenizing dataset...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "LoRA fine‑tuning of a 4‑bit quantized causal LM,\n",
    "using pre‑tokenization & Transformers Trainer + PEFT.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "model_id      = \"microsoft/Phi-3-mini-4k-instruct\"  \n",
    "dataset_path  = \"dependency_dataset.jsonl\" \n",
    "cuda_idx = torch.cuda.current_device()\n",
    "\n",
    "\n",
    "# LoRA hyperparams\n",
    "lora_r        = 16\n",
    "lora_alpha    = 32\n",
    "lora_dropout  = 0.05\n",
    "\n",
    "target_modules = ['down_proj', 'gate_up_proj', 'o_proj', 'qkv_proj']\n",
    "\n",
    "# Training hyperparams\n",
    "num_epochs    = 100\n",
    "per_device_bs = 4\n",
    "grad_accum    = 8\n",
    "learning_rate = 2e-4 \n",
    "max_seq_length = 1024 \n",
    "\n",
    "# 2. Load tokenizer & 4‑bit model\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading 4‑bit quantized model...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={ \"\": cuda_idx },\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# 3. Attach LoRA adapters\n",
    "\n",
    "print(\"Configuring LoRA adapters...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset from {dataset_path}...\")\n",
    "raw_ds = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# Format prompts\n",
    "def formatting_prompts_func(ex):\n",
    "    \"\"\"\n",
    "    Prompt formatter for microsoft/Phi-3-mini-4k-instruct.\n",
    "    Uses tokenizer.apply_chat_template to build:\n",
    "      [User:] prompt + input\n",
    "      [Assistant:] output\n",
    "    \"\"\"\n",
    "    # Combine the instruction and facts into one user message\n",
    "    user_content = f\"{ex['prompt']}\\n{ex['input']}\"\n",
    "\n",
    "    # Apply Phi‑3’s chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\",    \"content\": user_content},\n",
    "            {\"role\": \"assistant\",\"content\": ex[\"output\"]},\n",
    "        ],\n",
    "        add_generation_prompt=False, \n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    # Return as the single “text” field for downstream tokenization\n",
    "    return {\"text\": formatted}\n",
    "\n",
    "print(\"Applying prompt formatting...\")\n",
    "ds_text = raw_ds.map(\n",
    "    formatting_prompts_func,\n",
    "    remove_columns=list(raw_ds.features),\n",
    ")\n",
    "\n",
    "\n",
    "# pretokenize\n",
    "def tokenize_for_causal(examples):\n",
    "    # Tokenize & pad/truncate\n",
    "    batch = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_seq_length,\n",
    "    )\n",
    "    # Causal LM: labels = input_ids\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "    return batch\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_ds = ds_text.map(\n",
    "    tokenize_for_causal,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# │ 7. Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68b599a5-6eec-430e-8111-f49e942a082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╭───────────────────────────────╮\n",
    "# │ 8. Training arguments         │\n",
    "# ╰───────────────────────────────╯\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./lora_adapter\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=per_device_bs,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "551661c0-e9bd-4c40-a167-25d4d189c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'gate_up_proj', 'o_proj', 'qkv_proj']\n"
     ]
    }
   ],
   "source": [
    "uniq = set()\n",
    "for n,_ in model.named_modules():\n",
    "    if n.endswith(\"proj\"):\n",
    "        base_name = n.split(\".\")[-1] \n",
    "        uniq.add(base_name)\n",
    "print(sorted(uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a2a180ad-aadb-4cdc-ac1b-8790375133b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2025982/1817864525.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 🤗 Trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 1:43:15, Epoch 75/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! LoRA adapter saved in ./lora_adapter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ╭───────────────────────────────╮\n",
    "# │ 9. Trainer & train            │\n",
    "# ╰───────────────────────────────╯\n",
    "print(\"Starting training with Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "print(\"Done! LoRA adapter saved in ./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9f532fc6-1ab5-48fa-a6f2-ef2fc9c28a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3d761fab9b4abdbc435edd0a560524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pattern': 'TP Marcus (48, U.S. citizen) has a daughter Lily (23, U.S. citizen). Lily is a full‑time graduate student away at school but lives with Marcus on breaks (>5 months). Marcus pays 85% of Lily’s total support, including tuition. Lily earned $3,200 from campus work and files single.', 'model_answer': 'Yes'}, {'pattern': 'TP Ellen (42, U.S. citizen) supports her nephew Zack (17, U.S. citizen). Zack lived with Ellen only 5 months; the rest of the year he lived with friends. Ellen provided 40% of Zack’s support; Zack’s part‑time job covered the rest. Zack files single and earned $7,500.', 'model_answer': 'Yes'}, {'pattern': 'TP Raj (34, U.S. citizen) and two siblings each pay roughly one‑third of their mother Meena’s (68, U.S. citizen) support. Meena’s own income is $1,500 (below $4,700). The children signed a written multiple‑support agreement designating Raj to claim Meena.', 'model_answer': 'Yes'}, {'pattern': 'TP Luis (55, U.S. citizen) lets his cousin Paco (35, U.S. citizen) live with him the entire year. Luis pays 70% of Paco’s support. Paco earns $8,200 freelancing (above $4,700). Paco files single.', 'model_answer': 'No'}, {'pattern': \"TP is Helen (70, US Citizen). PD is Irene (90, Helen's mother, US Citizen). Irene lives in a nursing home and has $2000 income. Helen and her two brothers contribute significantly to Irene's nursing home costs, which constitute the bulk of her support. Collectively, the three children provide over 80% of Irene's support, with Helen contributing about 30% herself. They have verbally agreed Helen should claim the deduction this year.\", 'model_answer': 'Yes'}, {'pattern': \"TP is Brenda (65, US Citizen). PD is Carl (85, Brenda's father, US Citizen). Carl lives in his own apartment and has $4000 gross income (below exemption). Brenda helps Carl significantly by paying several major bills and buying groceries frequently throughout the year. Carl also uses his Social Security for other expenses. It's unclear if Brenda's contributions constitute more than half of Carl's total support for the year.\", 'model_answer': 'No'}]\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DynamicCache\n",
    "if not hasattr(DynamicCache, \"get_max_length\"):\n",
    "    def _get_max_length(self):  # self.get_max_cache_shape() returns (batch, heads, seq_len, hidden)\n",
    "        shape = getattr(self, \"get_max_cache_shape\", lambda: None)()\n",
    "        return shape[2] if shape else None\n",
    "    DynamicCache.get_max_length = _get_max_length\n",
    "\n",
    "# ── paths \n",
    "adapter_dir   = \"./lora_adapter\" \n",
    "base_model_id = model_id\n",
    "patterns_file = \"./fact_patterns.jsonl\"\n",
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ── load tokenizer & base model in 4‑bit\n",
    "tok = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, adapter_dir, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def format_chat(prompt, _input):\n",
    "    user = f\"{prompt}\\n{_input}\"\n",
    "    return tok.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": user},\n",
    "         {\"role\": \"assistant\", \"content\": \"\"}],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "# ── load patterns\n",
    "patterns = []\n",
    "with open(patterns_file) as fp:\n",
    "    for line in fp:\n",
    "        patterns.append(json.loads(line))\n",
    "results = []\n",
    "# ── run inference \n",
    "for i, ex in enumerate(patterns, 1):\n",
    "    chat_str = format_chat(ex[\"prompt\"], ex[\"input\"])\n",
    "    inputs   = tok(chat_str, return_tensors=\"pt\").to(device)\n",
    "    out_ids  = model.generate(**inputs, max_new_tokens=8, do_sample=False)\n",
    "    answer   = tok.decode(out_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "                          skip_special_tokens=True).strip()\n",
    "    results.append({\"pattern\": ex[\"input\"], \"model_answer\": answer})\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b737bc1-04d7-4160-b02f-4273fcc82083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model outputs ➜ data/model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, pathlib, json\n",
    "\n",
    "df_model = pd.DataFrame(results)\n",
    "csv_path = pathlib.Path(\"./model_predictions.csv\")\n",
    "df_model.to_csv(csv_path, index=False)\n",
    "print(\"Saved model outputs ➜\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfa5c0-436b-4ff0-85d2-c21313a9e6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
